<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.19" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.71" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://luohenyueji.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-10-01-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97.html"><meta property="og:site_name" content="落痕月极的博客"><meta property="og:title" content="[深度学习] 大模型学习5-高效微调框架Unsloth使用指北"><meta property="og:description" content="[深度学习] 大模型学习5-高效微调框架Unsloth使用指北 Unsloth是一个专注于加速大语言模型微调过程的开源项目。它通过一系列底层优化，显著提升了微调速度并大幅降低了内存消耗，同时能保持模型性能。无论是研究者还是开发者，都能借助Unsloth更高效地定制自己的大语言模型。本文将介绍Unsloth的使用，相关学习资源如下： 开源仓库：Unslo..."><meta property="og:type" content="article"><meta property="og:image" content="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img1.jpg"><meta property="og:locale" content="zh-CN"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="自然语言处理与语音识别"><meta property="article:tag" content="Python"><meta property="article:published_time" content="2025-10-01T11:18:01.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"[深度学习] 大模型学习5-高效微调框架Unsloth使用指北","image":["https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img1.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img2.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img3.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img4.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img5.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img6.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img6_2.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/img/img7.jpg"],"datePublished":"2025-10-01T11:18:01.000Z","dateModified":null,"author":[{"@type":"Person","name":"落痕月极","url":"/"}]}</script><link rel="icon" href="/logo.png"><title>[深度学习] 大模型学习5-高效微调框架Unsloth使用指北 | 落痕月极的博客</title><meta name="description" content="[深度学习] 大模型学习5-高效微调框架Unsloth使用指北 Unsloth是一个专注于加速大语言模型微调过程的开源项目。它通过一系列底层优化，显著提升了微调速度并大幅降低了内存消耗，同时能保持模型性能。无论是研究者还是开发者，都能借助Unsloth更高效地定制自己的大语言模型。本文将介绍Unsloth的使用，相关学习资源如下： 开源仓库：Unslo...">
    <link rel="preload" href="/assets/style-7oyYUlCQ.css" as="style"><link rel="stylesheet" href="/assets/style-7oyYUlCQ.css">
    <link rel="modulepreload" href="/assets/app-B1QbUTkN.js"><link rel="modulepreload" href="/assets/2025-10-01-_深度学习_ 大模型学习5-高效微调框架Unsloth使用指北.html-DqBI7L3g.js">
    <link rel="prefetch" href="/assets/about.html-2Qx6wz25.js" as="script"><link rel="prefetch" href="/assets/intro.html-CJMgStby.js" as="script"><link rel="prefetch" href="/assets/index.html-BhjB_f3u.js" as="script"><link rel="prefetch" href="/assets/index.html-DfDgK0oH.js" as="script"><link rel="prefetch" href="/assets/index.html-D64f2GoY.js" as="script"><link rel="prefetch" href="/assets/2021-08-04-_图像处理_ 基于图像哈希构建图像相似度对比算法.html-DQpcY0vh.js" as="script"><link rel="prefetch" href="/assets/2024-10-24-_图像处理_ 基于CleanVision库清洗图像数据集.html-zlYxdORa.js" as="script"><link rel="prefetch" href="/assets/2019-01-21-_常用工具_ 深度学习Caffe处理工具.html-DR6jz-Pv.js" as="script"><link rel="prefetch" href="/assets/2019-03-12-_常用工具_ Caffe ssd常见问题集合.html-C-O8Fz66.js" as="script"><link rel="prefetch" href="/assets/2019-04-19-_常用工具_ OpenCV获取网络摄像头实时视频流.html-C3zEXEzu.js" as="script"><link rel="prefetch" href="/assets/2020-02-16-_常用工具_ git基础学习笔记.html-BLU_VQA3.js" as="script"><link rel="prefetch" href="/assets/2020-04-07-_常用工具_ shell脚本快速入门笔记.html-GgWtu6mK.js" as="script"><link rel="prefetch" href="/assets/2020-05-07-_常用工具_ live555的搭建.html-D_SO9nS6.js" as="script"><link rel="prefetch" href="/assets/2020-08-11-_常用工具_ OpenCV_contrib库在windows下编译使用指南.html-Dq2fE92b.js" as="script"><link rel="prefetch" href="/assets/2021-02-10-_常用工具_ cvat安装与使用指北.html-CFer74EU.js" as="script"><link rel="prefetch" href="/assets/2021-04-23-_常用工具_ dlib编译调用指南.html-BBmLxyJG.js" as="script"><link rel="prefetch" href="/assets/2021-05-22-_常用工具_ mermaid学习笔记.html-BIOdC9Po.js" as="script"><link rel="prefetch" href="/assets/2021-12-21-_常用工具_ PyAutoGUI使用教程.html-p2eQT3AA.js" as="script"><link rel="prefetch" href="/assets/2022-03-20-_常用工具_ 搜索引擎的常用技巧总结.html-CSJXL6si.js" as="script"><link rel="prefetch" href="/assets/2022-07-13-_常用工具_ C__环境下Qt的安装.html-DK2qhiaf.js" as="script"><link rel="prefetch" href="/assets/2022-07-18-_常用工具_ 基于psutil和GPUtil获取系统状态信息.html-y7f0wbZW.js" as="script"><link rel="prefetch" href="/assets/2022-08-12-_常用工具_ Python视频处理库VidGear使用指北.html-BBy6zKnD.js" as="script"><link rel="prefetch" href="/assets/2022-08-19-_常用工具_ Python视频解码库DeFFcode使用指北.html-PJATDp3M.js" as="script"><link rel="prefetch" href="/assets/2017-10-24-_数学理论_ 单一数字评估指标.html-afL8cEMV.js" as="script"><link rel="prefetch" href="/assets/2017-10-25-_数学理论_ 不同分布训练集、验证集、测试集处理.html-CangkzDc.js" as="script"><link rel="prefetch" href="/assets/2021-06-27-_数据分析与可视化_ 科技论文配色心得.html-2y0pKdTx.js" as="script"><link rel="prefetch" href="/assets/2024-06-01-_机器学习_ 低代码机器学习工具PyCaret库使用指北.html-WhbFJ4yd.js" as="script"><link rel="prefetch" href="/assets/2025-12-18-_机器学习_ 类别变量编码库category_encoders使用指南.html-CxW_BTLw.js" as="script"><link rel="prefetch" href="/assets/2023-07-27-_自然语言处理_ 自然语言处理库spaCy使用指北.html-CMek4pdu.js" as="script"><link rel="prefetch" href="/assets/2023-08-21-_语音识别_ 基于Python构建简易的音频录制与语音识别应用.html-BKTeexdu.js" as="script"><link rel="prefetch" href="/assets/2023-09-24-_自然语言处理_ 基于pycorrector实现文本纠错.html-DdfRFVGB.js" as="script"><link rel="prefetch" href="/assets/2020-12-13-_讲座论坛_ 碧根果产业现状及产业化开发关键技术.html-BBzrhwhA.js" as="script"><link rel="prefetch" href="/assets/2020-12-20-_讲座论坛_ 竹资源培育与中国竹产业.html-DOOpr0Im.js" as="script"><link rel="prefetch" href="/assets/2020-12-27-_讲座论坛_ 经济林之核桃类.html-CcS9tTcy.js" as="script"><link rel="prefetch" href="/assets/2021-04-21-_讲座论坛_ 应对气候变化的中国视角.html-D2xZ4A03.js" as="script"><link rel="prefetch" href="/assets/2022-02-28-_讲座论坛_ 国家自然基金申请知识汇总.html-BE0I8hPJ.js" as="script"><link rel="prefetch" href="/assets/2022-12-31-_讲座论坛_ 研究的艺术学习笔记.html-DFFuguUm.js" as="script"><link rel="prefetch" href="/assets/2017-11-04-_能源化工_ TE田纳西-伊斯曼过程数据集.html-DCcsyiyx.js" as="script"><link rel="prefetch" href="/assets/2020-05-09-_能源化工_ 电力四遥.html-BQ0PncwZ.js" as="script"><link rel="prefetch" href="/assets/2022-01-25-_生命科学_ 生物基础实验之PCR验证.html-CNEdVTob.js" as="script"><link rel="prefetch" href="/assets/2022-01-31-_生命科学_ 生物基础实验之DNA提取.html-QCQv9J66.js" as="script"><link rel="prefetch" href="/assets/2022-03-04-_生命科学_ snapgene 构建载体方法分享.html-ivLIHWG3.js" as="script"><link rel="prefetch" href="/assets/2022-03-25-_生命科学_ 生物基础实验之三引物检测突变体.html-CnBeaA-b.js" as="script"><link rel="prefetch" href="/assets/2025-09-13-_能源化工_ 面向锂电池RUL预测的开源项目全景速览.html-B1K3le2S.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ kmeans聚类和WGCNA.html-B9c1DgEL.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 均匀和不均匀造林对生态多样性的影响综述.html-BBLEAtwZ.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 枣树的历史与现状研究进展.html-cz-P3kex.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 森林管理和造林业中复杂观念的转变.html-kRxqC0NZ.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 深度学习技术在植物领域的研究1.html-C-PwG8Ne.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 深度学习技术在植物领域的研究2.html-Bi0n395-.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 物联网技术在现代林业中的应用.html-Aj02yBA7.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 石榴综述论文阅读笔记.html-BP-695go.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 美国造林业：过去30年的惊人变化时期.html-nOP5zbtY.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 苗圃营建学习笔记.html-CgXdl_wz.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 集约经营下人工林造林技术研究进展.html-BmQqm9tP.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 农业工程领域中App和Web相关应用论文笔记.html-Cw-ICoXU.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 智慧农业论文摘要阅读概览.html-CUjsCvfy.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 深度学习在农业领域应用论文笔记.html-DNGspGXJ.js" as="script"><link rel="prefetch" href="/assets/2021-10-18-_论文总结_ 育种理论与基因检测.html-Chd1_kgH.js" as="script"><link rel="prefetch" href="/assets/2021-10-19-_论文总结_ 树木的营养生理.html-D-ThVVYw.js" as="script"><link rel="prefetch" href="/assets/2021-10-20-_论文总结_ 深度学习在农业领域应用论文笔记2.html-C7zOMgdJ.js" as="script"><link rel="prefetch" href="/assets/2021-10-21-_论文总结_ 深度学习在农业领域应用论文笔记3.html-Dnl18iNR.js" as="script"><link rel="prefetch" href="/assets/2021-10-27-_论文总结_ 深度学习在农业领域应用论文笔记4.html-B4Zczg0S.js" as="script"><link rel="prefetch" href="/assets/2021-10-28-_论文总结_ 深度学习在农业领域应用论文笔记5.html-Br2GNxHD.js" as="script"><link rel="prefetch" href="/assets/2021-10-29-_论文总结_ 深度学习在农业领域应用论文笔记6.html-rED-w7gt.js" as="script"><link rel="prefetch" href="/assets/2021-10-30-_论文总结_ 深度学习在农业领域应用论文笔记7.html-B5XvCXdc.js" as="script"><link rel="prefetch" href="/assets/2021-11-02-_论文总结_ 深度学习在农业领域应用论文笔记8.html-Buo37zHa.js" as="script"><link rel="prefetch" href="/assets/2021-11-03-_论文总结_ 深度学习在农业领域应用论文笔记9.html-COBrNk3l.js" as="script"><link rel="prefetch" href="/assets/2021-11-04-_论文总结_ 森林生态系统中的水生生境.html-DCRnuvIN.js" as="script"><link rel="prefetch" href="/assets/2022-04-05-_论文总结_  种群、保护与生态遗传学笔记.html-C2KFVslo.js" as="script"><link rel="prefetch" href="/assets/2022-05-01-_论文总结_ Genecology and Adaptation of Forest Trees 林木的基因生态学与适应性.html-DGdNHnN6.js" as="script"><link rel="prefetch" href="/assets/2022-05-20-_论文总结_ 中国工科生常见英文写作问题总结.html-BNNgGLOr.js" as="script"><link rel="prefetch" href="/assets/2022-05-31-_论文总结_ 科技论文英语写作笔记1.html-duTXgiET.js" as="script"><link rel="prefetch" href="/assets/2022-08-01-_论文总结_ 深度学习在农业领域应用论文笔记10.html-Cn0HsvBj.js" as="script"><link rel="prefetch" href="/assets/2023-02-28-_论文总结_ 深度学习在农业领域应用论文笔记11.html-CjE-c5iJ.js" as="script"><link rel="prefetch" href="/assets/2024-02-10-_论文总结_ 深度学习在农业领域应用论文笔记12.html-I4eDsXra.js" as="script"><link rel="prefetch" href="/assets/2024-09-25-_论文总结_ 深度学习在农业领域应用论文笔记13.html-BibvsTVZ.js" as="script"><link rel="prefetch" href="/assets/2025-01-28-_论文总结_ 深度学习在农业领域应用论文笔记14.html-DumjIxKX.js" as="script"><link rel="prefetch" href="/assets/2023-05-31-_音视频处理_ FFmpeg使用指北1-视频解码.html-eAmQzCpm.js" as="script"><link rel="prefetch" href="/assets/2020-05-13-_随笔所想_ CSDN认证博客专家申请通过随笔所想.html-C64OweXf.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_随笔所想_ 程序员中年失业随笔所想.html-poR3wLMz.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_随笔所想_ UBC学习生活经验分享.html-CtjTmQ9U.js" as="script"><link rel="prefetch" href="/assets/2021-01-14-_随笔所想_ 2021年新年碎碎念-加油了不起的干饭人!.html-BAHbtsWk.js" as="script"><link rel="prefetch" href="/assets/2021-02-12-_随笔所想_ 牛年碎碎念祝大家牛年大吉.html-D21j3dOu.js" as="script"><link rel="prefetch" href="/assets/2021-03-31-_随笔所想_ 买房和户型挑选入门.html-DB8Qr13R.js" as="script"><link rel="prefetch" href="/assets/2021-08-29-_随笔所想_ 学英语打卡2000天碎碎念.html-DoX1Njqe.js" as="script"><link rel="prefetch" href="/assets/2021-12-1-_随笔所想_ 沉痛悼念开发技术专家毛星云老师.html-w1oEzieF.js" as="script"><link rel="prefetch" href="/assets/2024-02-17-_随笔所想_ 劳动合同法学习笔记.html-JDN2_hbn.js" as="script"><link rel="prefetch" href="/assets/2019-03-04-_OpenCV实战_1 基于深度学习识别人脸性别和年龄.html-CkYtombX.js" as="script"><link rel="prefetch" href="/assets/2019-03-05-_OpenCV实战_2 人脸识别算法对比.html-D-SBZgKU.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_3 透明斗篷.html-DpUH7aC6.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_4 OpenCV中的颜色空间.html-qDn5hlbH.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_5 基于深度学习的文本检测.html-Bq-6TuCF.js" as="script"><link rel="prefetch" href="/assets/2019-03-08-_OpenCV实战_6 基于特征点匹配的视频稳像.html-CgvSUprF.js" as="script"><link rel="prefetch" href="/assets/2019-03-13-_OpenCV实战_7 使用YOLOv3和OpenCV进行基于深度学习的目标检测.html-tflf9nJ9.js" as="script"><link rel="prefetch" href="/assets/2019-03-15-_OpenCV实战_8 深度学习目标检测网络YOLOv3的训练.html-D9ERvLXA.js" as="script"><link rel="prefetch" href="/assets/2019-03-16-_OpenCV实战_10 使用Hu矩进行形状匹配.html-wxUyN4Vy.js" as="script"><link rel="prefetch" href="/assets/2019-03-16-_OpenCV实战_9 使用OpenCV寻找平面图形的质心.html-Dvmpt0gc.js" as="script"><link rel="prefetch" href="/assets/2019-03-19-_OpenCV实战_11 基于OpenCV的二维码扫描器.html-B1QT4B6m.js" as="script"><link rel="prefetch" href="/assets/2019-03-27-_OpenCV实战_12 使用深度学习和OpenCV进行手部关键点检测.html-Bg3_NuMl.js" as="script"><link rel="prefetch" href="/assets/2019-04-02-_OpenCV实战_13 OpenCV中使用Mask R-CNN进行对象检测和实例分割.html-fGJ36Ugp.js" as="script"><link rel="prefetch" href="/assets/2019-04-04-_OpenCV实战_14 使用OpenCV实现单目标跟踪.html-XEh0DQA6.js" as="script"><link rel="prefetch" href="/assets/2019-04-08-_OpenCV实战_15 基于深度学习的目标跟踪算法GOTURN.html-BXduqEd3.js" as="script"><link rel="prefetch" href="/assets/2019-04-08-_OpenCV实战_16 使用OpenCV实现多目标跟踪.html-DKcqbY9j.js" as="script"><link rel="prefetch" href="/assets/2019-04-10-_OpenCV实战_17 基于卷积神经网络的OpenCV图像着色.html-CeN01u0a.js" as="script"><link rel="prefetch" href="/assets/2019-04-16-_OpenCV实战_18 OpenCV中的单应性矩阵Homography.html-DNPPjwYx.js" as="script"><link rel="prefetch" href="/assets/2019-04-17-_OpenCV实战_19 使用OpenCV实现基于特征的图像对齐.html-DWutZQQq.js" as="script"><link rel="prefetch" href="/assets/2019-04-22-_OpenCV实战_20 使用OpenCV实现基于增强相关系数最大化的图像对齐.html-BBp5MVz2.js" as="script"><link rel="prefetch" href="/assets/2019-04-23-_OpenCV实战_21 使用OpenCV的Eigenface.html-B0Zko8wx.js" as="script"><link rel="prefetch" href="/assets/2019-04-24-_OpenCV实战_22 使用EigenFaces进行人脸重建.html-BLZMmFmE.js" as="script"><link rel="prefetch" href="/assets/2019-04-30-_OpenCV实战_23 使用OpenCV获取高动态范围成像HDR.html-Dgl97bZE.js" as="script"><link rel="prefetch" href="/assets/2019-05-05-_OpenCV实战_24 使用OpenCV进行曝光融合.html-Db7udi_n.js" as="script"><link rel="prefetch" href="/assets/2019-05-05-_OpenCV实战_25 使用OpenCV进行泊松克隆.html-a-uGftTf.js" as="script"><link rel="prefetch" href="/assets/2019-05-06-_OpenCV实战_26 基于OpenCV实现选择性搜索算法.html-D80oxwki.js" as="script"><link rel="prefetch" href="/assets/2019-05-07-_OpenCV实战_27 在OpenCV下使用forEach进行并行像素访问.html-2b53rWQe.js" as="script"><link rel="prefetch" href="/assets/2019-05-08-_OpenCV实战_28 基于OpenCV的GUI库cvui.html-CdeaLvkG.js" as="script"><link rel="prefetch" href="/assets/2019-05-09-_OpenCV实战_29 使用OpenCV实现红眼自动去除.html-CNJQIoim.js" as="script"><link rel="prefetch" href="/assets/2019-05-10-_OpenCV实战_30 使用OpenCV实现图像孔洞填充.html-CWPOvS3l.js" as="script"><link rel="prefetch" href="/assets/2019-05-23-_OpenCV实战_31 使用OpenCV将一个三角形仿射变换到另一个三角形.html-kCrZ6fF4.js" as="script"><link rel="prefetch" href="/assets/2019-05-24-_OpenCV实战_32 使用OpenCV进行非真实感渲染.html-DkAznXVP.js" as="script"><link rel="prefetch" href="/assets/2019-05-27-_OpenCV实战_33 使用OpenCV进行Hough变换.html-KNTIwA51.js" as="script"><link rel="prefetch" href="/assets/2019-05-28-_OpenCV实战_34 使用OpenCV进行图像修复.html-DigLtHtx.js" as="script"><link rel="prefetch" href="/assets/2019-07-16-_OpenCV实战_35 使用Tesseract和OpenCV实现文本识别.html-BY_F-U5Z.js" as="script"><link rel="prefetch" href="/assets/2019-08-30-_OpenCV实战_36 使用OpenCV在视频中实现简单背景估计.html-BDTNHodL.js" as="script"><link rel="prefetch" href="/assets/2020-02-29-_OpenCV实战_37 图像质量评价BRISQUE.html-DXp5QVGo.js" as="script"><link rel="prefetch" href="/assets/2020-03-06-_OpenCV实战_38 基于OpenCV的相机标定.html-D7VsWgi5.js" as="script"><link rel="prefetch" href="/assets/2020-03-31-_OpenCV实战_39 在OpenCV中使用ArUco标记的增强现实.html-m0w5XFbB.js" as="script"><link rel="prefetch" href="/assets/2020-05-07-_OpenCV实战_40 计算机视觉工具对比.html-Cp69lqV3.js" as="script"><link rel="prefetch" href="/assets/2020-05-10-_OpenCV实战_41 嵌入式计算机视觉设备选择.html-oQoYY4TU.js" as="script"><link rel="prefetch" href="/assets/2020-05-12-_OpenCV实战_42 数码单反相机的技术细节.html-Tx5-bcmi.js" as="script"><link rel="prefetch" href="/assets/2020-08-14-_OpenCV实战_43 使用OpenCV进行背景分割.html-CNAwQ0it.js" as="script"><link rel="prefetch" href="/assets/2020-08-24-_OpenCV实战_44 使用OpenCV进行图像超分放大.html-NHHaqgjE.js" as="script"><link rel="prefetch" href="/assets/2020-08-27-_OpenCV实战_45 基于OpenCV实现图像哈希算法.html-D3OI4ud7.js" as="script"><link rel="prefetch" href="/assets/2020-09-10-_OpenCV实战_46 在OpenCV下应用图像强度变换实现图像对比度均衡.html-DrZUnOcE.js" as="script"><link rel="prefetch" href="/assets/2020-09-15-_OpenCV实战_47 基于OpenCV实现视觉显著性检测.html-CICDqTTE.js" as="script"><link rel="prefetch" href="/assets/2020-10-09-_OpenCV实战_48 基于OpenCV实现图像质量评价.html-ae5N7BEz.js" as="script"><link rel="prefetch" href="/assets/2021-02-12-_OpenCV实战_49 对极几何与立体视觉初探.html-8PSTlBjB.js" as="script"><link rel="prefetch" href="/assets/2021-02-16-_OpenCV实战_50 用OpenCV制作低成本立体相机.html-CpvSbvur.js" as="script"><link rel="prefetch" href="/assets/2021-03-15-_OpenCV实战_51 基于OpenCV实现图像极坐标变换与逆变换.html-CC546Mza.js" as="script"><link rel="prefetch" href="/assets/2022-12-01-_OpenCV实战_52 在OpenCV中使用颜色直方图.html-85tYdeWx.js" as="script"><link rel="prefetch" href="/assets/index.html-HAxwbCBY.js" as="script"><link rel="prefetch" href="/assets/2019-06-25-_python_《Python编程快速上手让繁琐工作自动化》学习笔记1.html-DRtI5r9L.js" as="script"><link rel="prefetch" href="/assets/2019-06-26-_python_《Python编程快速上手让繁琐工作自动化》学习笔记2.html-Cp_lU4sl.js" as="script"><link rel="prefetch" href="/assets/2019-06-28-_python_《Python编程快速上手让繁琐工作自动化》学习笔记3.html-EOhdfbCe.js" as="script"><link rel="prefetch" href="/assets/2019-07-05-_python_《Python编程快速上手让繁琐工作自动化》学习笔记4.html-6g2cmj7O.js" as="script"><link rel="prefetch" href="/assets/2019-07-27-_python_《Python编程快速上手让繁琐工作自动化》学习笔记5.html-BxoI2Hic.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_python_《Python编程快速上手让繁琐工作自动化》学习笔记6.html-CAtmEMjz.js" as="script"><link rel="prefetch" href="/assets/2019-08-02-_python_《Python编程快速上手让繁琐工作自动化》学习笔记7.html-p8Hgtg1f.js" as="script"><link rel="prefetch" href="/assets/2017-11-13-_python_ tensorflow中的argmax()函数.html-B32_17U4.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_python_ 基于matplotlib实现树形图的绘制.html-BwB7MALN.js" as="script"><link rel="prefetch" href="/assets/2019-08-13-_python_ mxnet60分钟入门Gluon教程.html-CWkP-Iwd.js" as="script"><link rel="prefetch" href="/assets/2019-10-28-_python_ 基于NetworkX实现网络图的绘制.html-DhGpQzIO.js" as="script"><link rel="prefetch" href="/assets/2019-10-31-_python_ NetworkX实例.html-DwsAB-4i.js" as="script"><link rel="prefetch" href="/assets/2019-11-15-_python_ 基于matplotlib_venn实现维恩图的绘制.html-DeC2nugr.js" as="script"><link rel="prefetch" href="/assets/2019-12-30-_python_ CairoSVG使用教程.html-CJF__G2s.js" as="script"><link rel="prefetch" href="/assets/2020-03-25-_python_ 个人日常python工具代码.html-BDn5TzVh.js" as="script"><link rel="prefetch" href="/assets/2020-05-17-_python_ python模块graphviz使用入门.html-d0kVBdns.js" as="script"><link rel="prefetch" href="/assets/2020-09-01-_python_ 基于matplotlib实现圆环图的绘制.html-BM30nNzD.js" as="script"><link rel="prefetch" href="/assets/2020-09-01-_python_ 基于matplotlib实现雷达图的绘制.html-d2svnsIP.js" as="script"><link rel="prefetch" href="/assets/2021-07-20-_python_ Python二维码生成器qrcode库入门.html-CaNmkGJS.js" as="script"><link rel="prefetch" href="/assets/2021-07-21-_python_ Python map函数总结.html-2FedpkGB.js" as="script"><link rel="prefetch" href="/assets/2021-07-23-_python_ 圆形嵌套图Circular Packing.html-CYaJMYGe.js" as="script"><link rel="prefetch" href="/assets/2022-04-10-_python_ ​python-pinyin库.html-CBAoXPm_.js" as="script"><link rel="prefetch" href="/assets/2022-07-07-_python_ ​Python数据序列化模块pickle使用笔记.html-Cm-4RynY.js" as="script"><link rel="prefetch" href="/assets/2022-07-21-_python_ 向量检索库Faiss使用指北.html-BRbBz8rT.js" as="script"><link rel="prefetch" href="/assets/2022-07-25-_python_ 基于chardet识别字符编码.html-CTNwQRcu.js" as="script"><link rel="prefetch" href="/assets/2022-09-10-_python_ 基于diagrams库绘制系统架构图.html-DO0vMZQZ.js" as="script"><link rel="prefetch" href="/assets/2022-09-19-_python_ 基于blind-watermark库添加图片盲水印.html-DGhQFfVC.js" as="script"><link rel="prefetch" href="/assets/2022-10-24-_python_ 基于Gradio可视化部署机器学习应用.html-VQoY_-Tw.js" as="script"><link rel="prefetch" href="/assets/2022-12-07-_python_ 基于wordcloud库绘制词云图.html-mm3nRQlj.js" as="script"><link rel="prefetch" href="/assets/2023-01-01-_python_ 基于paramiko库操作远程服务器.html-CnaOcisS.js" as="script"><link rel="prefetch" href="/assets/2023-04-17-_python_ Python枚举模块enum总结.html-BXJszr5F.js" as="script"><link rel="prefetch" href="/assets/2023-05-10-_python_ Python类型提示总结.html-CMbNuaf3.js" as="script"><link rel="prefetch" href="/assets/2023-11-30-_python_ 基于Tablib库处理表格数据.html-C6YDG2qq.js" as="script"><link rel="prefetch" href="/assets/2023-12-29-_python_ 基于Dataset库操作数据库.html-IoC5wS3q.js" as="script"><link rel="prefetch" href="/assets/2024-01-25-_python_ 基于RapidFuzz库实现字符串模糊匹配.html-DhL5vRpO.js" as="script"><link rel="prefetch" href="/assets/2024-04-30-_python_ 基于PyWaffle库绘制华夫饼图.html-Cf_YpKXm.js" as="script"><link rel="prefetch" href="/assets/2024-06-30-_python_ Python日志记录库loguru使用指北.html-Bhdewvqs.js" as="script"><link rel="prefetch" href="/assets/2024-07-30-_python_ 启发式算法库scikit-opt使用指北.html-BJiYN70P.js" as="script"><link rel="prefetch" href="/assets/2024-08-10-_python_ Python并行计算库Joblib使用指北.html-pdEzfxAz.js" as="script"><link rel="prefetch" href="/assets/2024-10-01-_python_ 基于PyOD库实现数据异常检测.html-D7KgyEOO.js" as="script"><link rel="prefetch" href="/assets/2024-11-22-_python_ Python异步编程库asyncio使用指北.html-cRqpHsgc.js" as="script"><link rel="prefetch" href="/assets/2024-11-25-_python_ asyncio库常见问题与实践案例.html-C73mr33Z.js" as="script"><link rel="prefetch" href="/assets/2025-03-24-_python_ 使用Python实现Markdown文档格式转换.html-BZzR71uj.js" as="script"><link rel="prefetch" href="/assets/2025-04-27-_python_ 基于WatchDog库实现文件系统监控.html-BMJRtBVj.js" as="script"><link rel="prefetch" href="/assets/2025-05-20-_python_ 轻量级定时任务调度库schedule使用指北.html-BtCtxetb.js" as="script"><link rel="prefetch" href="/assets/2025-06-03-_python_ python抽象基类使用总结.html-D_UtvjXS.js" as="script"><link rel="prefetch" href="/assets/2025-10-24-_python_ 代码性能分析工具line_profiler使用指北.html-BmX9mI9u.js" as="script"><link rel="prefetch" href="/assets/2025-11-25-_python_ Python数据类使用指北.html-uuvmgeK3.js" as="script"><link rel="prefetch" href="/assets/2019-05-28-_seaborn_ seaborn学习笔记0 seaborn学习笔记章节.html-DsJWGxWv.js" as="script"><link rel="prefetch" href="/assets/2019-05-29-_seaborn_ seaborn学习笔记1 箱形图Boxplot.html-D_UuUS21.js" as="script"><link rel="prefetch" href="/assets/2019-05-30-_seaborn_ seaborn学习笔记2 散点图Scatterplot.html-BK6AdnuA.js" as="script"><link rel="prefetch" href="/assets/2019-05-30-_seaborn_ seaborn学习笔记3 直方图Histogramplot.html-D1x_uOoJ.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记4 核密度图DENSITYPLOT.html-Bjk29bhu.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记5 小提琴图VIOLINPLOT.html-6GD4vuEV.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记6 热图HEATMAPPLOT.html-MZN7SmZw.js" as="script"><link rel="prefetch" href="/assets/2019-06-01-_seaborn_ seaborn学习笔记7 常用参数调整Adjustment of Common Parameters.html-OemKejUi.js" as="script"><link rel="prefetch" href="/assets/2019-06-01-_seaborn_ seaborn学习笔记8 避免过度绘图Avoid Overplotting.html-C1caDpdU.js" as="script"><link rel="prefetch" href="/assets/2019-06-05-_seaborn_ seaborn学习笔记10 绘图实例(2) Drawing example(2).html-BI2ro3lt.js" as="script"><link rel="prefetch" href="/assets/2019-06-05-_seaborn_ seaborn学习笔记9 绘图实例(1) Drawing example(1).html-kERpDthp.js" as="script"><link rel="prefetch" href="/assets/2019-06-06-_seaborn_ seaborn学习笔记11 绘图实例(3) Drawing example(3).html-CfROPquZ.js" as="script"><link rel="prefetch" href="/assets/2019-06-06-_seaborn_ seaborn学习笔记12 绘图实例(4) Drawing example(4).html-Ctl26xLP.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记1—ggplot2简要教程.html-CV5WIJKv.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记2—通用教程ggplot2简介.html-DUce9mtK.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记3—通用教程如何自定义ggplot2.html-BHhHPfZM.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记4—前50个ggplot2可视化效果.html-B1ET5ZJ0.js" as="script"><link rel="prefetch" href="/assets/index.html-Bx6_TJfK.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_R语言_ R语言PCA分析教程 Principal Component Methods in R.html-CQQhFTPO.js" as="script"><link rel="prefetch" href="/assets/2020-01-10-_R语言_ WGCNA入门教程.html-CPv70a0p.js" as="script"><link rel="prefetch" href="/assets/2021-02-05-_R语言_ R语言快速入门教程.html-2IEnC0nK.js" as="script"><link rel="prefetch" href="/assets/2020-09-05-_R语言_ 基于R语言实现树形图的绘制.html-D-Tb3E37.js" as="script"><link rel="prefetch" href="/assets/2020-09-05-_R语言_ 基于R语言实现环状条形图的绘制.html-BmEcoHI5.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门1.html-pDeTFJty.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门2.html-DzcP8pFr.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门3.html-BNQD5H0Y.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门4.html-Da1tOTGZ.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门5.html-1mpAgyPl.js" as="script"><link rel="prefetch" href="/assets/index.html-BfsFmZmC.js" as="script"><link rel="prefetch" href="/assets/2021-11-21-_数据与分析可视化_ D3入门教程1-d3基础知识.html-DckmaTE_.js" as="script"><link rel="prefetch" href="/assets/2021-11-28-_数据与分析可视化_ D3入门教程2-在d3中构建形状.html-CGcvt-kG.js" as="script"><link rel="prefetch" href="/assets/2021-12-05-_数据与分析可视化_ D3入门教程3-d3中的数据操作.html-CzTLDw5H.js" as="script"><link rel="prefetch" href="/assets/2023-03-16-_数据分析与可视化_ Python绘制数据地图1-GeoPandas入门指北.html-GoTxGCWN.js" as="script"><link rel="prefetch" href="/assets/2023-04-09-_数据分析与可视化_ Python绘制数据地图2-GeoPandas地图可视化.html-D3E90YQw.js" as="script"><link rel="prefetch" href="/assets/2023-06-16-_数据分析与可视化_ Python绘制数据地图3-GeoPandas使用要点.html-Di-Jhljz.js" as="script"><link rel="prefetch" href="/assets/2023-08-03-_数据分析与可视化_ Python绘制数据地图4-MovingPandas入门指北.html-Cdz5l_eE.js" as="script"><link rel="prefetch" href="/assets/2023-08-11-_数据分析与可视化_ Python绘制数据地图5-MovingPandas绘图实例.html-CSbD3bHf.js" as="script"><link rel="prefetch" href="/assets/2023-06-28-_数据分析与可视化_ 基于matplotlib-scalebar库绘制比例尺.html-Bia0O4we.js" as="script"><link rel="prefetch" href="/assets/2023-07-10-_数据分析与可视化_ 基于matplotlib和plottable库绘制精美表格.html-CleztId4.js" as="script"><link rel="prefetch" href="/assets/2023-10-24-_数据分析与可视化_ 基于Python绘制简单动图.html-DPdaS8a3.js" as="script"><link rel="prefetch" href="/assets/2021-11-14-_数据分析与可视化_ 数据绘图要点1-注重数据排序.html-CROJiNNj.js" as="script"><link rel="prefetch" href="/assets/2021-11-18-_数据分析与可视化_ 数据绘图要点2-Y轴的开始与结束.html-C7KfsHxd.js" as="script"><link rel="prefetch" href="/assets/2021-11-24-_数据分析与可视化_ 数据绘图要点3-意大利面条图.html-X4KEWFRB.js" as="script"><link rel="prefetch" href="/assets/2021-12-01-_数据分析与可视化_ 数据绘图要点4-饼图的问题.html-DZMq12ma.js" as="script"><link rel="prefetch" href="/assets/2021-12-09-_数据分析与可视化_ 数据绘图要点5-误差线的问题.html-CYtDzBbw.js" as="script"><link rel="prefetch" href="/assets/2021-12-19-_数据分析与可视化_ 数据绘图要点6-数据组过多.html-B7O8ei5R.js" as="script"><link rel="prefetch" href="/assets/2021-12-25-_数据分析与可视化_ 数据绘图要点7-过度绘图.html-B55Peuv2.js" as="script"><link rel="prefetch" href="/assets/2021-12-29-_数据分析与可视化_ 数据绘图要点8-环状条形图的使用.html-BXKHQ34r.js" as="script"><link rel="prefetch" href="/assets/2022-01-01-_数据分析与可视化_ 数据绘图要点9-颜色的选择.html-Kxggh-Qt.js" as="script"><link rel="prefetch" href="/assets/2022-01-06-_数据分析与可视化_ 数据绘图要点10-图例的构建.html-DX0nd8Ml.js" as="script"><link rel="prefetch" href="/assets/2022-01-12-_数据分析与可视化_ 数据绘图要点11-雷达图的注意事项.html-HTGdFMQs.js" as="script"><link rel="prefetch" href="/assets/2022-01-18-_数据分析与可视化_ 数据绘图要点12-图表注释的重要性.html-BUjkXVhL.js" as="script"><link rel="prefetch" href="/assets/2017-12-10-_机器学习_ 集成学习简单投票法概率.html-DksbzHVo.js" as="script"><link rel="prefetch" href="/assets/2018-04-17-_机器学习_ sklearn聚类.html-CyD_HYXt.js" as="script"><link rel="prefetch" href="/assets/2018-04-19-_机器学习_ sklearn决策树、随机森林、隐马尔可夫模型.html-BPugFCS0.js" as="script"><link rel="prefetch" href="/assets/2018-04-19-_机器学习_ sklearn朴素贝叶斯算法.html-CSw7B2Un.js" as="script"><link rel="prefetch" href="/assets/2018-04-21-_机器学习_ sklearn支持向量机.html-tnYirz95.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记1-快速入门.html-BTYveotE.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记2-模型选择.html-BxT6G1ig.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记3-特征分析可视化.html-BqtW6VfH.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记4-目标可视化文件.html-BfDBHiZP.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记5-回归可视化.html-sBEdzGdt.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记6-分类可视化.html-BET1XU2i.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记7-聚类可视化.html-DgnujV06.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记8-模型选择可视化.html-BfJOBk48.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记1-删除低方差的特征.html-hxZuTsIx.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记2-单变量特征选择.html-DdDcGEbA.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记3-递归式特征消除.html-Ds17Xg5Z.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记4-使用SelectFromModel特征选择.html-Dr_l3Ion.js" as="script"><link rel="prefetch" href="/assets/2024-12-31-_深度学习_ 大模型学习1-大语言模型基础知识.html-BRi3URoX.js" as="script"><link rel="prefetch" href="/assets/2025-02-28-_深度学习_ 大模型学习2-提示词工程指北.html-CWemIyNw.js" as="script"><link rel="prefetch" href="/assets/2025-07-21-_深度学习_ 大模型学习3上-模型训练与微调.html-B6vmem6W.js" as="script"><link rel="prefetch" href="/assets/2025-07-23-_深度学习_ 大模型学习3下-模型训练与微调.html-5iwxlVfh.js" as="script"><link rel="prefetch" href="/assets/2025-08-08-_深度学习_ 大模型学习4-RAG技术全景解析.html-BnaSwphr.js" as="script"><link rel="prefetch" href="/assets/2017-10-11-_深度学习_ 网易云课堂深度学习工程师微专业相关资料.html-7VY67hBS.js" as="script"><link rel="prefetch" href="/assets/2017-10-12-_深度学习_ 深度学习快速入门资料.html-COYjjINJ.js" as="script"><link rel="prefetch" href="/assets/2017-10-13-_深度学习_ 卷积神经网络快速入门.html-BU06mCTf.js" as="script"><link rel="prefetch" href="/assets/2017-11-25-_深度学习_ 深度学习中卷积操作和数学中卷积操作的异同.html-CHBikvA7.js" as="script"><link rel="prefetch" href="/assets/2018-07-17-_深度学习_ tf.keras入门1-基本函数介绍.html-DyJBvaa9.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门2-分类.html-zS1HiCr4.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门3-回归.html-CldJZdwQ.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门4-过拟合和欠拟合.html-pbt4V8zE.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门5-模型保存和载入.html-BBJaiNDv.js" as="script"><link rel="prefetch" href="/assets/2018-07-27-_深度学习_ 经典深度学习模型及其微调（Caffe)总结.html-CGHq6Xe-.js" as="script"><link rel="prefetch" href="/assets/2019-07-23-_深度学习_ ncnn安装和调用基础教程.html-DSbISYUZ.js" as="script"><link rel="prefetch" href="/assets/2019-08-10-_深度学习_ caffe分类模型训练、结果可视化、部署及量化笔记.html-BMNjkYN-.js" as="script"><link rel="prefetch" href="/assets/2019-09-30-_深度学习_ ncnn编译使用.html-O3y7cWm6.js" as="script"><link rel="prefetch" href="/assets/2020-08-07-_深度学习_ ImageAI库使用笔记.html-CE4lmfro.js" as="script"><link rel="prefetch" href="/assets/2020-10-24-_深度学习_ imgaug库使用笔记.html-CPAZJcHf.js" as="script"><link rel="prefetch" href="/assets/2020-11-19-_深度学习_ 深度学习优化器选择学习笔记.html-B8OqLBZQ.js" as="script"><link rel="prefetch" href="/assets/2020-12-09-_深度学习_ Pytorch模型转换为onnx模型笔记.html-DwFN1KZo.js" as="script"><link rel="prefetch" href="/assets/2021-01-14-_深度学习_ ubuntu18.04配置深度学习环境笔记.html-DUToTd6h.js" as="script"><link rel="prefetch" href="/assets/2021-02-02-_深度学习_ imgaug边界框增强笔记.html-ClbNL7dU.js" as="script"><link rel="prefetch" href="/assets/2021-06-09-_深度学习_ CCPD车牌数据集介绍.html-2HZe9p0n.js" as="script"><link rel="prefetch" href="/assets/2022-01-14-_深度学习_ fast-reid入门教程.html-BGltTaz5.js" as="script"><link rel="prefetch" href="/assets/2022-02-26-_深度学习_ Python人脸识别库face_recognition使用教程.html-CuUkjgeZ.js" as="script"><link rel="prefetch" href="/assets/2022-07-02-_深度学习_ Python人脸识别库Deepface使用教程.html-PMzQvBTo.js" as="script"><link rel="prefetch" href="/assets/2022-11-24-_深度学习_ 搭建行人重识别系统心得.html-Dv38JcDx.js" as="script"><link rel="prefetch" href="/assets/2023-01-03-_深度学习_ 基于切片辅助超推理库SAHI优化小目标识别.html-CTo1O_5x.js" as="script"><link rel="prefetch" href="/assets/2024-03-18-_深度学习_ 计算机视觉低代码工具Supervision库使用指北.html-BWHi3geS.js" as="script"><link rel="prefetch" href="/assets/2024-08-28-_深度学习_ 时间序列分析工具TSLiB库使用指北.html-CXdi_REG.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门1-创建线程的三种不同方式.html-DbtitjWO.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门10-packaged_task示例.html-CQ62EWdz.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门2-连接和分离线程.html-DNbwbcET.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门3-小心地将参数传递给线程.html-Dpm3t5Qv.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门4-数据共享和资源竞争.html-Yazk1q0D.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门5-使用互斥锁解决资源竞争.html-1j517CGK.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门6-事件处理的需求.html-B276SA80.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门7-条件变量介绍.html-DvhyIyvF.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门8-从线程返回值.html-C3LSUF2s.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门9-async教程和示例.html-CAz7lp21.js" as="script"><link rel="prefetch" href="/assets/2021-10-24-_编程基础_ 常用html标签使用介绍.html-DsSXZLpe.js" as="script"><link rel="prefetch" href="/assets/2017-11-02-_编程基础_ C_自定义类调用窗体控件.html-Cu-1M8Bm.js" as="script"><link rel="prefetch" href="/assets/2020-05-09-_编程基础_ C和C__内置宏说明.html-DgBFFnO5.js" as="script"><link rel="prefetch" href="/assets/2020-06-17-_编程基础_ Python格式化字符串常量f-string总结.html-DZdlRmrG.js" as="script"><link rel="prefetch" href="/assets/2020-06-20-_编程基础_ Python谷歌翻译库googletrans总结.html-Z_trsvxH.js" as="script"><link rel="prefetch" href="/assets/2020-06-21-_编程基础_ Python数据生成库Faker总结.html-BM53WX7U.js" as="script"><link rel="prefetch" href="/assets/2020-06-21-_编程基础_ Python配置文件读取库ConfigParser总结.html-ZLreYVSF.js" as="script"><link rel="prefetch" href="/assets/2020-06-23-_编程基础_ Python日志记录库logging总结.html-B72PoUBf.js" as="script"><link rel="prefetch" href="/assets/2020-06-24-_编程基础_ Python随机数生成模块总结.html-DhHy_Osn.js" as="script"><link rel="prefetch" href="/assets/2020-06-25-_编程基础_ Python lambda函数总结.html-BYMazlom.js" as="script"><link rel="prefetch" href="/assets/2020-06-25-_编程基础_ Python装饰器入门总结.html-DxOl2cRD.js" as="script"><link rel="prefetch" href="/assets/2020-06-26-_编程基础_ Python列表解析总结.html-DixNsPKt.js" as="script"><link rel="prefetch" href="/assets/2020-08-01-_编程基础_ Python中的绝对导入与相对导入.html-DhpMhqTZ.js" as="script"><link rel="prefetch" href="/assets/2020-08-01-_编程基础_ Python模块和包使用笔记.html-DddJzA6W.js" as="script"><link rel="prefetch" href="/assets/2020-08-02-_编程基础_ Python对象的浅拷贝与深拷贝笔记.html-ClUY2kB4.js" as="script"><link rel="prefetch" href="/assets/2020-10-14-_编程基础_ Python中args和kwargs参数的使用.html-ClLIGnfK.js" as="script"><link rel="prefetch" href="/assets/2020-10-31-_编程基础_ Python命令行解析库argparse学习笔记.html-D24HlK3g.js" as="script"><link rel="prefetch" href="/assets/2021-08-16-_编程基础_ Python字符串替换笔记.html-30l8PNPp.js" as="script"><link rel="prefetch" href="/assets/2023-09-05-_编程基础_ Python内置模块collections使用笔记.html-BJ3kCrnd.js" as="script"><link rel="prefetch" href="/assets/404.html-BXFVvhe4.js" as="script"><link rel="prefetch" href="/assets/index.html-D5ad4Lfb.js" as="script"><link rel="prefetch" href="/assets/index.html-BeMrDVkI.js" as="script"><link rel="prefetch" href="/assets/index.html-ChYi_vNZ.js" as="script"><link rel="prefetch" href="/assets/index.html-O16vsj2E.js" as="script"><link rel="prefetch" href="/assets/index.html-wnKZfxgR.js" as="script"><link rel="prefetch" href="/assets/index.html-D7240Pt8.js" as="script"><link rel="prefetch" href="/assets/index.html-1Zymv3I5.js" as="script"><link rel="prefetch" href="/assets/index.html-B6LSNS7L.js" as="script"><link rel="prefetch" href="/assets/index.html-NRvaHDwM.js" as="script"><link rel="prefetch" href="/assets/index.html-DugxXWYm.js" as="script"><link rel="prefetch" href="/assets/index.html-CLnBQDZp.js" as="script"><link rel="prefetch" href="/assets/index.html-Dr46KbJ5.js" as="script"><link rel="prefetch" href="/assets/index.html-CBNl0VHU.js" as="script"><link rel="prefetch" href="/assets/index.html-Cf12Ladl.js" as="script"><link rel="prefetch" href="/assets/index.html-B8RHt7a7.js" as="script"><link rel="prefetch" href="/assets/index.html-WfdXeYFT.js" as="script"><link rel="prefetch" href="/assets/index.html-Mx23uBwJ.js" as="script"><link rel="prefetch" href="/assets/index.html-DHP3RK5d.js" as="script"><link rel="prefetch" href="/assets/index.html-D669xmdp.js" as="script"><link rel="prefetch" href="/assets/index.html-C3ty5Ona.js" as="script"><link rel="prefetch" href="/assets/index.html-DmsD_Yug.js" as="script"><link rel="prefetch" href="/assets/index.html-CBDdosow.js" as="script"><link rel="prefetch" href="/assets/index.html-CxrciUx-.js" as="script"><link rel="prefetch" href="/assets/index.html-Cv80Sry1.js" as="script"><link rel="prefetch" href="/assets/index.html-Ca9W_PWk.js" as="script"><link rel="prefetch" href="/assets/index.html-XC0Dc2w9.js" as="script"><link rel="prefetch" href="/assets/index.html-C20anzbi.js" as="script"><link rel="prefetch" href="/assets/index.html-BQQ1R1s_.js" as="script"><link rel="prefetch" href="/assets/index.html-BWVHiYmx.js" as="script"><link rel="prefetch" href="/assets/index.html-Df0QTn6X.js" as="script"><link rel="prefetch" href="/assets/index.html-D22xXGU8.js" as="script"><link rel="prefetch" href="/assets/index.html-Br_M2kwT.js" as="script"><link rel="prefetch" href="/assets/index.html-gPhgKCZM.js" as="script"><link rel="prefetch" href="/assets/index.html-BzrC1XGc.js" as="script"><link rel="prefetch" href="/assets/index.html-BN5zi7Mz.js" as="script"><link rel="prefetch" href="/assets/index.html-DFJLp68S.js" as="script"><link rel="prefetch" href="/assets/index.html-Dxebunfe.js" as="script"><link rel="prefetch" href="/assets/index.html-n4amN4kJ.js" as="script"><link rel="prefetch" href="/assets/index.html-bOsCaubx.js" as="script"><link rel="prefetch" href="/assets/index.html-BkLE_19k.js" as="script"><link rel="prefetch" href="/assets/index.html-DfnQEIWJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BzTgPaJr.js" as="script"><link rel="prefetch" href="/assets/index.html-BdlAAUbY.js" as="script"><link rel="prefetch" href="/assets/index.html-ClvIrUbQ.js" as="script"><link rel="prefetch" href="/assets/index.html-h6ztc3BB.js" as="script"><link rel="prefetch" href="/assets/index.html-DutVI6N8.js" as="script"><link rel="prefetch" href="/assets/index.html-AQKlCbKd.js" as="script"><link rel="prefetch" href="/assets/index.html-CZLcN8nP.js" as="script"><link rel="prefetch" href="/assets/index.html-C67H_iFK.js" as="script"><link rel="prefetch" href="/assets/index.html-Mzlwevkm.js" as="script"><link rel="prefetch" href="/assets/index.html-BgQDYcXx.js" as="script"><link rel="prefetch" href="/assets/index.html-BYZyZUPg.js" as="script"><link rel="prefetch" href="/assets/index.html-B45vSzLB.js" as="script"><link rel="prefetch" href="/assets/index.html-BN8pehxr.js" as="script"><link rel="prefetch" href="/assets/index.html-tjNrmf1w.js" as="script"><link rel="prefetch" href="/assets/index.html-B3e9TzN_.js" as="script"><link rel="prefetch" href="/assets/index.html-D22Be194.js" as="script"><link rel="prefetch" href="/assets/index.html-BncAZ3c_.js" as="script"><link rel="prefetch" href="/assets/index.html-C4Oa1A1B.js" as="script"><link rel="prefetch" href="/assets/index.html-CopTnxwi.js" as="script"><link rel="prefetch" href="/assets/index.html-BDNTNZqX.js" as="script"><link rel="prefetch" href="/assets/index.html-DpbNyxgF.js" as="script"><link rel="prefetch" href="/assets/index.html-DqpmXne6.js" as="script"><link rel="prefetch" href="/assets/index.html-D5hV7nKl.js" as="script"><link rel="prefetch" href="/assets/index.html-Hrx6frA1.js" as="script"><link rel="prefetch" href="/assets/index.html-zdkRCTKC.js" as="script"><link rel="prefetch" href="/assets/index.html-DbUmSDrX.js" as="script"><link rel="prefetch" href="/assets/index.html-B8QKy1I-.js" as="script"><link rel="prefetch" href="/assets/index.html-DIcdfIc3.js" as="script"><link rel="prefetch" href="/assets/index.html-CH3uqTTY.js" as="script"><link rel="prefetch" href="/assets/index.html-B8J_krAi.js" as="script"><link rel="prefetch" href="/assets/index.html-DDrJ-jjp.js" as="script"><link rel="prefetch" href="/assets/flowchart-CAFN9Lqb.js" as="script"><link rel="prefetch" href="/assets/mermaid.esm.min-D_bQPAHy.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CMg0yb1C.js" as="script"><link rel="prefetch" href="/assets/browser-CYdOP0d3.js" as="script"><link rel="prefetch" href="/assets/SearchResult-CPaRil2p.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-7eocv5M2.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="博客主页" iconsizing="height"><!--[--><i class="vp-icon fas fa-home" sizing="height"></i><!--]-->博客主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/blog/" aria-label="个人博客" iconsizing="height"><!--[--><i class="vp-icon fas fa-blog" sizing="height"></i><!--]-->个人博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="关于"><!--[--><i class="vp-icon fas fa-user-plus" sizing="height"></i>关于<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/intro.html" aria-label="关于我" iconsizing="both"><!--[--><i class="vp-icon fas fa-user fa-fw" sizing="both"></i><!--]-->关于我<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/about.html" aria-label="关于本站" iconsizing="both"><!--[--><i class="vp-icon fas fa-circle-info fa-fw" sizing="both"></i><!--]-->关于本站<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-center"><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://blog.csdn.net/LuohenYJ" target="_blank" title="开往我的csdn" rel="noopener noreferrer" aria-label="travelling"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" fill="currentColor" style="width:1.25rem;height:1.25rem;vertical-align:middle"><path d="M658.836 519.32c-22.121 0-40.145 18.431-40.145 40.957 0 22.528 18.023 40.962 40.145 40.962 22.117 0 40.141-18.434 40.141-40.962 0-22.526-18.024-40.957-40.141-40.957zM364.742 519.32c-22.121 0-40.141 18.431-40.141 40.957 0.41 22.528 18.02 40.962 40.141 40.962 22.117 0 40.141-18.434 40.141-40.962 0-22.526-18.024-40.957-40.141-40.957z" p-id="8700"></path><path d="M512 0C229.23 0 0 229.23 0 512s229.23 512 512 512 512-229.23 512-512S794.77 0 512 0z m133.727 804.81c0 7.375-6.145 13.52-13.516 13.52H391.773c-7.371 0-13.515-6.145-13.515-13.52v-13.516c0-7.371 6.144-13.517 13.515-13.517h240.438c7.371 0 13.516 6.146 13.516 13.517v13.516z m120.832 37.273c-12.289 6.965-27.441 2.867-34.406-9.418l-54.887-96.668c-4.504 0.82-9.422 1.23-13.926 1.23H361.054c-4.914 0-9.421-0.41-13.925-1.23l-54.887 96.668c-6.965 12.285-22.527 16.383-34.406 9.418-12.289-6.961-16.383-22.938-9.422-35.223l51.199-90.113c-27.031-19.66-43.418-52.43-40.957-88.883l19.25-293.273c3.277-49.152 34.406-88.066 87.246-88.066h80.281c0-37.684 29.899-67.993 66.762-67.993 36.868 0 66.766 30.309 66.766 67.993h93.391c53.246 0 70.449 38.914 73.727 88.066l19.25 293.273c2.461 36.453-13.926 69.223-40.957 88.883l51.199 90.113c6.964 12.696 2.867 28.262-9.012 35.223z" p-id="8701"></path><path d="M672.352 314.931H351.633c-14.747 0-26.622 12.285-26.622 27.441v108.953c0 15.157 11.875 27.446 26.622 27.446h320.719c14.746 0 26.625-12.289 26.625-27.446V342.372c0-15.156-11.879-27.441-26.625-27.441z" p-id="8702"></svg></a></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/luohenyueji" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--></div><div class="vp-navbar-end"><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/logo.png" alt><!----><span class="vp-site-name hide-in-pad">落痕月极的博客</span></a><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="博客主页" iconsizing="both"><!--[--><i class="vp-icon fas fa-home fa-fw" sizing="both"></i><!--]-->博客主页<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><i class="vp-icon fas fa-blog fa-fw" sizing="both"></i><span class="vp-sidebar-title">博客</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">编程基础</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">常用工具</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">机器学习</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">讲座论坛</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">论文总结</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">能源化工与仪器科学</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">深度学习</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">大模型</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2024-12-31-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" aria-label="[深度学习] 大模型学习1-大语言模型基础知识" iconsizing="both"><!---->[深度学习] 大模型学习1-大语言模型基础知识<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-02-28-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97.html" aria-label="[深度学习] 大模型学习2-提示词工程指北" iconsizing="both"><!---->[深度学习] 大模型学习2-提示词工程指北<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-21-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8A-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html" aria-label="[深度学习] 大模型学习3上-模型训练与微调" iconsizing="both"><!---->[深度学习] 大模型学习3上-模型训练与微调<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-23-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8B-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html" aria-label="[深度学习] 大模型学习3下-模型训练与微调" iconsizing="both"><!---->[深度学习] 大模型学习3下-模型训练与微调<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-08-08-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A04-RAG%E6%8A%80%E6%9C%AF%E5%85%A8%E6%99%AF%E8%A7%A3%E6%9E%90.html" aria-label="[深度学习] 大模型学习4-RAG技术全景解析" iconsizing="both"><!---->[深度学习] 大模型学习4-RAG技术全景解析<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-10-01-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97.html" aria-label="[深度学习] 大模型学习5-高效微调框架Unsloth使用指北" iconsizing="both"><!---->[深度学习] 大模型学习5-高效微调框架Unsloth使用指北<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">深度学习笔记</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">数据分析与可视化</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">数学理论</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">随笔所想</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">图像处理</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">音视频处理</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">自然语言处理与语音识别</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">OpenCV</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">R语言</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->[深度学习] 大模型学习5-高效微调框架Unsloth使用指北</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="/" target="_blank" rel="noopener noreferrer">落痕月极</a></span><span property="author" content="落痕月极"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年10月1日</span><meta property="datePublished" content="2025-10-01T11:18:01.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 36 分钟</span><meta property="timeRequired" content="PT36M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color4 clickable" role="navigation">深度学习</span><!--]--><meta property="articleSection" content="深度学习"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color4 clickable" role="navigation">深度学习</span><span class="page-tag-item color2 clickable" role="navigation">自然语言处理与语音识别</span><span class="page-tag-item color6 clickable" role="navigation">Python</span><!--]--><meta property="keywords" content="深度学习,自然语言处理与语音识别,Python"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><!--[--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_1-unsloth框架介绍">1 Unsloth框架介绍</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-1-unsloth概览">1.1 Unsloth概览</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-2-微调技术概览">1.2 微调技术概览</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-3-unsloth安装">1.3 Unsloth安装</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_2-unsloth微调教程">2 Unsloth微调教程</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-1-模型与训练方法选择">2.1 模型与训练方法选择</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-2-lora和数据集">2.2 LoRA和数据集</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-3-qwen3使用示例">2.3 Qwen3使用示例</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-4-unsloth训练qwen3教程">2.4 Unsloth训练Qwen3教程</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-参考">3 参考</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--]--><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="深度学习-大模型学习5-高效微调框架unsloth使用指北" tabindex="-1"><a class="header-anchor" href="#深度学习-大模型学习5-高效微调框架unsloth使用指北"><span>[深度学习] 大模型学习5-高效微调框架Unsloth使用指北</span></a></h1><p>Unsloth是一个专注于加速大语言模型微调过程的开源项目。它通过一系列底层优化，显著提升了微调速度并大幅降低了内存消耗，同时能保持模型性能。无论是研究者还是开发者，都能借助Unsloth更高效地定制自己的大语言模型。本文将介绍Unsloth的使用，相关学习资源如下：</p><ul><li>开源仓库：<a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer">Unsloth</a></li><li>官方文档：<a href="https://docs.unsloth.ai/" target="_blank" rel="noopener noreferrer">Unsloth Docs</a></li></ul><h2 id="_1-unsloth框架介绍" tabindex="-1"><a class="header-anchor" href="#_1-unsloth框架介绍"><span>1 Unsloth框架介绍</span></a></h2><h3 id="_1-1-unsloth概览" tabindex="-1"><a class="header-anchor" href="#_1-1-unsloth概览"><span>1.1 Unsloth概览</span></a></h3><p>Unsloth是一款专为大语言模型微调与强化学习设计的开源框架，致力于以更高的效率和更低的资源成本推动人工智能技术的普及。用户可在本地环境、Google Colab、Kaggle等平台上，借助其运算加速与显存优化能力，轻松完成Qwen、DeepSeek等主流大模型的训练、评估、保存及推理优化。</p><p>传统大语言模型微调往往面临硬件要求高、迭代速度慢和资源受限等挑战，而Unsloth通过高效的底层实现和友好的接口设计，显著降低了微调的技术门槛，使更多人能够高效、低成本地训练属于自己的定制模型。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img1.jpg" alt="https://www.codemajin.net/fine-tuning-llm-with-unsloth/" tabindex="0" loading="lazy"><figcaption>https://www.codemajin.net/fine-tuning-llm-with-unsloth/</figcaption></figure><p><strong>核心优势</strong></p><table><thead><tr><th>特点</th><th>说明</th><th>适用场景/用户</th></tr></thead><tbody><tr><td>🚀极致速度</td><td>相比Hugging Face，Unsloth训练模型更快</td><td>需快速实验与迭代的研发场景</td></tr><tr><td>💾省内存</td><td>减少GPU显存占用</td><td>注重成本控制的用户</td></tr><tr><td>✅无损精度</td><td>无需依赖近似计算</td><td>对精度要求极高的任务</td></tr><tr><td>🔗广泛兼容</td><td>支持主流Transformer类模型（涵盖多模态、语音、文本及扩散模型）；支持全量微调、预训练及4/8/16位精度训练；兼容Linux、Windows及主流云平台</td><td>使用多种架构的团队</td></tr><tr><td>🧩易于使用</td><td>提供简洁API，兼容Hugging Face生态，可导出GGUF、Ollama等格式</td><td>初学者、资源有限的小型团队</td></tr><tr><td>⚡高效推理</td><td>支持INT4量化（QLoRA），推理阶段同步提速</td><td>需兼顾微调与推理效率的应用场景</td></tr><tr><td>💡低成本</td><td>单张GPU（如4090或8GB显存卡）即可微调10B+参数模型</td><td>个人开发者</td></tr><tr><td>🔧高效计算</td><td>基于Triton（OpenAI开源的高性能GPU编程语言）实现高效计算</td><td>对技术底层效率有要求的开发团队</td></tr></tbody></table><p>目前，Unsloth支持借助Accelerate、DeepSpeed等库实现多GPU训练，但实际配置过程较复杂，需手动完成设置，相关训练教程可参考：<a href="https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth" target="_blank" rel="noopener noreferrer">Multi-GPU-Unsloth</a>，Unsloth团队正积极优化多GPU训练功能。</p><p><strong>使用建议</strong></p><p>Unsloth与Meta、Google、Microsoft、Mistral、Qwen等主流模型团队深度合作，持续修复关键漏洞，提升框架的准确性与稳定性。该框架支持用户灵活调整聊天模板和数据集格式，并提供涵盖视觉模型、TTS、BERT、强化学习等多样化示例Notebook，助力用户快速上手，详情可参考：<a href="https://docs.unsloth.ai/get-started/unsloth-notebooks" target="_blank" rel="noopener noreferrer">Unsloth Notebooks</a>。快速上手建议：</p><ul><li>从QLoRA起步：4-bit量化是资源有限用户的理想选择；</li><li>调整关键参数：如LoRA秩（<code>r</code>）和<code>alpha</code>，建议从小值（如16）开始尝试，以平衡模型能力与过拟合风险；</li><li>监控训练过程：密切关注损失曲线，借助Unsloth的快速迭代优势积极调参；</li><li>利用社区资源：通过Discord聊天社区等渠道获取帮助、交流经验。</li></ul><h3 id="_1-2-微调技术概览" tabindex="-1"><a class="header-anchor" href="#_1-2-微调技术概览"><span>1.2 微调技术概览</span></a></h3><p><strong>什么是微调？</strong></p><p>微调（Fine-tuning）是一种基于预训练大语言模型、利用特定领域数据进一步训练的技术，其核心目标提升模型在特定场景下的性能表现。该技术主要包括两个层面：一是对预训练模型进行持续的无监督预训练；二是指令微调（SFT），即引导模型学习如何根据指令调用已有知识，完成特定格式的任务或匹配特定风格。通过微调，通用大模型能够逐步转化为专业化的领域专家。与检索增强生成（RAG）不同，微调将知识直接内化至模型参数中，实现更深层次的能力融合。本文聚焦于大语言模型的指令微调。</p><p>那么，为什么要进行微调？</p><ol><li>知识增强：向模型注入领域新知识，扩展其认知边界</li><li>行为定制：调整模型的输出风格、语气及响应方式</li><li>性能优化：提升模型在特定任务上的准确性、相关性和可靠性</li></ol><p>利用Unsloth实现完整指令微调训练的教程见： <a href="https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms" target="_blank" rel="noopener noreferrer">How To Fine-tune &amp; Run LLMs</a>。</p><p><strong>微调常见问题</strong></p><ul><li><p>微调能否增加新知识？<br> 可以。只要训练数据中包含新信息，模型就能有效学习并掌握新的知识或模式。</p></li><li><p>RAG是否一定优于微调？<br> 并非如此。经过良好优化的微调模型在特定任务上可以媲美甚至超越RAG系统。借助如Unsloth等高效训练工具，微调的技术门槛也显著降低。</p></li><li><p>微调成本是否很高？<br> 并非必然。采用LoRA/QLoRA等参数高效微调方法，结合免费或低成本的算力资源，完全能够实现低成本甚至零成本的微调。</p></li><li><p>微调如何与其他技术结合？<br> 微调与RAG具有互补优势：微调赋予模型领域基础能力，RAG则提供实时外部知识，兼顾专业性与时效性。此外，强化学习（RL）也可在微调后通过奖励机制进一步优化模型表现。</p></li></ul><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img2.jpg" alt="https://medium.com/decodingml/8b-parameters-1-gpu-no-problems-the-ultimate-llm-fine-tuning-pipeline-f68ef6c359c2" tabindex="0" loading="lazy"><figcaption>https://medium.com/decodingml/8b-parameters-1-gpu-no-problems-the-ultimate-llm-fine-tuning-pipeline-f68ef6c359c2</figcaption></figure><h3 id="_1-3-unsloth安装" tabindex="-1"><a class="header-anchor" href="#_1-3-unsloth安装"><span>1.3 Unsloth安装</span></a></h3><p><strong>Unsloth安装命令</strong></p><p>Unsloth可直接在Linux、Windows、Google Colab等系统上运行，直接安装命令如下：</p><blockquote><p>pip install unsloth</p></blockquote><p><strong>系统要求</strong></p><ul><li>操作系统：支持Linux与Windows</li><li>显卡： <ul><li>兼容2018年及之后发布的NVIDIA显卡</li><li>需至少支持CUDA 7.0，例如V100、T4、Titan V、RTX 20/30/40系列、A100、H100、L40等</li><li>GTX 1070/1080可运行，但性能较慢</li><li>支持AMD与Intel的CPU，Apple Silicon版本目前仍在开发中</li></ul></li><li>软件兼容：安装Unsloth时将自动更新已有环境中的torch、transformers等库至最新版本，无需手动处理版本冲突</li><li>依赖项：需安装xformers、torch、BitsandBytes及triton</li></ul><p><strong>微调显存要求</strong></p><p>在使用Unsloth对大语言模型进行微调时，出现内存不足错误通常是由于批处理大小设置过高。将批处理大小调整为1、2或3可有效降低显存占用。</p><p>下表列出了不同参数规模与微调方法下的显存需求，其中QLoRA使用4位精度，LoRA使用16位精度。所列数据为理论最低值，部分模型实际可能需要更多显存。详见：<a href="https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements" target="_blank" rel="noopener noreferrer">Unsloth-requirements</a>。</p><table><thead><tr><th>模型参数</th><th>QLoRA（4位）显存</th><th>LoRA（16位）显存</th></tr></thead><tbody><tr><td>3B</td><td>3.5GB</td><td>8GB</td></tr><tr><td>7B</td><td>5GB</td><td>19GB</td></tr><tr><td>8B</td><td>6GB</td><td>22GB</td></tr><tr><td>9B</td><td>6.5GB</td><td>24GB</td></tr><tr><td>11B</td><td>7.5GB</td><td>29GB</td></tr><tr><td>14B</td><td>8.5GB</td><td>33GB</td></tr><tr><td>27B</td><td>22GB</td><td>64GB</td></tr><tr><td>32B</td><td>26GB</td><td>76GB</td></tr><tr><td>40B</td><td>30GB</td><td>96GB</td></tr><tr><td>70B</td><td>41GB</td><td>164GB</td></tr><tr><td>81B</td><td>48GB</td><td>192GB</td></tr><tr><td>90B</td><td>53GB</td><td>212GB</td></tr><tr><td>405B</td><td>237GB</td><td>950GB</td></tr></tbody></table><h2 id="_2-unsloth微调教程" tabindex="-1"><a class="header-anchor" href="#_2-unsloth微调教程"><span>2 Unsloth微调教程</span></a></h2><h3 id="_2-1-模型与训练方法选择" tabindex="-1"><a class="header-anchor" href="#_2-1-模型与训练方法选择"><span>2.1 模型与训练方法选择</span></a></h3><p><strong>优先选择指令模型</strong></p><p>大语言模型主要分为基座模型（Base）和指令模型（Instruct）两类，两者均基于文本预测任务进行训练。基座模型通常仅经过预训练和少量通用指令微调；指令模型则在基座模型基础上，进一步通过大规模指令微调和人类反馈强化学习优化其理解和生成能力。常提到的对话模型（Chat Model）本质上属于指令模型。</p><p>选择基座模型还是指令模型，通常取决于数据规模、质量与类型：</p><ul><li>1000行以上数据：数据量较大时，微调基座模型效果更佳。</li><li>300–1000行高质量数据：中等规模高质量数据下，微调基座模型或指令模型均可。</li><li>300行以下数据：数据量较小时，建议选择指令模型。微调后既能适配特定任务，又可保留其内置的指令遵循能力，无需额外提示即可响应一般指令（除非需大幅改变模型行为）。</li></ul><p>推荐优先从指令模型入手，原因包括：</p><ul><li>支持直接使用ChatML、ShareGPT等对话模板进行微调，所需数据量更少；</li><li>基座模型需依赖Alpaca、Vicuna等特定模板，对数据量要求相对更高。</li></ul><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img3.jpg" alt="https://medium.com/data-science-in-your-pocket/unsloth-the-fastest-way-to-fine-tune-llms-041bb6a785ac" tabindex="0" loading="lazy"><figcaption>https://medium.com/data-science-in-your-pocket/unsloth-the-fastest-way-to-fine-tune-llms-041bb6a785ac</figcaption></figure><p><strong>Unsloth模型格式</strong></p><p>在Hugging Face中Unsloth仓库不同后缀代表模型的量化格式或优化版本，选择时可参考以下说明：</p><ul><li>名称以<code>unsloth-bnb-4bit</code> 结尾：为Unsloth动态4位量化模型。其显存占用略高于标准位量化模型，但精度显著更高。</li><li>名称仅以<code>bnb-4bit</code> 结尾（不含<code>unsloth</code>）：为标准位量化模型。</li><li>无后缀：为原始16位或8位格式。这类模型是官方发布的原始版本，但Unsloth会在部分版本中加入对话模板、分词器等重要修复。</li></ul><p>在此基础上，在准备微调时，首要决策之一就是选择合适的模型：</p><ol><li><p>选择与用例匹配的模型 例如：若进行基于图像的训练，可选择Llama 3.2 Vision等视觉模型；针对代码数据集，则适合选用Qwen Coder 2.5等专用模型。</p></li><li><p>留意授权与要求 不同模型可能有特定的授权条款和系统要求，务必仔细查看。</p></li><li><p>评估存储、计算能力和数据集 可参考Unsloth的显存指南，确定目标模型所需的显存配置。数据集的类型会影响模型的选择，同时也会决定训练所需的时间。</p></li><li><p>选定模型及参数 建议选用最新模型，以获得最佳性能和功能。可以通过浏览Unsloth的模型目录，及时了解最新且相关的选项。</p></li></ol><p>可以将模型名称修改为任意名称，只需使其与Hugging Face上Unsloth仓库的模型名称相匹配即可，例如 “unsloth/llama-3.1-8b-unsloth-bnb-4bit”。对于初学者，建议从诸如<code>unsloth/llama-3.1-8b-unsloth-bnb-4bit</code>之类的小型指令模型入手，再逐步探索更多可能性。</p><p>所有Unsloth支持的模型见：<a href="https://docs.unsloth.ai/get-started/all-our-models" target="_blank" rel="noopener noreferrer">Unsloth Models</a>。</p><p><strong>训练方法的选择：LoRA与QLoRA</strong></p><p>在实施微调时，降低计算与内存需求的主流技术主要有以下两种：</p><ul><li>LoRA（低秩适配）：仅微调少量16位的适配器权重矩阵，保持原始模型参数基本不变，从而显著减少训练过程中需要更新的参数量。</li><li>QLoRA（量化LoRA）：在LoRA基础上引入模型权重的4位量化，可在有限硬件资源下高效微调超大规模模型，通过4位精度显著降低内存占用与计算开销。</li></ul><p>建议从QLoRA入手，它是当前高效且易于使用的微调方法之一。借助如Unsloth所采用的动态4位量化技术，其精度损失相较于标准的16位LoRA微调已几乎可忽略不计。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img4.jpg" alt="https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab/" tabindex="0" loading="lazy"><figcaption>https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab/</figcaption></figure><p>已微调的模型可再次多次微调，但最佳做法是合并所有数据集一次性完成。若基于已微调模型续训，可能改变其此前获得的质量与知识。需注意，实验验证至关重要。微调无唯一最佳方法，仅有适配不同场景的最佳实践，需尝试多种方法与配置，才能找到最契合自身数据集及需求的方案。</p><h3 id="_2-2-lora和数据集" tabindex="-1"><a class="header-anchor" href="#_2-2-lora和数据集"><span>2.2 LoRA和数据集</span></a></h3><h4 id="_2-2-1-lora介绍" tabindex="-1"><a class="header-anchor" href="#_2-2-1-lora介绍"><span>2.2.1 LoRA介绍</span></a></h4><p>LoRA提供了众多超参数（如学习率、训练轮次等），其组合可能达数百万种。合理选择参数对微调至关重要，直接影响模型的准确性、稳定性与输出质量。Unsloth基于数百篇研究论文与实验经验，总结了这些参数的最佳实践，并解析了它们对模型行为的影响。虽然建议直接使用其默认配置，但理解这些概念将有助于更全面地掌控整个微调过程。</p><p>超参数调整的目标是在提升模型准确率的同时避免过拟合或欠拟合。对于大型语言模型（如Llama 70B），其权重包含数百亿参数，通常不会全部参与更新，而是采用LoRA等参数高效微调方法。LoRA在每一层旁引入两个小型矩阵A和B，仅优化这两个矩阵，实际训练参数量通常仅占总量的1%左右。通过冻结原始权重、仅更新新增的适配器参数，LoRA显著降低了计算与存储开销，同时在多数任务中保持模型性能，已成为当前大模型微调的主流方法之一。关于LoRA的详细介绍见：<a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide" target="_blank" rel="noopener noreferrer">LoRA Hyperparameters Guide</a>。</p><p>以下简要介绍相关参数：</p><p><strong>学习率</strong></p><p>定义模型训练中每一步的权重更新幅度。</p><ul><li>较高学习率：收敛快，但过高易造成训练震荡，可能错过最优解。</li><li>较低学习率：训练更稳定、精度高，但收敛慢、耗时长；虽常被认为易欠拟合，实际也可能引发过拟合或阻碍有效学习。</li><li>常用范围：2e-4（0.0002）至 5e-6（0.000005） <ul><li>LoRA/QLoRA微调：建议初始值2e-4</li><li>强化学习：推荐5e-6</li><li>全量微调：通常适用更低学习率</li></ul></li></ul><p><strong>训练次数（Epochs）</strong></p><p>指模型完整遍历训练数据集的次数。</p><ul><li>轮次过多：可能提升训练集上的表现，但也容易导致过拟合，降低模型泛化能力。</li><li>轮次过少：训练时间短且不易过拟合，但若模型未能充分学习数据规律，可能造成欠拟合。</li><li>建议：多数指令微调任务建议训练1–3轮。超过3轮后收益递减，过拟合风险显著增加。</li></ul><p><strong>超参数设置</strong></p><p>其他常用参数如下：</p><table><thead><tr><th>Hyperparameter</th><th>功能说明</th><th>推荐值</th></tr></thead><tbody><tr><td>Rank(r)</td><td>控制可训练参数数量，秩越高能力越强，内存占用越大</td><td>8,16,32,64,128（常用16或32）</td></tr><tr><td>LoRA Alpha(lora_alpha)</td><td>用于控制低秩矩阵的缩放系数</td><td>通常设为r或2r</td></tr><tr><td>LoRA Dropout(lora_dropout)</td><td>训练时随机丢弃部分激活值，防止过拟合</td><td>0（默认0.1）</td></tr><tr><td>Target Modules(target_modules)</td><td>指定添加LoRA的模型模块</td><td>q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj（推荐全部）</td></tr><tr><td>Weight Decay</td><td>抑制权重过大，提升泛化能力</td><td>0.01至0.1</td></tr><tr><td>Warmup Steps</td><td>训练初期逐步提高学习率，稳定训练</td><td>总步数的5%–10%</td></tr><tr><td>Scheduler Type</td><td>训练过程中调整学习率的方式</td><td>linear或cosine</td></tr><tr><td>Seed</td><td>固定随机数种子，保证结果可复现</td><td>任意整数（如42）</td></tr></tbody></table><p>关于LoRA超参数详细介绍可见：<a href="https://zhuanlan.zhihu.com/p/671089942" target="_blank" rel="noopener noreferrer">LoRA、QLoRA、QA-LoRA 原理笔记</a>。</p><p><strong>作用模块</strong></p><p>在QLoRA与LoRA的对比中，QLoRA采用4-bit精度，可降低超过75%的显存占用，而LoRA（16-bit）在精度和速度上略优。根据论文及实验经验，建议将LoRA同时作用于注意力层与MLP层（如<code>target_modules=[&quot;q_proj&quot;,&quot;k_proj&quot;,&quot;v_proj&quot;,&quot;o_proj&quot;,&quot;gate_proj&quot;,&quot;up_proj&quot;,&quot;down_proj&quot;]</code>），以有效提升模型精度。</p><p>下图对比了不同目标模块配置下LoRA与QLoRA的Rouge分数（分数越高越好），前三组分别为：</p><ul><li>QLoRA-All：将LoRA应用于所有FFN/MLP层和注意力层，是本实验中表现最佳的配置。</li><li>QLoRA-FFN：仅在FFN层（包括gate_proj, up_proj, down_proj）上应用LoRA。</li><li>QLoRA-Attention：仅在注意力层（包括q_proj, k_proj, v_proj, o_proj）上应用LoRA。</li></ul><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img5.jpg" alt="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide" tabindex="0" loading="lazy"><figcaption>https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide</figcaption></figure><p><strong>梯度累积与批次大小的等效关系</strong></p><p>较大的有效批次通常能稳定训练，而较小的批次可能因梯度方差增大而影响收敛。有效批次大小由以下两个参数共同决定：</p><blockquote><p>有效批次大小 = batch_size × gradient_accumulation_steps</p></blockquote><p>以下为Unsloth推荐配置，适用于多数微调场景：</p><table><thead><tr><th>参数</th><th>定义</th><th>影响</th><th>推荐值</th></tr></thead><tbody><tr><td>batch_size</td><td>单次前向或反向传播中各GPU处理的样本数</td><td>主要影响内存占用</td><td>2</td></tr><tr><td>gradient_accumulation_steps</td><td>权重更新前累积梯度的步数</td><td>模拟更大批次以节省显存；步数增加会延长每轮训练时间</td><td>8</td></tr><tr><td>有效批次大小</td><td>实际用于梯度更新的样本总数</td><td>影响训练稳定性与性能</td><td>16（2×8）</td></tr></tbody></table><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img6.jpg" alt="https://huggingface.co/docs/trl/main/distributing_training" tabindex="0" loading="lazy"><figcaption>https://huggingface.co/docs/trl/main/distributing_training</figcaption></figure><h4 id="_2-2-2-避免过拟合和欠拟合" tabindex="-1"><a class="header-anchor" href="#_2-2-2-避免过拟合和欠拟合"><span>2.2.2 避免过拟合和欠拟合</span></a></h4><p><strong>过拟合</strong></p><p>深度学习模型容易过度记忆训练数据和噪声，导致泛化能力下降。当训练损失低于0.2时，常提示过拟合，模型在未知任务上表现变差。</p><p>一种简单的缓解方法是LoRA Alpha缩放：将每个LoRA矩阵的alpha值乘以0.5。其原理类似于权重平均，将基础模型与LoRA权重相加后除以2，等同于alpha减半。该方法通过平均化机制抑制过拟合，提升模型在未知任务上的泛化性能。</p><p>其他常用解决方案包括：</p><ul><li>调整学习率：过高易引发过拟合，训练周期短时尤需注意；周期较长可适当提高。建议尝试不同取值以寻优。</li><li>控制训练轮数：通常1–3轮即可，避免过度训练。</li><li>增大权重衰减（weight_decay）：初始建议设为0.01或0.1。</li><li>启用LoRA Dropout：可设为0.1以提高泛化能力。</li><li>增大批次大小或梯度累积步数：有助于提升训练稳定性。</li><li>扩展数据集：结合高质量开源数据与自有数据，扩大样本规模。</li><li>早停机制：验证损失连续多轮上升时自动停止训练。</li><li>权重平均：将原始模型与微调后的模型权重相加取平均，平滑输出表现。</li></ul><p><strong>欠拟合（过于泛化）</strong></p><p>指模型未能充分学习训练数据中的特征，通常因模型复杂度过低或训练不足导致。改进方法包括：</p><ul><li>调整学习率：初期可适当提高以加速收敛，长期训练则需降低，需实验确定最优值。</li><li>增加训练轮次：延长训练时间，同时监控验证集损失以防过拟合。</li><li>提高LoRA秩与alpha值：秩建议不低于alpha，模型越小或数据越复杂，秩应越大，通常设为4至64。</li><li>使用领域相关数据集：确保训练数据质量高且与目标任务相关。</li><li>将批大小设为1：增强每次参数更新的强度，提高模型对数据的敏感度。</li></ul><h4 id="_2-2-3-训练数据集介绍" tabindex="-1"><a class="header-anchor" href="#_2-2-3-训练数据集介绍"><span>2.2.3 训练数据集介绍</span></a></h4><p>构建大语言模型训练数据集的关键环节之一是设计恰当的对话模板，以利于模型高效处理。关于数据集的详细介绍见：<a href="https://docs.unsloth.ai/basics/datasets-guide" target="_blank" rel="noopener noreferrer">Unsloth Datasets Guide</a>。</p><p><strong>数据格式要求</strong></p><p>为进行分词处理，数据集需采用可被分词器读取的格式。请注意，每种数据类型对应不同的格式样式。</p><table><thead><tr><th>格式类型</th><th>说明</th><th>训练类型</th></tr></thead><tbody><tr><td>原始语料</td><td>来自网站、书籍或文章等的原始文本</td><td>持续预训练（CPT）</td></tr><tr><td>指令文本</td><td>包含指令及对应输出的示例</td><td>监督微调（SFT）</td></tr><tr><td>对话记录</td><td>用户与AI助手之间的多轮对话</td><td>监督微调（SFT）</td></tr><tr><td>强化学习数据</td><td>用户与AI助手的对话，助手回复带有人工/模型/脚本的排序评分</td><td>强化学习（RL）</td></tr></tbody></table><p><strong>格式化数据</strong></p><p>在明确数据筛选标准并完成收集后，需将数据转换为机器可读的格式，以适应不同阶段的模型训练需求。以下从四种核心训练场景出发，分别介绍对应的主流数据格式及示例：</p><ol><li>预训练数据格式</li></ol><p>在模型的继续预训练阶段，通常无需对文本结构做特殊设计，直接采用原始文本即可。这种无结构化的输入方式有助于模型从连续文本中自然学习语言规律与常识知识。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;text&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;北京烤鸭是中国著名的京菜代表，其制作需经过烫皮、挂色、风干、烤制等多道工序，成品鸭皮酥脆...&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ol start="2"><li>指令微调格式</li></ol><p>为让模型适应特定任务（如问答、总结、创作），可采用Alpaca风格的指令格式。该格式包含指令（任务目标），输入（任务素材），输出（预期结果）三部分，结构清晰，便于标注。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  &quot;Instruction&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;为以下城市写一句旅游宣传语&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  &quot;Input&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;西安（关键词：兵马俑、古城墙、大唐不夜城）&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  &quot;Output&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;穿越秦唐，梦回长安——西安等你来探秘&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>多轮对话格式</li></ol><p>针对多轮对话场景（如客服、聊天助手），需保留上下文逻辑，常用ShareGPT格式。通过from字段标注角色（human为用户，gpt为模型），value记录发言内容，清晰呈现对话流程。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  &quot;conversations&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;from&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;human&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;value&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;推荐一道适合初学者的家常菜&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;from&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;gpt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;value&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;番茄炒蛋简单易学，需要我介绍具体步骤吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;from&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;human&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;value&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;好的，请说明关键步骤和注意事项&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;from&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;gpt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;value&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;步骤：1. 番茄切块，鸡蛋打散；2. 热油炒蛋后盛出；3. 炒番茄至出汁，加糖调味；4. 混入鸡蛋翻炒。注意火候，避免蛋炒老。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  ]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="4"><li>ChatML格式</li></ol><p>ChatML格式由OpenAI提出，是当前工业界广泛使用的对话格式，也被Hugging Face等平台默认支持。它通过role字段定义角色（如user，assistant，system），用content记录内容，结构清晰且兼容性强。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">  &quot;messages&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你是一位中文烹饪助手，回答需简明实用&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;蒸鱼应该用大火还是小火？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;建议大火蒸制，时间约8–10分钟，这样鱼肉更鲜嫩。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  ]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>合成数据生成</strong></p><p>为获得理想的微调效果，建议数据集不少于100条；若追求更优性能，推荐使用1000条以上的数据。通常情况下，数据量越大，效果越好。若原始数据不足，可引入合成数据或补充Hugging Face上的相关数据集以增强多样性。请注意，微调效果高度依赖数据质量，务必做好数据清洗和预处理。</p><p>生成合成数据时，可使用本地大语言模型（如Llama 3.3 70B）或OpenAI的GPT-4.5。通常更推荐使用参数规模更大的模型以保证生成质量。通过vLLM、Ollama或 llama.cpp等推理引擎可直接生成数据，但需手动收集生成结果，并优化提示词以扩展内容。合成数据的主要用途包括：</p><ol><li>创造全新数据：既可完全从头生成，也可基于现有样本进行改写或扩展；</li><li>增强数据多样性：避免模型过拟合，提升泛化能力；</li><li>完善现有数据：例如将文本自动转换为指定格式（如将对话转为问答形式）。</li></ol><h3 id="_2-3-qwen3使用示例" tabindex="-1"><a class="header-anchor" href="#_2-3-qwen3使用示例"><span>2.3 Qwen3使用示例</span></a></h3><p>本文将以Qwen3为例进行模型训练演示。Qwen3由阿里通义千问推出，在推理、指令遵循及多语言支持等核心能力上实现行业领先，是大语言模型训练的优选架构。</p><p>Unsloth已于2025年7月完成升级，支持最新的Qwen-2507模型。在使用Unsloth运行或微调量化版Qwen模型时，几乎无损精度。同时，Unsloth为Qwen3原生支持128K上下文长度，可一次性处理数万字的长文档或对话；该扩展基于YaRN技术，将模型原有的40K处理上限提升至128K。优化后，模型训练速度提升2倍，显存占用降低70%。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img6_2.jpg" alt="https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune/qwen3-2507" tabindex="0" loading="lazy"><figcaption>https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune/qwen3-2507</figcaption></figure><p><strong>模型版本</strong></p><p>为帮助开发者根据模型运行、长上下文支持、微调与部署等场景需求，选择合适规模的Qwen3模型，Unsloth基于其技术能力，围绕以下三个维度提供了多种参数规格的版本：</p><ol><li><p>Dynamic 2.0 GGUF（适用于模型运行）<br> 涵盖0.6B至235B-A22B等多种参数规模，支持用户直接运行Qwen3模型，适用于常规推理与基础任务场景。</p></li><li><p>128K Context GGUF（适用于长上下文处理）<br> 提供4B到235B-A22B等多个版本，重点优化了128K上下文长度的处理能力，适用于长文档分析、超长对话及对语义连贯性要求较高的复杂任务。</p></li><li><p>Dynamic 4-bit Safetensor（适用于微调与部署）<br> 覆盖0.6B至32B参数规模，采用4位量化的Safetensor格式，在保持模型性能的同时显著降低存储与计算资源开销，便于进行任务特定微调或生产环境部署。</p></li></ol><p><strong>推理参数</strong></p><p>为达到每秒6个token以上的推理速度，Unsloth建议总内存（即显存、内存或两者总和）不低于所使用模型的大小。即使总内存低于模型大小，仍可运行模型，但推理速度会降低。根据Qwen官方建议，模型推理的推荐设置如下：</p><table><thead><tr><th>参数</th><th>非思考模式（Non-ThinkingMode）</th><th>思考模式（ThinkingMode）</th><th>解释</th></tr></thead><tbody><tr><td>温度（Temperature）</td><td>0.7</td><td>0.6</td><td>值越低输出越确定</td></tr><tr><td>最小概率（Min_P）</td><td>0.0（可选，0.01效果更佳）</td><td>0.0</td><td>仅考虑累积概率达到该值的候选词</td></tr><tr><td>累积概率（Top_P）</td><td>0.8</td><td>0.95</td><td>从累积概率前百分之几的候选词中选取</td></tr><tr><td>候选词数量（TopK）</td><td>20</td><td>20</td><td>每次只从概率最高的K个词中选择</td></tr></tbody></table><p><strong>Qwen3对话模板</strong></p><p>Qwen3系列模型采用ChatML对话模板，默认启用思考模式。请注意，若使用贪婪解码，可能导致模型性能下降或生成内容无限重复。基础对话格式如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&lt;|im_start|&gt;user\nWhat is 2+2?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如需关闭思考模式，需插入一对空的<code>&lt;think&gt;</code>与<code>&lt;/think&gt;</code>标签，格式如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&lt;|im_start|&gt;user\nWhat is 2+2?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>示例推理代码</strong></p><p>下面代码展示通过Unsloth的FastModel类加载Qwen3-0.6B模型并启用4位量化：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastModel</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 定义要使用的模型名称，这里使用的是Qwen3-0.6B模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;Qwen/Qwen3-0.6B&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 利用modelscope加速下载模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 指定模型所在的目录路径</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 设置最大序列长度为2048，可以根据需要调整以支持长文本</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    load_in_4bit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,     </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 启用4位量化以减少内存占用</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    load_in_8bit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,    </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 禁用8位量化（新特性：8位量化精度稍高，但内存占用是4位的2倍）</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    full_finetuning</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 禁用全参数微调（新特性：现在支持全参数微调）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 准备模型输入</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;推荐一部搞笑的科幻电影。&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 构造对话消息列表，包含用户角色和内容</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: prompt}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 应用聊天模板处理消息，转换为模型所需的输入格式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    messages,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 不直接进行token化</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 添加生成提示</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    enable_thinking</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 启用思考模式，默认为True</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将文本转换为模型输入张量，并移动到模型所在设备</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([text], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 进行文本生成，输出为token</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">generated_ids </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    **model_inputs,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 解包模型输入</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2048</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 最大生成的新token数量</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 提取生成的部分（排除输入部分）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">output_ids </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> generated_ids[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_inputs.input_ids[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]):].</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">tolist</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 解析思考内容</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">try</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 查找特殊标记151668（表示思考内容结束）的位置</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    index </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(output_ids) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> output_ids[::</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">].</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">index</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">151668</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 这个结束符就是&lt;/think&gt;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # tokenizer.decode(output_ids[index-1])</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">except</span><span style="--shiki-light:#0184BC;--shiki-dark:#ABB2BF;"> ValueError</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 如果未找到特殊标记，索引设为0</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    index </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 解码思考内容（特殊标记之前的部分）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">thinking_content </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(output_ids[:index], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_special_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">strip</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 解码回复内容（特殊标记之后的部分）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">content </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(output_ids[index:], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_special_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">strip</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印思考内容和最终回复内容</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;思考内容:&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, thinking_content)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;回复内容:&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, content)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-4-unsloth训练qwen3教程" tabindex="-1"><a class="header-anchor" href="#_2-4-unsloth训练qwen3教程"><span>2.4 Unsloth训练Qwen3教程</span></a></h3><p>Qwen3能够同时进行数学推理和常识问答。但如果只用“天空是什么颜色？蓝色”这类常识样本训练，模型在微调后可能出现能力退化，甚至无法正确解答“1+2×3=？”这类简单题目。 为保持模型的推理能力，建议在训练素材中混合使用推理类和非推理类样本。例如，可组合75%的思维链样本。如“1+2×3：先算乘法2×3=6，再加1得7”，以及25%的常识类样本，如直接提供答案的问题。这样模型既能正确回答常识问题，也能维持数学推理能力，实现两类任务的平衡。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习5-高效微调框架Unsloth使用指北/img/img7.jpg" alt="https://medium.com/data-and-beyond/a-practical-guide-to-fine-tune-mistral-7b-with-unsloth-for-phishing-email-detection-2faa5b531e27" tabindex="0" loading="lazy"><figcaption>https://medium.com/data-and-beyond/a-practical-guide-to-fine-tune-mistral-7b-with-unsloth-for-phishing-email-detection-2faa5b531e27</figcaption></figure><p>下面将依次介绍如何使用Unsloth加载Qwen3模型，并详细讲解数据预处理、模型训练、模型运行及模型保存的完整流程。</p><h4 id="_2-4-1-预训练模型初始化" tabindex="-1"><a class="header-anchor" href="#_2-4-1-预训练模型初始化"><span>2.4.1 预训练模型初始化</span></a></h4><p>以下代码演示了如何利用Unsloth库加载Qwen3-0.6B模型，通过4位精度量化大幅减少内存使用，并借助LoRA方法实现高效的参数微调。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从modelscope库导入snapshot_download函数，用于下载模型快照</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从unsloth库导入FastLanguageModel类，用于高效加载语言模型</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 定义要使用的模型名称，这里使用的是Qwen3-0.6B模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;Qwen/Qwen3-0.6B&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 利用modelscope的snapshot_download函数加速下载模型，并返回模型保存的目录路径</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用FastLanguageModel的from_pretrained方法加载预训练模型和分词器</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir,       </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 模型所在的目录路径</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,        </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 上下文长度 - 可以设置更长，但会占用更多内存</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    load_in_4bit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,          </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 以4位精度加载，使用更少内存</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    load_in_8bit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,         </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 以8位精度加载会更准确，但占用2倍内存</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    full_finetuning</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,      </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 是否使用全量微调，当前设置为否</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 为模型配置LoRA方法</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">get_peft_model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    model,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    r</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 32</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,           </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># LoRA注意力维度，可选择任何大于0的值，建议8, 16, 32, 64, 128</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    target_modules</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;q_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;k_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;v_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;o_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 注意力模型和FFN模块</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">                      &quot;gate_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;up_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;down_proj&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,],  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 指定要微调的模块</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    lora_alpha</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 32</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># LoRA缩放参数，建议设置为与rank相同或rank的2倍</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    lora_dropout</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># LoRA 层的 dropout 率（这里设为 0 以优化性能）</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    bias</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,    </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 是否训练偏置（这里设为 &quot;none&quot; 表示不训练）</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # [新特性] &quot;unsloth&quot;模式可减少30%的VRAM使用，支持2倍大的批量大小</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    use_gradient_checkpointing</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;unsloth&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># True或&quot;unsloth&quot;用于超长上下文</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    random_state</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 随机种子，确保结果可复现</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    use_rslora</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 是否使用RSLoRA</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    loftq_config</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 是否使用 LoftQ</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-4-2-数据集加载" tabindex="-1"><a class="header-anchor" href="#_2-4-2-数据集加载"><span>2.4.2 数据集加载</span></a></h4><p>Qwen3包含推理和非推理两种模式，本示例使用以下两个训练数据集：</p><ol><li><p>推理数据：Open Math Reasoning（开放数学推理）数据集<br> 从中采样了10%的可验证推理轨迹，这些样本使用了DeepSeek R1，且准确率超过95%。<br> 从这些数据里，Unsloth仅筛出DeepSeek-R1回答、正确率≥95%且每一步都可验证的标准答案，再从中随机抽取10%使用。</p><ul><li>用处：专注于数学推理能力的微调数据集，包含各种数学问题及其详细解答过程。</li><li>来源：<a href="https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini" target="_blank" rel="noopener noreferrer">unsloth/OpenMathReasoning-mini</a></li><li>样本数量：19,252</li><li>格式：包含数学问题、期望答案、问题类型、解答过程等字段</li><li>特点：增强模型的数学推理和思维链（Chain-of-Thought）能力</li></ul></li><li><p>通用对话数据：Maxime Labonne的FineTome-100k数据集<br> 其格式为ShareGPT风格，已转换为Hugging Face标准的多轮对话格式。</p><ul><li>用处：高质量的指令遵循数据集，专为大语言模型微调设计</li><li>来源：<a href="https://huggingface.co/datasets/unsloth/mlabonne/FineTome-100k" target="_blank" rel="noopener noreferrer">mlabonne/FineTome-100k</a></li><li>样本数量：100,000</li><li>格式：包含对话内容、来源和质量分数</li><li>特点：数据质量高，覆盖广泛的指令类型和领域</li></ul></li></ol><p>数据处理代码如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> datasets </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> load_dataset</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 数据集下载链接</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># reasoning_dataset = load_dataset(&quot;unsloth/OpenMathReasoning-mini&quot;, split = &quot;cot&quot;)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># non_reasoning_dataset = load_dataset(&quot;mlabonne/FineTome-100k&quot;, split = &quot;train&quot;)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 如果无法直接访问Hugging Face，可以使用以下两个命令从镜像网站下载数据集到本地（速度也很慢）</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># git clone https://hf-mirror.com/datasets/unsloth/OpenMathReasoning-mini</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># git clone https://hf-mirror.com/datasets/mlabonne/FineTome-100k</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从本地加载数据集</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">reasoning_dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> load_dataset</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;./OpenMathReasoning-mini&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">split</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cot&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">non_reasoning_dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> load_dataset</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;./FineTome-100k&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">split</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;train&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查看数据集结构</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 特征包含预期答案、题目类型、题目来源、生成模型，72B TIR模式下的通过率、题目本身、解答过程、推理模式</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_dataset)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 特征包含对话、来源、分数</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_dataset)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将reasoning_dataset转换为对话格式</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> generate_conversation</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">examples</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    problems  </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> examples[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;problem&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    solutions </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> examples[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;generated_solution&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 初始化一个空列表，用于存储转换后的对话</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    conversations </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> []</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 同时遍历问题和解决方案列表，将它们配对成对话</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> problem, solution </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> zip</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(problems, solutions):</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 为每对问题和解决方案创建一个对话结构</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 每个对话包含两个角色的消息：用户（提问）和助手（回答）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        conversations.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">            {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> : </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,      </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> : problem},      </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 用户角色的消息内容是问题</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">            {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> : </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> : solution},     </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 助手角色的消息内容是解决方案</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        ])</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> { </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;conversations&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: conversations, }</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用tokenizer将推理数据集转换为模型可理解的对话模板格式</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 参数tokenize=False表示只进行格式转换，不进行分词处理</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">reasoning_conversations </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    reasoning_dataset.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">map</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(generate_conversation, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">batched</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;conversations&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">],</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 接下来，处理处理非推理型数据集，并同样将其转换为对话格式。</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用standardize_sharegpt函数，对该数据集的格式进行规范化处理。</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth.chat_templates </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> standardize_sharegpt</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> standardize_sharegpt</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_dataset)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">non_reasoning_conversations </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    dataset[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;conversations&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">],</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_conversations[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 查看数据集尺寸</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations))</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_conversations))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 非推理类数据集规模大的多。希望模型保留一定推理能力，训练数据选取75%推理类数据搭配25%对话类数据</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">chat_percentage </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0.25</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pandas </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pd</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">non_reasoning_subset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">Series</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_conversations)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">non_reasoning_subset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> non_reasoning_subset.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">sample</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    int</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations)</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(chat_percentage</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> -</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> chat_percentage))),</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    random_state</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印各类数据量及实际比例用于验证</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations))  </span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_subset))   </span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_subset) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> (</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_subset) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations)))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 合并推理类数据和抽样后的非推理类数据</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">data </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">concat</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">Series</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(reasoning_conversations),  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">Series</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(non_reasoning_subset)      </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">data.name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;text&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 为合并后的数据系列命名</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> datasets </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> Dataset</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将pandas DataFrame转换为Hugging Face数据集格式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">combined_dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> Dataset.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pandas</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">DataFrame</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(data))</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 对数据集进行随机打乱，确保数据分布均匀</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">combined_dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> combined_dataset.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">shuffle</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">seed</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-4-3-模型训练" tabindex="-1"><a class="header-anchor" href="#_2-4-3-模型训练"><span>2.4.3 模型训练</span></a></h4><p>为加快训练速度，训练仅迭代30步。若需完整训练，可将num_train_epochs设为1，并将max_steps设为None以取消步数限制。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># trl库是Hugging Face开发的，用于通过强化学习来微调与对齐大型语言模型的工具</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># SFTTrainer用于监督微调训练，SFTConfig用于配置训练参数</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> trl </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> SFTTrainer, SFTConfig</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 初始化SFTTrainer训练器</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">trainer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> SFTTrainer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    train_dataset</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">combined_dataset,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 训练数据集</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    eval_dataset</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 评估数据集</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    args</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">SFTConfig</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        dataset_text_field</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;text&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 数据集中用于训练的文本字段名称</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        per_device_train_batch_size</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 每个设备的训练批次大小</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 梯度累积步数，通过累积梯度来模拟更大的批次大小</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # 实际等效批次大小 = per_device_train_batch_size * gradient_accumulation_steps</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        gradient_accumulation_steps</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        warmup_steps</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 学习率预热步数，逐步增加到设定的学习率</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # num_train_epochs = 1,  # 训练轮数，注释掉表示不使用轮数限制</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        max_steps</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">30</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 最大训练步数，达到后停止训练</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        learning_rate</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2e-4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 学习率，长时间训练建议降低到2e-5</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        logging_steps</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 每多少步记录一次日志</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        optim</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;adamw_8bit&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用8位AdamW优化器，节省内存</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        weight_decay</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.01</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 权重衰减系数，用于防止过拟合</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        lr_scheduler_type</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;linear&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 学习率调度器类型，此处为线性衰减</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        seed</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        report_to</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 日志报告工具</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    ),</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 获取编号为0的GPU设备属性信息，包括名称、总内存等</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">gpu_stats </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.cuda.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">get_device_properties</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算当前程序已保留的最大GPU内存，也就是PyTorch的CUDA分配器最高向操作系统申请了多少内存</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">start_gpu_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(torch.cuda.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">max_memory_reserved</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算GPU的总内存容量</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">max_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(gpu_stats.total_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印GPU名称和总内存信息</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;GPU = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">gpu_stats.name</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">. Max memory = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">max_memory</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> GB.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印当前已保留的GPU内存信息</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">start_gpu_memory</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> GB of memory reserved.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 开始训练</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># resume_from_checkpoint是否从之前保存的检查点恢复训练</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">trainer_stats </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> trainer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">train</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">resume_from_checkpoint</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算GPU的最大预留内存</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(torch.cuda.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">max_memory_reserved</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算LoRA训练额外占用的GPU内存</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_memory_for_lora </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(used_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> start_gpu_memory, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算峰值内存占GPU总内存的百分比</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_percentage </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(used_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> max_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算LoRA训练占用内存占GPU总内存的百分比，保留3位小数</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">lora_percentage </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> round</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(used_memory_for_lora </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> max_memory </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印训练总耗时</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">trainer_stats.metrics[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;train_runtime&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> seconds used for training.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印GPU峰值预留内存</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Peak reserved memory = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_memory</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> GB.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印训练过程中额外占用的GPU峰值内存</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Peak reserved memory for training = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_memory_for_lora</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> GB.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印GPU峰值内存占总内存的百分比</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Peak reserved memory % of max memory = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">used_percentage</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> %.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 打印LoRA额外占用内存占GPU总内存的百分比</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Peak reserved memory for training % of max memory = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">lora_percentage</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> %.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-4-4-模型推理" tabindex="-1"><a class="header-anchor" href="#_2-4-4-模型推理"><span>2.4.4 模型推理</span></a></h4><p>推理阶段，据Qwen-3团队的建议：</p><ul><li>若用于普通对话任务，推荐参数设置为：temperature=0.7，top_p=0.8，top_k=20。</li><li>若用于推理任务，推荐参数设置为：temperature=0.6，top_p=0.95，top_k=20。</li></ul><p>以下代码对比这两种生成模式。前者直接给结果；后者先思考解题步骤再给结果，更适合需解释过程的数学问题场景。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 定义对话消息列表，包含用户的问题</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 这里问题是求解方程 (x + 3)^2 = 0</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Solve (x + 3)^2 = 0.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用tokenizer的聊天模板处理消息</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将消息转换为模型可以理解的格式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    messages,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    enable_thinking</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 导入TextStreamer，用于实时流式输出模型生成的内容</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> TextStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">_ </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    **</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">),</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">256</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 最大生成的新token数量，控制回答长度</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    temperature</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.7</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_p</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.8</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">20</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    streamer</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">TextStreamer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        tokenizer, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    ),  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 流式输出器，跳过提示部分只显示回答</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 再次定义相同的用户问题，用于演示思考模式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;Solve (x + 3)^2 = 0.&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用聊天模板处理消息，这次启用思考模式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    messages,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    enable_thinking</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 启用思考模式，模型会先思考再给出答案</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 在思考模式下生成回答</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">_ </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    **</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">),</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 思考模式需要更多token来容纳思考过程</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    temperature</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.6</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 稍低的温度，使思考过程更集中</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_p</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0.95</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 更高的核采样参数，允许更多样化的思考</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    top_k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">20</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 同样从概率最高的20个token中选择</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    streamer</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">TextStreamer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(tokenizer, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">),</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-4-5-模型保存" tabindex="-1"><a class="header-anchor" href="#_2-4-5-模型保存"><span>2.4.5 模型保存</span></a></h4><p>Unsloth支持两条互补的持久化路线：</p><ol><li>保留LoRA适配器：体积最小，便于继续微调或增量更新；</li><li>合并并量化导出：得到独立权重文件，方便直接部署或上传到 Hub。</li></ol><p><strong>保存LoRA适配器</strong></p><p>训练完成后，只需把模型与分词器以<code>save_pretrained</code>写入同一目录即可：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;lora_model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)      </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 仅保存 LoRA 参数</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;lora_model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>后续加载时，用<code>from_pretrained</code>接口指定本地路径，Unsloth会自动把基础模型与LoRA权重重新组装：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;lora_model&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载lora参数，同时加载训练时的基础模型</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    load_in_4bit</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>合并后导出，用于部署</strong></p><p>可将LoRA合并到基础模型中，并支持将合并后的模型一次性导出为float16、int4或GGUF（GPT-Generated Unified Format）格式，便于在GPU或CPU端侧进行高效推理。</p><p>GGUF是一种高效的模型存储与交换格式，将大模型封装为单一文件，具备秒级加载能力。GGUF可直接用于llama.cpp系列工具，实现快速部署与应用。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 导出float16完整权重</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained_merged</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model-f16&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, tokenizer, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">save_method</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;merged_16bit&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 导出int4量化权重，会有精度损失</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained_merged</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model-int4&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, tokenizer, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">save_method</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;merged_4bit_forced&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 导出GGUF系列</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained_gguf</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model-q8&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  tokenizer)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 默认 Q8_0</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained_gguf</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model-f16&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, tokenizer,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                           quantization_method</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;f16&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 16-bit GGUF</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">save_pretrained_gguf</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;model-q4&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  tokenizer,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                           quantization_method</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;q4_k_m&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 4-bit GGUF</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_3-参考" tabindex="-1"><a class="header-anchor" href="#_3-参考"><span>3 参考</span></a></h2><ul><li><a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer">Unsloth</a></li><li><a href="https://docs.unsloth.ai/" target="_blank" rel="noopener noreferrer">Unsloth Docs</a></li><li><a href="https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth" target="_blank" rel="noopener noreferrer">Multi-GPU-Unsloth</a></li><li><a href="https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms" target="_blank" rel="noopener noreferrer">How To Fine-tune &amp; Run LLMs</a></li><li><a href="https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements" target="_blank" rel="noopener noreferrer">Unsloth-requirements</a></li><li><a href="https://docs.unsloth.ai/get-started/all-our-models" target="_blank" rel="noopener noreferrer">Unsloth Models</a></li><li><a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide" target="_blank" rel="noopener noreferrer">LoRA Hyperparameters Guide</a></li><li><a href="https://zhuanlan.zhihu.com/p/671089942" target="_blank" rel="noopener noreferrer">LoRA、QLoRA、QA-LoRA 原理笔记</a></li><li><a href="https://docs.unsloth.ai/basics/datasets-guide" target="_blank" rel="noopener noreferrer">Unsloth Datasets Guide</a></li><li><a href="https://docs.unsloth.ai/get-started/unsloth-notebooks" target="_blank" rel="noopener noreferrer">Unsloth Notebooks</a></li></ul></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-08-08-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A04-RAG%E6%8A%80%E6%9C%AF%E5%85%A8%E6%99%AF%E8%A7%A3%E6%9E%90.html" aria-label="[深度学习] 大模型学习4-RAG技术全景解析" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->[深度学习] 大模型学习4-RAG技术全景解析</div></a><!----></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><!----><div class="vp-copyright">Copyright © 2025 落痕月极 </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-B1QbUTkN.js" defer></script>
  </body>
</html>
