<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.19" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.71" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://luohenyueji.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-23-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8B-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html"><meta property="og:site_name" content="落痕月极的博客"><meta property="og:title" content="[深度学习] 大模型学习3下-模型训练与微调"><meta property="og:description" content="[深度学习] 大模型学习3下-模型训练与微调 在文章大语言模型基础知识里，模型训练与微调作为大语言模型（Large Language Model，LLM）应用构建的主要方式被简要提及，本系列文章将从技术原理、实施流程及应用场景等维度展开深度解析。相关知识的进一步参考见：LLM训练理论和实战。本文作为系列的下半部分，包含第3章并聚焦于大语言模型构建的实操..."><meta property="og:type" content="article"><meta property="og:image" content="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img10.jpg"><meta property="og:locale" content="zh-CN"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="自然语言处理与语音识别"><meta property="article:tag" content="Python"><meta property="article:published_time" content="2025-07-23T20:01:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"[深度学习] 大模型学习3下-模型训练与微调","image":["https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img10.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img11.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img12.gif","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img13.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img14.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img15.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img15.gif","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img16.jpg","https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83/img/img17.jpg"],"datePublished":"2025-07-23T20:01:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"落痕月极","url":"/"}]}</script><link rel="icon" href="/logo.png"><title>[深度学习] 大模型学习3下-模型训练与微调 | 落痕月极的博客</title><meta name="description" content="[深度学习] 大模型学习3下-模型训练与微调 在文章大语言模型基础知识里，模型训练与微调作为大语言模型（Large Language Model，LLM）应用构建的主要方式被简要提及，本系列文章将从技术原理、实施流程及应用场景等维度展开深度解析。相关知识的进一步参考见：LLM训练理论和实战。本文作为系列的下半部分，包含第3章并聚焦于大语言模型构建的实操...">
    <link rel="preload" href="/assets/style-7oyYUlCQ.css" as="style"><link rel="stylesheet" href="/assets/style-7oyYUlCQ.css">
    <link rel="modulepreload" href="/assets/app-HB0Nuzez.js"><link rel="modulepreload" href="/assets/2025-07-23-_深度学习_ 大模型学习3下-模型训练与微调.html-DPPt8yXC.js">
    <link rel="prefetch" href="/assets/about.html-Bbn09tJN.js" as="script"><link rel="prefetch" href="/assets/intro.html-DCcAFvbf.js" as="script"><link rel="prefetch" href="/assets/index.html-B3NGOCJU.js" as="script"><link rel="prefetch" href="/assets/index.html-CnQXhKw3.js" as="script"><link rel="prefetch" href="/assets/index.html-2e8nD_dn.js" as="script"><link rel="prefetch" href="/assets/2021-08-04-_图像处理_ 基于图像哈希构建图像相似度对比算法.html-1kscxwrZ.js" as="script"><link rel="prefetch" href="/assets/2024-10-24-_图像处理_ 基于CleanVision库清洗图像数据集.html-CtWzX4Ox.js" as="script"><link rel="prefetch" href="/assets/2019-01-21-_常用工具_ 深度学习Caffe处理工具.html-BtzESI1I.js" as="script"><link rel="prefetch" href="/assets/2019-03-12-_常用工具_ Caffe ssd常见问题集合.html-C4tPk2Uh.js" as="script"><link rel="prefetch" href="/assets/2019-04-19-_常用工具_ OpenCV获取网络摄像头实时视频流.html-CjESZIFO.js" as="script"><link rel="prefetch" href="/assets/2020-02-16-_常用工具_ git基础学习笔记.html-PyXpR9Sb.js" as="script"><link rel="prefetch" href="/assets/2020-04-07-_常用工具_ shell脚本快速入门笔记.html-BJOZRVzz.js" as="script"><link rel="prefetch" href="/assets/2020-05-07-_常用工具_ live555的搭建.html-KCjKY5Ss.js" as="script"><link rel="prefetch" href="/assets/2020-08-11-_常用工具_ OpenCV_contrib库在windows下编译使用指南.html-D7dSRi_w.js" as="script"><link rel="prefetch" href="/assets/2021-02-10-_常用工具_ cvat安装与使用指北.html-B016w2Wg.js" as="script"><link rel="prefetch" href="/assets/2021-04-23-_常用工具_ dlib编译调用指南.html-DQy38mPw.js" as="script"><link rel="prefetch" href="/assets/2021-05-22-_常用工具_ mermaid学习笔记.html-JkXuW2fP.js" as="script"><link rel="prefetch" href="/assets/2021-12-21-_常用工具_ PyAutoGUI使用教程.html-ltCWe32d.js" as="script"><link rel="prefetch" href="/assets/2022-03-20-_常用工具_ 搜索引擎的常用技巧总结.html-DBBkAFdo.js" as="script"><link rel="prefetch" href="/assets/2022-07-13-_常用工具_ C__环境下Qt的安装.html-DFst7p6m.js" as="script"><link rel="prefetch" href="/assets/2022-07-18-_常用工具_ 基于psutil和GPUtil获取系统状态信息.html-DC_QUVcn.js" as="script"><link rel="prefetch" href="/assets/2022-08-12-_常用工具_ Python视频处理库VidGear使用指北.html--QBabZkh.js" as="script"><link rel="prefetch" href="/assets/2022-08-19-_常用工具_ Python视频解码库DeFFcode使用指北.html-_tMNYlfQ.js" as="script"><link rel="prefetch" href="/assets/2021-06-27-_数据分析与可视化_ 科技论文配色心得.html-Bk29lCdM.js" as="script"><link rel="prefetch" href="/assets/2017-10-24-_数学理论_ 单一数字评估指标.html-CdDj_vDd.js" as="script"><link rel="prefetch" href="/assets/2017-10-25-_数学理论_ 不同分布训练集、验证集、测试集处理.html-Dh1PBjHs.js" as="script"><link rel="prefetch" href="/assets/2024-06-01-_机器学习_ 低代码机器学习工具PyCaret库使用指北.html-Cg1qtA74.js" as="script"><link rel="prefetch" href="/assets/2023-07-27-_自然语言处理_ 自然语言处理库spaCy使用指北.html-CfwlmCOU.js" as="script"><link rel="prefetch" href="/assets/2023-08-21-_语音识别_ 基于Python构建简易的音频录制与语音识别应用.html-CSqD5W05.js" as="script"><link rel="prefetch" href="/assets/2023-09-24-_自然语言处理_ 基于pycorrector实现文本纠错.html-ClviZYMM.js" as="script"><link rel="prefetch" href="/assets/2020-05-13-_随笔所想_ CSDN认证博客专家申请通过随笔所想.html-C4Wu5uye.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_随笔所想_ 程序员中年失业随笔所想.html-CPyp8Xtp.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_随笔所想_ UBC学习生活经验分享.html-BEDNvA_y.js" as="script"><link rel="prefetch" href="/assets/2021-01-14-_随笔所想_ 2021年新年碎碎念-加油了不起的干饭人!.html-CnBGbpuP.js" as="script"><link rel="prefetch" href="/assets/2021-02-12-_随笔所想_ 牛年碎碎念祝大家牛年大吉.html-GqCmuiJN.js" as="script"><link rel="prefetch" href="/assets/2021-03-31-_随笔所想_ 买房和户型挑选入门.html-COkKF901.js" as="script"><link rel="prefetch" href="/assets/2021-08-29-_随笔所想_ 学英语打卡2000天碎碎念.html-DZa-oEIS.js" as="script"><link rel="prefetch" href="/assets/2021-12-1-_随笔所想_ 沉痛悼念开发技术专家毛星云老师.html-sJHDDaBn.js" as="script"><link rel="prefetch" href="/assets/2024-02-17-_随笔所想_ 劳动合同法学习笔记.html-CTesUnX5.js" as="script"><link rel="prefetch" href="/assets/2017-11-04-_能源化工_ TE田纳西-伊斯曼过程数据集.html-2ayL3XM3.js" as="script"><link rel="prefetch" href="/assets/2020-05-09-_能源化工_ 电力四遥.html-CaSUUUmw.js" as="script"><link rel="prefetch" href="/assets/2022-01-25-_生命科学_ 生物基础实验之PCR验证.html-DbGBfBhp.js" as="script"><link rel="prefetch" href="/assets/2022-01-31-_生命科学_ 生物基础实验之DNA提取.html-Dp8lh_0Q.js" as="script"><link rel="prefetch" href="/assets/2022-03-04-_生命科学_ snapgene 构建载体方法分享.html-Csto7Qf8.js" as="script"><link rel="prefetch" href="/assets/2022-03-25-_生命科学_ 生物基础实验之三引物检测突变体.html-DLeD1o5C.js" as="script"><link rel="prefetch" href="/assets/2025-09-13-_能源化工_ 面向锂电池RUL预测的开源项目全景速览.html-vPQiYUzp.js" as="script"><link rel="prefetch" href="/assets/2020-12-13-_讲座论坛_ 碧根果产业现状及产业化开发关键技术.html-By7n6ptZ.js" as="script"><link rel="prefetch" href="/assets/2020-12-20-_讲座论坛_ 竹资源培育与中国竹产业.html-Dcp5DT1k.js" as="script"><link rel="prefetch" href="/assets/2020-12-27-_讲座论坛_ 经济林之核桃类.html-BBVlnCkj.js" as="script"><link rel="prefetch" href="/assets/2021-04-21-_讲座论坛_ 应对气候变化的中国视角.html-CiMEMYzv.js" as="script"><link rel="prefetch" href="/assets/2022-02-28-_讲座论坛_ 国家自然基金申请知识汇总.html-Dp0Xp6ko.js" as="script"><link rel="prefetch" href="/assets/2022-12-31-_讲座论坛_ 研究的艺术学习笔记.html-unMJ2qNy.js" as="script"><link rel="prefetch" href="/assets/2023-05-31-_音视频处理_ FFmpeg使用指北1-视频解码.html-1PEAUgWb.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ kmeans聚类和WGCNA.html-Dr37pwn7.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 均匀和不均匀造林对生态多样性的影响综述.html-CWd1EP3U.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 枣树的历史与现状研究进展.html-D89Q16nL.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 森林管理和造林业中复杂观念的转变.html-CIY2AROO.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 深度学习技术在植物领域的研究1.html-NSfTbEwW.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 深度学习技术在植物领域的研究2.html-ChDap4mV.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 物联网技术在现代林业中的应用.html-fxMIbbF3.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 石榴综述论文阅读笔记.html-fSyiqKJS.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 美国造林业：过去30年的惊人变化时期.html-ZwcWXy3n.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 苗圃营建学习笔记.html-BygiOGmj.js" as="script"><link rel="prefetch" href="/assets/2020-12-07-_论文总结_ 集约经营下人工林造林技术研究进展.html-DHn_gMcM.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 农业工程领域中App和Web相关应用论文笔记.html-BIQmACVk.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 智慧农业论文摘要阅读概览.html-B8RnuO57.js" as="script"><link rel="prefetch" href="/assets/2021-09-23-_论文总结_ 深度学习在农业领域应用论文笔记.html-Dla1CYxG.js" as="script"><link rel="prefetch" href="/assets/2021-10-18-_论文总结_ 育种理论与基因检测.html-BCN5DBjb.js" as="script"><link rel="prefetch" href="/assets/2021-10-19-_论文总结_ 树木的营养生理.html-rHwdBuPq.js" as="script"><link rel="prefetch" href="/assets/2021-10-20-_论文总结_ 深度学习在农业领域应用论文笔记2.html-7ODDJfOS.js" as="script"><link rel="prefetch" href="/assets/2021-10-21-_论文总结_ 深度学习在农业领域应用论文笔记3.html-DvfBQvcp.js" as="script"><link rel="prefetch" href="/assets/2021-10-27-_论文总结_ 深度学习在农业领域应用论文笔记4.html-B8BUFzmp.js" as="script"><link rel="prefetch" href="/assets/2021-10-28-_论文总结_ 深度学习在农业领域应用论文笔记5.html-Cg7V-dbp.js" as="script"><link rel="prefetch" href="/assets/2021-10-29-_论文总结_ 深度学习在农业领域应用论文笔记6.html-1IcKcQuk.js" as="script"><link rel="prefetch" href="/assets/2021-10-30-_论文总结_ 深度学习在农业领域应用论文笔记7.html-BkKyj0-p.js" as="script"><link rel="prefetch" href="/assets/2021-11-02-_论文总结_ 深度学习在农业领域应用论文笔记8.html-B3fCs8QY.js" as="script"><link rel="prefetch" href="/assets/2021-11-03-_论文总结_ 深度学习在农业领域应用论文笔记9.html-C1CVOZlK.js" as="script"><link rel="prefetch" href="/assets/2021-11-04-_论文总结_ 森林生态系统中的水生生境.html-Bngu4PWI.js" as="script"><link rel="prefetch" href="/assets/2022-04-05-_论文总结_  种群、保护与生态遗传学笔记.html-CDn1Dhnz.js" as="script"><link rel="prefetch" href="/assets/2022-05-01-_论文总结_ Genecology and Adaptation of Forest Trees 林木的基因生态学与适应性.html-zhBMMXXL.js" as="script"><link rel="prefetch" href="/assets/2022-05-20-_论文总结_ 中国工科生常见英文写作问题总结.html-CMw4OR5f.js" as="script"><link rel="prefetch" href="/assets/2022-05-31-_论文总结_ 科技论文英语写作笔记1.html-DScZIQ5v.js" as="script"><link rel="prefetch" href="/assets/2022-08-01-_论文总结_ 深度学习在农业领域应用论文笔记10.html-Due6CB0d.js" as="script"><link rel="prefetch" href="/assets/2023-02-28-_论文总结_ 深度学习在农业领域应用论文笔记11.html-4NW2lJZw.js" as="script"><link rel="prefetch" href="/assets/2024-02-10-_论文总结_ 深度学习在农业领域应用论文笔记12.html-DFRaSwIL.js" as="script"><link rel="prefetch" href="/assets/2024-09-25-_论文总结_ 深度学习在农业领域应用论文笔记13.html-CdrPecYz.js" as="script"><link rel="prefetch" href="/assets/2025-01-28-_论文总结_ 深度学习在农业领域应用论文笔记14.html-CRoBX_Hk.js" as="script"><link rel="prefetch" href="/assets/2017-11-13-_python_ tensorflow中的argmax()函数.html-DkjtmA1k.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_python_ 基于matplotlib实现树形图的绘制.html-ClhlZly5.js" as="script"><link rel="prefetch" href="/assets/2019-08-13-_python_ mxnet60分钟入门Gluon教程.html-Cx_4H4mv.js" as="script"><link rel="prefetch" href="/assets/2019-10-28-_python_ 基于NetworkX实现网络图的绘制.html-C2FrFpFE.js" as="script"><link rel="prefetch" href="/assets/2019-10-31-_python_ NetworkX实例.html-B4fbffwd.js" as="script"><link rel="prefetch" href="/assets/2019-11-15-_python_ 基于matplotlib_venn实现维恩图的绘制.html-CkioCn76.js" as="script"><link rel="prefetch" href="/assets/2019-12-30-_python_ CairoSVG使用教程.html-CjSbIoas.js" as="script"><link rel="prefetch" href="/assets/2020-03-25-_python_ 个人日常python工具代码.html-BtwMnHUL.js" as="script"><link rel="prefetch" href="/assets/2020-05-17-_python_ python模块graphviz使用入门.html-Bx7qI6EY.js" as="script"><link rel="prefetch" href="/assets/2020-09-01-_python_ 基于matplotlib实现圆环图的绘制.html-BQ_urbVp.js" as="script"><link rel="prefetch" href="/assets/2020-09-01-_python_ 基于matplotlib实现雷达图的绘制.html-aR3impjv.js" as="script"><link rel="prefetch" href="/assets/2021-07-20-_python_ Python二维码生成器qrcode库入门.html-DbehIUGX.js" as="script"><link rel="prefetch" href="/assets/2021-07-21-_python_ Python map函数总结.html-DSfyjAUH.js" as="script"><link rel="prefetch" href="/assets/2021-07-23-_python_ 圆形嵌套图Circular Packing.html-UzbR1UIM.js" as="script"><link rel="prefetch" href="/assets/2022-04-10-_python_ ​python-pinyin库.html-BEW9CaaD.js" as="script"><link rel="prefetch" href="/assets/2022-07-07-_python_ ​Python数据序列化模块pickle使用笔记.html-CAmDnFcf.js" as="script"><link rel="prefetch" href="/assets/2022-07-21-_python_ 向量检索库Faiss使用指北.html-CKGPoRRw.js" as="script"><link rel="prefetch" href="/assets/2022-07-25-_python_ 基于chardet识别字符编码.html-Dt0C1m72.js" as="script"><link rel="prefetch" href="/assets/2022-09-10-_python_ 基于diagrams库绘制系统架构图.html-DoCDEEx6.js" as="script"><link rel="prefetch" href="/assets/2022-09-19-_python_ 基于blind-watermark库添加图片盲水印.html-D9dzH-IU.js" as="script"><link rel="prefetch" href="/assets/2022-10-24-_python_ 基于Gradio可视化部署机器学习应用.html-Yyl53ZFf.js" as="script"><link rel="prefetch" href="/assets/2022-12-07-_python_ 基于wordcloud库绘制词云图.html-DbQwIWv1.js" as="script"><link rel="prefetch" href="/assets/2023-01-01-_python_ 基于paramiko库操作远程服务器.html-BkuYn7u-.js" as="script"><link rel="prefetch" href="/assets/2023-04-17-_python_ Python枚举模块enum总结.html-BMbShfyg.js" as="script"><link rel="prefetch" href="/assets/2023-05-10-_python_ Python类型提示总结.html-qAVkgMNP.js" as="script"><link rel="prefetch" href="/assets/2023-11-30-_python_ 基于Tablib库处理表格数据.html-ghCGwron.js" as="script"><link rel="prefetch" href="/assets/2023-12-29-_python_ 基于Dataset库操作数据库.html-iJJ3ING2.js" as="script"><link rel="prefetch" href="/assets/2024-01-25-_python_ 基于RapidFuzz库实现字符串模糊匹配.html-Da28EYCL.js" as="script"><link rel="prefetch" href="/assets/2024-04-30-_python_ 基于PyWaffle库绘制华夫饼图.html-BZ7t14_U.js" as="script"><link rel="prefetch" href="/assets/2024-06-30-_python_ Python日志记录库loguru使用指北.html-CTxscKT6.js" as="script"><link rel="prefetch" href="/assets/2024-07-30-_python_ 启发式算法库scikit-opt使用指北.html-61oOz43W.js" as="script"><link rel="prefetch" href="/assets/2024-08-10-_python_ Python并行计算库Joblib使用指北.html-cV5TTpVp.js" as="script"><link rel="prefetch" href="/assets/2024-10-01-_python_ 基于PyOD库实现数据异常检测.html-B56VH8vt.js" as="script"><link rel="prefetch" href="/assets/2024-11-22-_python_ Python异步编程库asyncio使用指北.html-DdZQZ1YB.js" as="script"><link rel="prefetch" href="/assets/2024-11-25-_python_ asyncio库常见问题与实践案例.html-CH_tDXWe.js" as="script"><link rel="prefetch" href="/assets/2025-03-24-_python_ 使用Python实现Markdown文档格式转换.html-CqKGPAkH.js" as="script"><link rel="prefetch" href="/assets/2025-04-27-_python_ 基于WatchDog库实现文件系统监控.html-DJZ4t4-E.js" as="script"><link rel="prefetch" href="/assets/2025-05-20-_python_ 轻量级定时任务调度库schedule使用指北.html-B9wPqpDH.js" as="script"><link rel="prefetch" href="/assets/2025-06-03-_python_ python抽象基类使用总结.html-DjbcKqK1.js" as="script"><link rel="prefetch" href="/assets/2025-10-24-_python_ 代码性能分析工具line_profiler使用指北.html-B5EIJOiz.js" as="script"><link rel="prefetch" href="/assets/2019-03-04-_OpenCV实战_1 基于深度学习识别人脸性别和年龄.html-BnKbCaL8.js" as="script"><link rel="prefetch" href="/assets/2019-03-05-_OpenCV实战_2 人脸识别算法对比.html-CHRTj-d3.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_3 透明斗篷.html-CH8dn1w9.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_4 OpenCV中的颜色空间.html-BRAVSUME.js" as="script"><link rel="prefetch" href="/assets/2019-03-06-_OpenCV实战_5 基于深度学习的文本检测.html-CDIdNOWI.js" as="script"><link rel="prefetch" href="/assets/2019-03-08-_OpenCV实战_6 基于特征点匹配的视频稳像.html-BcXBlslZ.js" as="script"><link rel="prefetch" href="/assets/2019-03-13-_OpenCV实战_7 使用YOLOv3和OpenCV进行基于深度学习的目标检测.html-3R38gVaA.js" as="script"><link rel="prefetch" href="/assets/2019-03-15-_OpenCV实战_8 深度学习目标检测网络YOLOv3的训练.html-Bzs7MZWl.js" as="script"><link rel="prefetch" href="/assets/2019-03-16-_OpenCV实战_10 使用Hu矩进行形状匹配.html-Gjq6Dx6d.js" as="script"><link rel="prefetch" href="/assets/2019-03-16-_OpenCV实战_9 使用OpenCV寻找平面图形的质心.html-B8cN0FQT.js" as="script"><link rel="prefetch" href="/assets/2019-03-19-_OpenCV实战_11 基于OpenCV的二维码扫描器.html-CL1ADYNE.js" as="script"><link rel="prefetch" href="/assets/2019-03-27-_OpenCV实战_12 使用深度学习和OpenCV进行手部关键点检测.html-BRwQNhBi.js" as="script"><link rel="prefetch" href="/assets/2019-04-02-_OpenCV实战_13 OpenCV中使用Mask R-CNN进行对象检测和实例分割.html-Bbe6GRMU.js" as="script"><link rel="prefetch" href="/assets/2019-04-04-_OpenCV实战_14 使用OpenCV实现单目标跟踪.html-BNE8KZeu.js" as="script"><link rel="prefetch" href="/assets/2019-04-08-_OpenCV实战_15 基于深度学习的目标跟踪算法GOTURN.html-Bs-R_B-M.js" as="script"><link rel="prefetch" href="/assets/2019-04-08-_OpenCV实战_16 使用OpenCV实现多目标跟踪.html-BwIXkB5G.js" as="script"><link rel="prefetch" href="/assets/2019-04-10-_OpenCV实战_17 基于卷积神经网络的OpenCV图像着色.html-D5txbCkK.js" as="script"><link rel="prefetch" href="/assets/2019-04-16-_OpenCV实战_18 OpenCV中的单应性矩阵Homography.html-Dl-qgk2v.js" as="script"><link rel="prefetch" href="/assets/2019-04-17-_OpenCV实战_19 使用OpenCV实现基于特征的图像对齐.html-BDjOJmZQ.js" as="script"><link rel="prefetch" href="/assets/2019-04-22-_OpenCV实战_20 使用OpenCV实现基于增强相关系数最大化的图像对齐.html-CLvKA1Cq.js" as="script"><link rel="prefetch" href="/assets/2019-04-23-_OpenCV实战_21 使用OpenCV的Eigenface.html-COqrTvrb.js" as="script"><link rel="prefetch" href="/assets/2019-04-24-_OpenCV实战_22 使用EigenFaces进行人脸重建.html-CjIjO9yn.js" as="script"><link rel="prefetch" href="/assets/2019-04-30-_OpenCV实战_23 使用OpenCV获取高动态范围成像HDR.html-n7tJcipG.js" as="script"><link rel="prefetch" href="/assets/2019-05-05-_OpenCV实战_24 使用OpenCV进行曝光融合.html-CPbTSKw6.js" as="script"><link rel="prefetch" href="/assets/2019-05-05-_OpenCV实战_25 使用OpenCV进行泊松克隆.html-CcnpNLg0.js" as="script"><link rel="prefetch" href="/assets/2019-05-06-_OpenCV实战_26 基于OpenCV实现选择性搜索算法.html-CSecjk6j.js" as="script"><link rel="prefetch" href="/assets/2019-05-07-_OpenCV实战_27 在OpenCV下使用forEach进行并行像素访问.html-DSEk85it.js" as="script"><link rel="prefetch" href="/assets/2019-05-08-_OpenCV实战_28 基于OpenCV的GUI库cvui.html-CNnwQJDv.js" as="script"><link rel="prefetch" href="/assets/2019-05-09-_OpenCV实战_29 使用OpenCV实现红眼自动去除.html-DtsMDMx6.js" as="script"><link rel="prefetch" href="/assets/2019-05-10-_OpenCV实战_30 使用OpenCV实现图像孔洞填充.html-CTpbPlyl.js" as="script"><link rel="prefetch" href="/assets/2019-05-23-_OpenCV实战_31 使用OpenCV将一个三角形仿射变换到另一个三角形.html-Bjre_gk8.js" as="script"><link rel="prefetch" href="/assets/2019-05-24-_OpenCV实战_32 使用OpenCV进行非真实感渲染.html-DWmUL62p.js" as="script"><link rel="prefetch" href="/assets/2019-05-27-_OpenCV实战_33 使用OpenCV进行Hough变换.html-COyTPz5P.js" as="script"><link rel="prefetch" href="/assets/2019-05-28-_OpenCV实战_34 使用OpenCV进行图像修复.html-DUe2QdO0.js" as="script"><link rel="prefetch" href="/assets/2019-07-16-_OpenCV实战_35 使用Tesseract和OpenCV实现文本识别.html-SSx9F_Gg.js" as="script"><link rel="prefetch" href="/assets/2019-08-30-_OpenCV实战_36 使用OpenCV在视频中实现简单背景估计.html-qa59kjAm.js" as="script"><link rel="prefetch" href="/assets/2020-02-29-_OpenCV实战_37 图像质量评价BRISQUE.html-CSgL5bBu.js" as="script"><link rel="prefetch" href="/assets/2020-03-06-_OpenCV实战_38 基于OpenCV的相机标定.html-B3H8U79Z.js" as="script"><link rel="prefetch" href="/assets/2020-03-31-_OpenCV实战_39 在OpenCV中使用ArUco标记的增强现实.html-CTrhSgPu.js" as="script"><link rel="prefetch" href="/assets/2020-05-07-_OpenCV实战_40 计算机视觉工具对比.html-BS2ZhYS4.js" as="script"><link rel="prefetch" href="/assets/2020-05-10-_OpenCV实战_41 嵌入式计算机视觉设备选择.html-DOodkqti.js" as="script"><link rel="prefetch" href="/assets/2020-05-12-_OpenCV实战_42 数码单反相机的技术细节.html-tlSXt0xD.js" as="script"><link rel="prefetch" href="/assets/2020-08-14-_OpenCV实战_43 使用OpenCV进行背景分割.html-BFc_0MRu.js" as="script"><link rel="prefetch" href="/assets/2020-08-24-_OpenCV实战_44 使用OpenCV进行图像超分放大.html-BL5uqa79.js" as="script"><link rel="prefetch" href="/assets/2020-08-27-_OpenCV实战_45 基于OpenCV实现图像哈希算法.html-BDBJTTdM.js" as="script"><link rel="prefetch" href="/assets/2020-09-10-_OpenCV实战_46 在OpenCV下应用图像强度变换实现图像对比度均衡.html-C03Y7P59.js" as="script"><link rel="prefetch" href="/assets/2020-09-15-_OpenCV实战_47 基于OpenCV实现视觉显著性检测.html-CFGvt4ii.js" as="script"><link rel="prefetch" href="/assets/2020-10-09-_OpenCV实战_48 基于OpenCV实现图像质量评价.html-an47GYY1.js" as="script"><link rel="prefetch" href="/assets/2021-02-12-_OpenCV实战_49 对极几何与立体视觉初探.html-DM-BfYgg.js" as="script"><link rel="prefetch" href="/assets/2021-02-16-_OpenCV实战_50 用OpenCV制作低成本立体相机.html-CBX46Bvy.js" as="script"><link rel="prefetch" href="/assets/2021-03-15-_OpenCV实战_51 基于OpenCV实现图像极坐标变换与逆变换.html-C_0W835u.js" as="script"><link rel="prefetch" href="/assets/2022-12-01-_OpenCV实战_52 在OpenCV中使用颜色直方图.html-DA-0rL6Z.js" as="script"><link rel="prefetch" href="/assets/index.html-mk7lBBZn.js" as="script"><link rel="prefetch" href="/assets/2019-06-25-_python_《Python编程快速上手让繁琐工作自动化》学习笔记1.html-L_ieBeuK.js" as="script"><link rel="prefetch" href="/assets/2019-06-26-_python_《Python编程快速上手让繁琐工作自动化》学习笔记2.html-B68I0T4X.js" as="script"><link rel="prefetch" href="/assets/2019-06-28-_python_《Python编程快速上手让繁琐工作自动化》学习笔记3.html-CRS9Fz0U.js" as="script"><link rel="prefetch" href="/assets/2019-07-05-_python_《Python编程快速上手让繁琐工作自动化》学习笔记4.html-BZRJ5gi2.js" as="script"><link rel="prefetch" href="/assets/2019-07-27-_python_《Python编程快速上手让繁琐工作自动化》学习笔记5.html-Dmbic_hD.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_python_《Python编程快速上手让繁琐工作自动化》学习笔记6.html-BoIT9_xc.js" as="script"><link rel="prefetch" href="/assets/2019-08-02-_python_《Python编程快速上手让繁琐工作自动化》学习笔记7.html-Bq9AyNbv.js" as="script"><link rel="prefetch" href="/assets/2019-05-28-_seaborn_ seaborn学习笔记0 seaborn学习笔记章节.html-BsuqrLDg.js" as="script"><link rel="prefetch" href="/assets/2019-05-29-_seaborn_ seaborn学习笔记1 箱形图Boxplot.html-oBSgQ16Q.js" as="script"><link rel="prefetch" href="/assets/2019-05-30-_seaborn_ seaborn学习笔记2 散点图Scatterplot.html-BD_Dx0hw.js" as="script"><link rel="prefetch" href="/assets/2019-05-30-_seaborn_ seaborn学习笔记3 直方图Histogramplot.html-DC2Tdz2Q.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记4 核密度图DENSITYPLOT.html-DyEYXPLf.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记5 小提琴图VIOLINPLOT.html-06AgwkfD.js" as="script"><link rel="prefetch" href="/assets/2019-05-31-_seaborn_ seaborn学习笔记6 热图HEATMAPPLOT.html-DCDEfWML.js" as="script"><link rel="prefetch" href="/assets/2019-06-01-_seaborn_ seaborn学习笔记7 常用参数调整Adjustment of Common Parameters.html-BqIKepnk.js" as="script"><link rel="prefetch" href="/assets/2019-06-01-_seaborn_ seaborn学习笔记8 避免过度绘图Avoid Overplotting.html-3DwQ-bqJ.js" as="script"><link rel="prefetch" href="/assets/2019-06-05-_seaborn_ seaborn学习笔记10 绘图实例(2) Drawing example(2).html-CY2re5UW.js" as="script"><link rel="prefetch" href="/assets/2019-06-05-_seaborn_ seaborn学习笔记9 绘图实例(1) Drawing example(1).html-_sKdfPd5.js" as="script"><link rel="prefetch" href="/assets/2019-06-06-_seaborn_ seaborn学习笔记11 绘图实例(3) Drawing example(3).html-BbJR8aAJ.js" as="script"><link rel="prefetch" href="/assets/2019-06-06-_seaborn_ seaborn学习笔记12 绘图实例(4) Drawing example(4).html-B7-YuRbD.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记1—ggplot2简要教程.html-uEELJKj-.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记2—通用教程ggplot2简介.html-1kC-EVb7.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记3—通用教程如何自定义ggplot2.html-jHpfV85G.js" as="script"><link rel="prefetch" href="/assets/2020-03-21-_R语言_ ggplot2入门笔记4—前50个ggplot2可视化效果.html-CmMwRM-Q.js" as="script"><link rel="prefetch" href="/assets/index.html-F7jFB9cX.js" as="script"><link rel="prefetch" href="/assets/2020-09-05-_R语言_ 基于R语言实现树形图的绘制.html-Ct5jsRzU.js" as="script"><link rel="prefetch" href="/assets/2020-09-05-_R语言_ 基于R语言实现环状条形图的绘制.html-CWKBwF2U.js" as="script"><link rel="prefetch" href="/assets/2019-07-31-_R语言_ R语言PCA分析教程 Principal Component Methods in R.html-Czu_QJpF.js" as="script"><link rel="prefetch" href="/assets/2020-01-10-_R语言_ WGCNA入门教程.html-GV5ZDNgT.js" as="script"><link rel="prefetch" href="/assets/2021-02-05-_R语言_ R语言快速入门教程.html-BhyYBWR3.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门1.html-C1pkZPlE.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门2.html-DPD9CESn.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门3.html-D3sdrc5o.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门4.html-BJym4IhN.js" as="script"><link rel="prefetch" href="/assets/2018-12-11-_图像处理_ YUV图像处理入门5.html-IpROHvLM.js" as="script"><link rel="prefetch" href="/assets/index.html-hKMNX-_1.js" as="script"><link rel="prefetch" href="/assets/2021-11-21-_数据与分析可视化_ D3入门教程1-d3基础知识.html-DWCMJuA8.js" as="script"><link rel="prefetch" href="/assets/2021-11-28-_数据与分析可视化_ D3入门教程2-在d3中构建形状.html-BIRXnijT.js" as="script"><link rel="prefetch" href="/assets/2021-12-05-_数据与分析可视化_ D3入门教程3-d3中的数据操作.html-CeRyf-ym.js" as="script"><link rel="prefetch" href="/assets/2023-03-16-_数据分析与可视化_ Python绘制数据地图1-GeoPandas入门指北.html-B6Gln0rK.js" as="script"><link rel="prefetch" href="/assets/2023-04-09-_数据分析与可视化_ Python绘制数据地图2-GeoPandas地图可视化.html-B1xnJf_t.js" as="script"><link rel="prefetch" href="/assets/2023-06-16-_数据分析与可视化_ Python绘制数据地图3-GeoPandas使用要点.html-UNAi-BPT.js" as="script"><link rel="prefetch" href="/assets/2023-08-03-_数据分析与可视化_ Python绘制数据地图4-MovingPandas入门指北.html-DZrgBDil.js" as="script"><link rel="prefetch" href="/assets/2023-08-11-_数据分析与可视化_ Python绘制数据地图5-MovingPandas绘图实例.html-BBLQTuFM.js" as="script"><link rel="prefetch" href="/assets/2023-06-28-_数据分析与可视化_ 基于matplotlib-scalebar库绘制比例尺.html-BI4YLk4x.js" as="script"><link rel="prefetch" href="/assets/2023-07-10-_数据分析与可视化_ 基于matplotlib和plottable库绘制精美表格.html-9FAGxkIk.js" as="script"><link rel="prefetch" href="/assets/2023-10-24-_数据分析与可视化_ 基于Python绘制简单动图.html-D4Kefaq7.js" as="script"><link rel="prefetch" href="/assets/2021-11-14-_数据分析与可视化_ 数据绘图要点1-注重数据排序.html-DVVrwI44.js" as="script"><link rel="prefetch" href="/assets/2021-11-18-_数据分析与可视化_ 数据绘图要点2-Y轴的开始与结束.html-4lCKCWoc.js" as="script"><link rel="prefetch" href="/assets/2021-11-24-_数据分析与可视化_ 数据绘图要点3-意大利面条图.html-CQTYBAcn.js" as="script"><link rel="prefetch" href="/assets/2021-12-01-_数据分析与可视化_ 数据绘图要点4-饼图的问题.html-0UeMSQqi.js" as="script"><link rel="prefetch" href="/assets/2021-12-09-_数据分析与可视化_ 数据绘图要点5-误差线的问题.html-DhzzPi1A.js" as="script"><link rel="prefetch" href="/assets/2021-12-19-_数据分析与可视化_ 数据绘图要点6-数据组过多.html-DxlKtPtk.js" as="script"><link rel="prefetch" href="/assets/2021-12-25-_数据分析与可视化_ 数据绘图要点7-过度绘图.html-D-Ae8_u7.js" as="script"><link rel="prefetch" href="/assets/2021-12-29-_数据分析与可视化_ 数据绘图要点8-环状条形图的使用.html-C1VAhFyI.js" as="script"><link rel="prefetch" href="/assets/2022-01-01-_数据分析与可视化_ 数据绘图要点9-颜色的选择.html-CTGyFg8U.js" as="script"><link rel="prefetch" href="/assets/2022-01-06-_数据分析与可视化_ 数据绘图要点10-图例的构建.html-DayQ3cm0.js" as="script"><link rel="prefetch" href="/assets/2022-01-12-_数据分析与可视化_ 数据绘图要点11-雷达图的注意事项.html-Bsu8Dlza.js" as="script"><link rel="prefetch" href="/assets/2022-01-18-_数据分析与可视化_ 数据绘图要点12-图表注释的重要性.html-Cmw0mw4H.js" as="script"><link rel="prefetch" href="/assets/2017-12-10-_机器学习_ 集成学习简单投票法概率.html-C6DrSh3W.js" as="script"><link rel="prefetch" href="/assets/2018-04-17-_机器学习_ sklearn聚类.html-LNhHAFbP.js" as="script"><link rel="prefetch" href="/assets/2018-04-19-_机器学习_ sklearn决策树、随机森林、隐马尔可夫模型.html-B9UuoszF.js" as="script"><link rel="prefetch" href="/assets/2018-04-19-_机器学习_ sklearn朴素贝叶斯算法.html-Ckfxl7XD.js" as="script"><link rel="prefetch" href="/assets/2018-04-21-_机器学习_ sklearn支持向量机.html-DM6X7BKq.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记1-快速入门.html-ChAURZ8Q.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记2-模型选择.html-BaSkOjx4.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记3-特征分析可视化.html-w7uLvbFI.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记4-目标可视化文件.html-D-LfLc49.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记5-回归可视化.html-Be6Xmj4W.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记6-分类可视化.html-0WbfELp4.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记7-聚类可视化.html-DjoaqHQY.js" as="script"><link rel="prefetch" href="/assets/2020-07-25-_机器学习_ Yellowbrick使用笔记8-模型选择可视化.html-DrTzqQqG.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记1-删除低方差的特征.html-Dt5TCZ4Q.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记2-单变量特征选择.html-B9mqV5Vn.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记3-递归式特征消除.html-DZRj0EJS.js" as="script"><link rel="prefetch" href="/assets/2020-07-09-_机器学习_ 特征选择笔记4-使用SelectFromModel特征选择.html-kIYsRYls.js" as="script"><link rel="prefetch" href="/assets/2024-12-31-_深度学习_ 大模型学习1-大语言模型基础知识.html-Bihg0SCe.js" as="script"><link rel="prefetch" href="/assets/2025-02-28-_深度学习_ 大模型学习2-提示词工程指北.html-CAnmndU-.js" as="script"><link rel="prefetch" href="/assets/2025-07-21-_深度学习_ 大模型学习3上-模型训练与微调.html-DMU-aWll.js" as="script"><link rel="prefetch" href="/assets/2025-08-08-_深度学习_ 大模型学习4-RAG技术全景解析.html-DITo2uQq.js" as="script"><link rel="prefetch" href="/assets/2025-10-01-_深度学习_ 大模型学习5-高效微调框架Unsloth使用指北.html-Kh37C-sA.js" as="script"><link rel="prefetch" href="/assets/2017-10-11-_深度学习_ 网易云课堂深度学习工程师微专业相关资料.html-D0fPCeaT.js" as="script"><link rel="prefetch" href="/assets/2017-10-12-_深度学习_ 深度学习快速入门资料.html-DWuyL2oq.js" as="script"><link rel="prefetch" href="/assets/2017-10-13-_深度学习_ 卷积神经网络快速入门.html-qgd_i4O5.js" as="script"><link rel="prefetch" href="/assets/2017-11-25-_深度学习_ 深度学习中卷积操作和数学中卷积操作的异同.html-COak7kvh.js" as="script"><link rel="prefetch" href="/assets/2018-07-17-_深度学习_ tf.keras入门1-基本函数介绍.html-DCeqEKut.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门2-分类.html-DQ1qWMpc.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门3-回归.html-OZr-TODD.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门4-过拟合和欠拟合.html-BtS_6ilj.js" as="script"><link rel="prefetch" href="/assets/2018-07-18-_深度学习_ tf.keras入门5-模型保存和载入.html-B1yyZbkq.js" as="script"><link rel="prefetch" href="/assets/2018-07-27-_深度学习_ 经典深度学习模型及其微调（Caffe)总结.html-DtXXt1fm.js" as="script"><link rel="prefetch" href="/assets/2019-07-23-_深度学习_ ncnn安装和调用基础教程.html-mla9LpAW.js" as="script"><link rel="prefetch" href="/assets/2019-08-10-_深度学习_ caffe分类模型训练、结果可视化、部署及量化笔记.html-BjydIN_i.js" as="script"><link rel="prefetch" href="/assets/2019-09-30-_深度学习_ ncnn编译使用.html-bO7MGxCT.js" as="script"><link rel="prefetch" href="/assets/2020-08-07-_深度学习_ ImageAI库使用笔记.html-dO51fmVU.js" as="script"><link rel="prefetch" href="/assets/2020-10-24-_深度学习_ imgaug库使用笔记.html-BN1SjGv9.js" as="script"><link rel="prefetch" href="/assets/2020-11-19-_深度学习_ 深度学习优化器选择学习笔记.html-BCN3-7OZ.js" as="script"><link rel="prefetch" href="/assets/2020-12-09-_深度学习_ Pytorch模型转换为onnx模型笔记.html-SkuWAeMq.js" as="script"><link rel="prefetch" href="/assets/2021-01-14-_深度学习_ ubuntu18.04配置深度学习环境笔记.html-CUJooeLT.js" as="script"><link rel="prefetch" href="/assets/2021-02-02-_深度学习_ imgaug边界框增强笔记.html-DEpFyeyD.js" as="script"><link rel="prefetch" href="/assets/2021-06-09-_深度学习_ CCPD车牌数据集介绍.html-BlQVLzjc.js" as="script"><link rel="prefetch" href="/assets/2022-01-14-_深度学习_ fast-reid入门教程.html-Dgr0eVF4.js" as="script"><link rel="prefetch" href="/assets/2022-02-26-_深度学习_ Python人脸识别库face_recognition使用教程.html-Wbd5rvzA.js" as="script"><link rel="prefetch" href="/assets/2022-07-02-_深度学习_ Python人脸识别库Deepface使用教程.html-HqRv53aU.js" as="script"><link rel="prefetch" href="/assets/2022-11-24-_深度学习_ 搭建行人重识别系统心得.html-RWB2MYyE.js" as="script"><link rel="prefetch" href="/assets/2023-01-03-_深度学习_ 基于切片辅助超推理库SAHI优化小目标识别.html-D3YAfHBx.js" as="script"><link rel="prefetch" href="/assets/2024-03-18-_深度学习_ 计算机视觉低代码工具Supervision库使用指北.html-BRlabnrC.js" as="script"><link rel="prefetch" href="/assets/2024-08-28-_深度学习_ 时间序列分析工具TSLiB库使用指北.html-48pm1e_O.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门1-创建线程的三种不同方式.html-CEVm5uIQ.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门10-packaged_task示例.html-DocMoJhX.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门2-连接和分离线程.html-NL52Y4WU.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门3-小心地将参数传递给线程.html-OJzhYgeX.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门4-数据共享和资源竞争.html-Ds5fjYsz.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门5-使用互斥锁解决资源竞争.html-QD-7To9Z.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门6-事件处理的需求.html-C1r3a8kd.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门7-条件变量介绍.html-Bdz8H8YF.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门8-从线程返回值.html-BXsNyfs0.js" as="script"><link rel="prefetch" href="/assets/2020-05-29-_编程基础_ C__多线程入门9-async教程和示例.html-SiBpmzBy.js" as="script"><link rel="prefetch" href="/assets/2021-10-24-_编程基础_ 常用html标签使用介绍.html-AaJqnKDW.js" as="script"><link rel="prefetch" href="/assets/2017-11-02-_编程基础_ C_自定义类调用窗体控件.html-Dt9lAyct.js" as="script"><link rel="prefetch" href="/assets/2020-05-09-_编程基础_ C和C__内置宏说明.html-CodRHg4y.js" as="script"><link rel="prefetch" href="/assets/2020-06-17-_编程基础_ Python格式化字符串常量f-string总结.html-BOp1GFLb.js" as="script"><link rel="prefetch" href="/assets/2020-06-20-_编程基础_ Python谷歌翻译库googletrans总结.html-Cpv6UlMd.js" as="script"><link rel="prefetch" href="/assets/2020-06-21-_编程基础_ Python数据生成库Faker总结.html-DXZB7D_9.js" as="script"><link rel="prefetch" href="/assets/2020-06-21-_编程基础_ Python配置文件读取库ConfigParser总结.html-D8ceiP57.js" as="script"><link rel="prefetch" href="/assets/2020-06-23-_编程基础_ Python日志记录库logging总结.html-DoQ8bQot.js" as="script"><link rel="prefetch" href="/assets/2020-06-24-_编程基础_ Python随机数生成模块总结.html-Czu3Olw5.js" as="script"><link rel="prefetch" href="/assets/2020-06-25-_编程基础_ Python lambda函数总结.html-DmzyLDq0.js" as="script"><link rel="prefetch" href="/assets/2020-06-25-_编程基础_ Python装饰器入门总结.html-DD6nrhIn.js" as="script"><link rel="prefetch" href="/assets/2020-06-26-_编程基础_ Python列表解析总结.html-2FT6dQUC.js" as="script"><link rel="prefetch" href="/assets/2020-08-01-_编程基础_ Python中的绝对导入与相对导入.html-D-kgzsR0.js" as="script"><link rel="prefetch" href="/assets/2020-08-01-_编程基础_ Python模块和包使用笔记.html-B1JZc-v_.js" as="script"><link rel="prefetch" href="/assets/2020-08-02-_编程基础_ Python对象的浅拷贝与深拷贝笔记.html-C3eV9bNf.js" as="script"><link rel="prefetch" href="/assets/2020-10-14-_编程基础_ Python中args和kwargs参数的使用.html-C7g2eATk.js" as="script"><link rel="prefetch" href="/assets/2020-10-31-_编程基础_ Python命令行解析库argparse学习笔记.html-CSxN_erG.js" as="script"><link rel="prefetch" href="/assets/2021-08-16-_编程基础_ Python字符串替换笔记.html-DWEYK_j_.js" as="script"><link rel="prefetch" href="/assets/2023-09-05-_编程基础_ Python内置模块collections使用笔记.html-BHXC7z3a.js" as="script"><link rel="prefetch" href="/assets/404.html-D48VWrFv.js" as="script"><link rel="prefetch" href="/assets/index.html-CJLCbGgK.js" as="script"><link rel="prefetch" href="/assets/index.html-pCKj9wSE.js" as="script"><link rel="prefetch" href="/assets/index.html-WWMD7EPl.js" as="script"><link rel="prefetch" href="/assets/index.html-B869YxfH.js" as="script"><link rel="prefetch" href="/assets/index.html-BhZvVtO2.js" as="script"><link rel="prefetch" href="/assets/index.html-BRKyYzBl.js" as="script"><link rel="prefetch" href="/assets/index.html-CDrWD1P8.js" as="script"><link rel="prefetch" href="/assets/index.html-JUJPpe1l.js" as="script"><link rel="prefetch" href="/assets/index.html-BEKZDv3H.js" as="script"><link rel="prefetch" href="/assets/index.html-DTG3YjWG.js" as="script"><link rel="prefetch" href="/assets/index.html-CtqtGAYx.js" as="script"><link rel="prefetch" href="/assets/index.html-DTtrZrGn.js" as="script"><link rel="prefetch" href="/assets/index.html-BMYV2MgQ.js" as="script"><link rel="prefetch" href="/assets/index.html-D98NEHXk.js" as="script"><link rel="prefetch" href="/assets/index.html-6NeWiwdL.js" as="script"><link rel="prefetch" href="/assets/index.html-DNOn1htr.js" as="script"><link rel="prefetch" href="/assets/index.html-CToYOmNQ.js" as="script"><link rel="prefetch" href="/assets/index.html-B4EYwIYk.js" as="script"><link rel="prefetch" href="/assets/index.html-C2ntcOFG.js" as="script"><link rel="prefetch" href="/assets/index.html-WeZphgAN.js" as="script"><link rel="prefetch" href="/assets/index.html-YoL9HC1h.js" as="script"><link rel="prefetch" href="/assets/index.html-DSgytngG.js" as="script"><link rel="prefetch" href="/assets/index.html-hBQEf_Yc.js" as="script"><link rel="prefetch" href="/assets/index.html-DI-HAaOP.js" as="script"><link rel="prefetch" href="/assets/index.html-kIbNbN47.js" as="script"><link rel="prefetch" href="/assets/index.html-CPpMvAAZ.js" as="script"><link rel="prefetch" href="/assets/index.html-C484w4wo.js" as="script"><link rel="prefetch" href="/assets/index.html-Dm47NPkL.js" as="script"><link rel="prefetch" href="/assets/index.html-Cu51DFn_.js" as="script"><link rel="prefetch" href="/assets/index.html-BSqZB4al.js" as="script"><link rel="prefetch" href="/assets/index.html-EhGOUNjz.js" as="script"><link rel="prefetch" href="/assets/index.html-BPAGpY_M.js" as="script"><link rel="prefetch" href="/assets/index.html-BqK7hLD-.js" as="script"><link rel="prefetch" href="/assets/index.html--JOn8dCl.js" as="script"><link rel="prefetch" href="/assets/index.html-DjNKAKaO.js" as="script"><link rel="prefetch" href="/assets/index.html-9rk7OVGa.js" as="script"><link rel="prefetch" href="/assets/index.html-CGATpSvT.js" as="script"><link rel="prefetch" href="/assets/index.html-ByNcQHnq.js" as="script"><link rel="prefetch" href="/assets/index.html-CCfZ2xVk.js" as="script"><link rel="prefetch" href="/assets/index.html-DvCvYUAR.js" as="script"><link rel="prefetch" href="/assets/index.html-BSiHg2ya.js" as="script"><link rel="prefetch" href="/assets/index.html-D04oSy5V.js" as="script"><link rel="prefetch" href="/assets/index.html-Dq_FGEcY.js" as="script"><link rel="prefetch" href="/assets/index.html-RGwWhHaL.js" as="script"><link rel="prefetch" href="/assets/index.html-DShK_W7P.js" as="script"><link rel="prefetch" href="/assets/index.html-Bm2_gqBv.js" as="script"><link rel="prefetch" href="/assets/index.html-mTiLUQ2F.js" as="script"><link rel="prefetch" href="/assets/index.html-DWyX0ur1.js" as="script"><link rel="prefetch" href="/assets/index.html-C3t02tlK.js" as="script"><link rel="prefetch" href="/assets/index.html-DHOxM711.js" as="script"><link rel="prefetch" href="/assets/index.html-dsGbDYul.js" as="script"><link rel="prefetch" href="/assets/index.html-DKjHahan.js" as="script"><link rel="prefetch" href="/assets/index.html-DaGXsi4-.js" as="script"><link rel="prefetch" href="/assets/index.html-B7Tt6tvO.js" as="script"><link rel="prefetch" href="/assets/index.html-CN6T4Q_B.js" as="script"><link rel="prefetch" href="/assets/index.html-VD6zeWpH.js" as="script"><link rel="prefetch" href="/assets/index.html-DZ5jbvGm.js" as="script"><link rel="prefetch" href="/assets/index.html-C3WAJ7eJ.js" as="script"><link rel="prefetch" href="/assets/index.html-C9c1YeCQ.js" as="script"><link rel="prefetch" href="/assets/index.html-DYJWH9wb.js" as="script"><link rel="prefetch" href="/assets/index.html-C6mLuqDE.js" as="script"><link rel="prefetch" href="/assets/index.html-DZn1gVyD.js" as="script"><link rel="prefetch" href="/assets/index.html-BxlmUFAF.js" as="script"><link rel="prefetch" href="/assets/index.html-LRxP8P4B.js" as="script"><link rel="prefetch" href="/assets/index.html-Dmi1zZYb.js" as="script"><link rel="prefetch" href="/assets/index.html-C-GhexBQ.js" as="script"><link rel="prefetch" href="/assets/index.html-CR-equgD.js" as="script"><link rel="prefetch" href="/assets/index.html-CDZM2pc9.js" as="script"><link rel="prefetch" href="/assets/index.html-CQqlfi4P.js" as="script"><link rel="prefetch" href="/assets/index.html-DUirfBXV.js" as="script"><link rel="prefetch" href="/assets/index.html-B7UT4ojE.js" as="script"><link rel="prefetch" href="/assets/index.html-q7SdGqBs.js" as="script"><link rel="prefetch" href="/assets/flowchart-CAFN9Lqb.js" as="script"><link rel="prefetch" href="/assets/mermaid.esm.min-DPbUqLDP.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CMg0yb1C.js" as="script"><link rel="prefetch" href="/assets/browser-CYdOP0d3.js" as="script"><link rel="prefetch" href="/assets/SearchResult-DO0qc5pT.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-DeOkXxfI.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="博客主页" iconsizing="height"><!--[--><i class="vp-icon fas fa-home" sizing="height"></i><!--]-->博客主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/blog/" aria-label="个人博客" iconsizing="height"><!--[--><i class="vp-icon fas fa-blog" sizing="height"></i><!--]-->个人博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="关于"><!--[--><i class="vp-icon fas fa-user-plus" sizing="height"></i>关于<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/intro.html" aria-label="关于我" iconsizing="both"><!--[--><i class="vp-icon fas fa-user fa-fw" sizing="both"></i><!--]-->关于我<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/about.html" aria-label="关于本站" iconsizing="both"><!--[--><i class="vp-icon fas fa-circle-info fa-fw" sizing="both"></i><!--]-->关于本站<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-center"><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://blog.csdn.net/LuohenYJ" target="_blank" title="开往我的csdn" rel="noopener noreferrer" aria-label="travelling"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" fill="currentColor" style="width:1.25rem;height:1.25rem;vertical-align:middle"><path d="M658.836 519.32c-22.121 0-40.145 18.431-40.145 40.957 0 22.528 18.023 40.962 40.145 40.962 22.117 0 40.141-18.434 40.141-40.962 0-22.526-18.024-40.957-40.141-40.957zM364.742 519.32c-22.121 0-40.141 18.431-40.141 40.957 0.41 22.528 18.02 40.962 40.141 40.962 22.117 0 40.141-18.434 40.141-40.962 0-22.526-18.024-40.957-40.141-40.957z" p-id="8700"></path><path d="M512 0C229.23 0 0 229.23 0 512s229.23 512 512 512 512-229.23 512-512S794.77 0 512 0z m133.727 804.81c0 7.375-6.145 13.52-13.516 13.52H391.773c-7.371 0-13.515-6.145-13.515-13.52v-13.516c0-7.371 6.144-13.517 13.515-13.517h240.438c7.371 0 13.516 6.146 13.516 13.517v13.516z m120.832 37.273c-12.289 6.965-27.441 2.867-34.406-9.418l-54.887-96.668c-4.504 0.82-9.422 1.23-13.926 1.23H361.054c-4.914 0-9.421-0.41-13.925-1.23l-54.887 96.668c-6.965 12.285-22.527 16.383-34.406 9.418-12.289-6.961-16.383-22.938-9.422-35.223l51.199-90.113c-27.031-19.66-43.418-52.43-40.957-88.883l19.25-293.273c3.277-49.152 34.406-88.066 87.246-88.066h80.281c0-37.684 29.899-67.993 66.762-67.993 36.868 0 66.766 30.309 66.766 67.993h93.391c53.246 0 70.449 38.914 73.727 88.066l19.25 293.273c2.461 36.453-13.926 69.223-40.957 88.883l51.199 90.113c6.964 12.696 2.867 28.262-9.012 35.223z" p-id="8701"></path><path d="M672.352 314.931H351.633c-14.747 0-26.622 12.285-26.622 27.441v108.953c0 15.157 11.875 27.446 26.622 27.446h320.719c14.746 0 26.625-12.289 26.625-27.446V342.372c0-15.156-11.879-27.441-26.625-27.441z" p-id="8702"></svg></a></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/luohenyueji" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--></div><div class="vp-navbar-end"><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/logo.png" alt><!----><span class="vp-site-name hide-in-pad">落痕月极的博客</span></a><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="博客主页" iconsizing="both"><!--[--><i class="vp-icon fas fa-home fa-fw" sizing="both"></i><!--]-->博客主页<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><i class="vp-icon fas fa-blog fa-fw" sizing="both"></i><span class="vp-sidebar-title">博客</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">编程基础</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">常用工具</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">机器学习</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">讲座论坛</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">论文总结</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">能源化工与仪器科学</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">深度学习</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">大模型</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2024-12-31-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A01-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" aria-label="[深度学习] 大模型学习1-大语言模型基础知识" iconsizing="both"><!---->[深度学习] 大模型学习1-大语言模型基础知识<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-02-28-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A02-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8C%97.html" aria-label="[深度学习] 大模型学习2-提示词工程指北" iconsizing="both"><!---->[深度学习] 大模型学习2-提示词工程指北<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-21-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8A-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html" aria-label="[深度学习] 大模型学习3上-模型训练与微调" iconsizing="both"><!---->[深度学习] 大模型学习3上-模型训练与微调<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-23-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8B-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html" aria-label="[深度学习] 大模型学习3下-模型训练与微调" iconsizing="both"><!---->[深度学习] 大模型学习3下-模型训练与微调<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-08-08-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A04-RAG%E6%8A%80%E6%9C%AF%E5%85%A8%E6%99%AF%E8%A7%A3%E6%9E%90.html" aria-label="[深度学习] 大模型学习4-RAG技术全景解析" iconsizing="both"><!---->[深度学习] 大模型学习4-RAG技术全景解析<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-10-01-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A05-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%E6%A1%86%E6%9E%B6Unsloth%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97.html" aria-label="[深度学习] 大模型学习5-高效微调框架Unsloth使用指北" iconsizing="both"><!---->[深度学习] 大模型学习5-高效微调框架Unsloth使用指北<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">深度学习笔记</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">数据分析与可视化</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">数学理论</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">随笔所想</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">图像处理</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">音视频处理</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">自然语言处理与语音识别</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">OpenCV</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Python</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">R语言</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->[深度学习] 大模型学习3下-模型训练与微调</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="/" target="_blank" rel="noopener noreferrer">落痕月极</a></span><span property="author" content="落痕月极"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025年7月24日</span><meta property="datePublished" content="2025-07-23T20:01:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 34 分钟</span><meta property="timeRequired" content="PT34M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color4 clickable" role="navigation">深度学习</span><!--]--><meta property="articleSection" content="深度学习"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color4 clickable" role="navigation">深度学习</span><span class="page-tag-item color2 clickable" role="navigation">自然语言处理与语音识别</span><span class="page-tag-item color6 clickable" role="navigation">Python</span><!--]--><meta property="keywords" content="深度学习,自然语言处理与语音识别,Python"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><!--[--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_3-大语言模型构建">3 大语言模型构建</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-1-数据预处理">3.1 数据预处理</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-2-chat-template">3.2 Chat Template</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-3-大语言模型训练路径选择">3.3 大语言模型训练路径选择</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-4-训练方式介绍">3.4 训练方式介绍</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#_4-参考">4 参考</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--]--><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="深度学习-大模型学习3下-模型训练与微调" tabindex="-1"><a class="header-anchor" href="#深度学习-大模型学习3下-模型训练与微调"><span>[深度学习] 大模型学习3下-模型训练与微调</span></a></h1><p>在文章<a href="https://blog.csdn.net/LuohenYJ/article/details/144858528" target="_blank" rel="noopener noreferrer">大语言模型基础知识</a>里，模型训练与微调作为大语言模型（Large Language Model，LLM）应用构建的主要方式被简要提及，本系列文章将从技术原理、实施流程及应用场景等维度展开深度解析。相关知识的进一步参考见：<a href="https://modelscope.cn/learn/399?pid=342" target="_blank" rel="noopener noreferrer">LLM训练理论和实战</a>。本文作为系列的下半部分，包含第3章并聚焦于大语言模型构建的实操细节与技术要点。上半部分，即文章<a href="https://www.cnblogs.com/luohenyueji/p/18996523" target="_blank" rel="noopener noreferrer">大模型学习3上-模型训练与微调</a>已系统阐述了大语言模型的基础理论与核心结构。</p><h2 id="_3-大语言模型构建" tabindex="-1"><a class="header-anchor" href="#_3-大语言模型构建"><span>3 大语言模型构建</span></a></h2><p>本章涉及相关代码的介绍，且代码运行需要<strong>GPU</strong>支持。</p><h3 id="_3-1-数据预处理" tabindex="-1"><a class="header-anchor" href="#_3-1-数据预处理"><span>3.1 数据预处理</span></a></h3><p>在模型训练中，数据预处理至关重要。如今训练流程已较为成熟，数据集的质量往往成为训练成败的关键。模型训练的一个核心环节是将文本转换为索引，这一过程依赖于分词器 (Tokenizer)。不同模型的分词器虽有差异，但其核心处理逻辑基本一致。</p><p>分词器如同“文本剪刀”，将句子切分为有意义的token（如单字或词语），再将每个token映射为一个唯一的数字索引，以供模型处理。不同模型的分词器差异主要体现在分词粒度（如子词、字符、词级）、词汇表构建方式与规模，以及文本标准化规则（如大小写、标点处理）和特殊符号设计等方面。文本到索引的转换过程如下：</p><ol><li>分词 (Tokenization)：分词器首先将句子切分为token（例如，“我爱月亮”被切分为“我”、“爱”、“月亮”）</li><li>索引映射 (Index Mapping)：然后为每个token分配一个唯一的数字标识，即索引（例如，“我”对应索引 1，“爱”对应索引 2，“月亮”对应索引 3）</li><li>序列生成 (Sequence Generation)：最终，句子“我爱月亮”就被转换为token索引序列 [1, 2, 3]</li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img10.jpg" alt="https://jfrog.com/blog/utilizing-llms-with-embedding-stores/" tabindex="0" loading="lazy"><figcaption>https://jfrog.com/blog/utilizing-llms-with-embedding-stores/</figcaption></figure><p>以下代码展示了如何使用unsloth库调用DeepSeek-R1-Distill-Qwen-1.5B模型及其分词器，将输入文本快速转换为模型所需的token序列。unsloth是一个专注于优化大语言模型推理的库，而 DeepSeek-R1-Distill-Qwen-1.5B是轻量级大语言模型，也是DeepSeek-R1系列中最小的模型。若想具体了解DeepSeek-R1系列模型，可参考： <a href="https://blog.csdn.net/Tang_is_learning/article/details/146303837" target="_blank" rel="noopener noreferrer">一张图彻底拆解DeepSeek V3和R1双模型</a>。</p><p>unsloth的安装方法见其官方仓库<a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer">unsloth</a>和<a href="https://mp.weixin.qq.com/s/IxJgucJK9RM3PmMotzEtpQ" target="_blank" rel="noopener noreferrer">unsloth微调环境搭建</a>。unsloth预置了多种常见的大语言模型，且所有模型均托管于Hugging Face。示例代码如下，为加快模型加载速度，代码中提供了从modelScope或镜像网站加载的选项。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># torch2.5版本以下防止unsloth加载出问题</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> not</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> hasattr</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(modeling_utils, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;ALL_PARALLEL_STYLES&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">or</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> is</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;tp&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;colwise&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;rowwise&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 原始模型地址：https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 1. 利用modelscope库下载模型到本体，然后通过unsloth加载模型</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载预训练模型 ,利用modelscope库</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 假设这是支持分词的模型名称</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 下载模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从huggingface的镜像https://hf-mirror.com/中下载模型到本地</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># model_dir= &quot;./DeepSeek-R1-Distill-Qwen-1.5B&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 设置最大序列长度，表示模型在一次前向传递中可以处理的最大令牌数量。</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">max_seq_length </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># https://hf-mirror.com/</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 调用FastLanguageModel.from_pretrained()方法加载预训练的模型和对应的Tokenizer（分词器）。</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从hf中直接调用：model_name = unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 最大序列长度，表示模型在一次前向传递中可以处理的最大令牌数量。</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,           </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 自动检测（BF16或FP16）。BF16范围大，FP16精度高</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    local_files_only</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">   # 只用本地文件</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 分词</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">text </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;《Deep Learning》中文版&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将文本编码为模型输入格式</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;编码结果:&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, inputs)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># {&#39;input_ids&#39;: tensor([[151646,  26940,  33464,  20909,  25067, 104811,  40301]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1]])}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 将编码转换回文本</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">decoded </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(inputs[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;input_ids&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;解码结果:&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, decoded)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># &lt;｜begin▁of▁sentence｜&gt;《Deep Learning》中文版</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 分词器的词汇表大小</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(tokenizer))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上述代码中，<code>input_ids</code>是文本对应的token序列。需注意，token数量通常少于实际字数，因为token化并非一个汉字对应一个token，模型可能合并词语或拆分英文单词。如示例中&quot;中文&quot;对应索引104811。 <code>attention_mask</code>是一个与输入序列等长的二进制掩码（0/1组成），其中1表示对应位置的token需参与注意力计算。0表示该位置可被忽略（如填充token），以避免模型学习无效信息。</p><p>编码结果中的151646是分词器自动添加的起始标记<code>&lt;｜begin▁of▁sentence｜&gt;</code>的token ID，用于辅助模型理解文本结构。不同模型的起始标记可能不同，例如<code>&lt;s&gt;</code>或<code>[CLS]</code>。</p><h3 id="_3-2-chat-template" tabindex="-1"><a class="header-anchor" href="#_3-2-chat-template"><span>3.2 Chat Template</span></a></h3><h4 id="_3-2-1-什么是chat-template" tabindex="-1"><a class="header-anchor" href="#_3-2-1-什么是chat-template"><span>3.2.1 什么是Chat Template</span></a></h4><p>前面提到大语言模型发布时通常会推出基础版与对话版两个版本。其中，基础模型是经过大规模语料无监督预训练的模型，这类模型虽然学习了大量通用知识，但没有经过任何行为指导；而对话模型则是专门为用户交互构建的，通常采用提问与回答的格式，它是在基础模型的基础上，通过指令监督微调与基于人类反馈的强化学习进行优化得到的，能够与人进行对话，并且输出的结果更加符合预期、更易于控制，也更加安全。</p><p>想让大语言模型理解并生成好的对话，需要给它一个清晰的“剧本”，这就是 Chat Template（聊天模板）。LLM的Chat Template是一种预定义规则，其作用是将对话历史，包括多轮用户消息、助理回复、系统提示等，格式化为模型能够理解和处理的单一字符串。从本质上来说，它是对话结构的“编码指南”，目的是确保模型接收的输入符合其训练时所见到的格式。</p><p>那么，为什么需要Chat Template呢？原因主要有以下几点：</p><ul><li>结构化输入：LLM本身处理的是连续文本字符串，而对话是包含不同角色，如用户、助理、系统等的多轮交互，Chat Template定义了如何将这些角色、消息内容以及必要的特殊标记组合成连贯的字符串。</li><li>模型兼容性：不同的模型，像Llama 2、Mistral、ChatGPT、Claude等，在训练时使用的对话格式不同，例如用不同的特殊标记来表示角色、消息边界等，Chat Template能够确保输入符合特定模型期望的格式。</li><li>区分角色：清晰地标明文本是来自用户、助理还是系统指令，这对于模型理解上下文、遵循指令以及生成符合角色的回复来说至关重要。</li><li>添加必要标记：通常需要添加一些特殊标记，比如<code>&lt;|im_start|&gt;</code>、<code>&lt;|im_end|&gt;</code>等，这些标记用于标识消息的开始和结束、角色的切换，同时还包括角色标识符，如system、user、assistant，以及分隔符，如换行符<code>\n</code>，用于分隔不同的部分。</li><li>统一处理：它为开发者提供了一种标准化的方式来处理各种对话场景，包括单轮、多轮以及包含系统提示的场景，从而简化代码逻辑。</li><li>防止提示注入：正确的模板有助于分离用户输入与指令，进而降低模型被诱导执行意外操作的风险。</li></ul><p>Chat Template通常包含以下核心部分：</p><ol><li><p>角色 (Role)：标识对话参与者</p><ul><li>system (系统)： 类似于导演或旁白，用于设定对话的背景、模型扮演的角色以及需要遵守的规则。该角色通常只在对话开始时出现一次（可选但常用）。</li><li>user (用户)： 代表真实人类用户输入的话语或提出的问题。</li><li>assistant (助手)： 代表 AI 模型自身在对话历史中给出的回复（在连续对话中尤为重要）。</li></ul></li><li><p>消息 (Message)：对话的具体内容</p><ul><li>指每个角色对应的实际文本。例如，<code>user</code> 的消息是用户的问题文本，<code>assistant</code> 的消息是模型之前的回答文本。</li></ul></li><li><p>特殊标记 (Special Tokens)：对话的结构分隔符</p><ul><li>一些预定义、具有特定含义的词汇或符号。它们如同对话的“标点符号”，用于清晰标记对话的开始、结束、角色切换等结构边界。常见的例子包括 <code>&lt;|im_start|&gt;</code>, <code>&lt;|im_end|&gt;</code>, <code>[INST]</code>, <code>[/INST]</code> 等。</li></ul></li><li><p>格式化规则 (Formatting Rules)：组合各种元素的语法</p><ul><li>这是一套具体的语法规则，定义了如何将“角色”、“消息”和“特殊标记”按照正确的顺序和格式拼接组合，形成最终输入给模型处理的完整文本序列。它规定了整个“剧本”的书写规范。</li></ul></li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img11.jpg" alt="https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques" tabindex="0" loading="lazy"><figcaption>https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques</figcaption></figure><p>关于Chat Template的详细介绍，可参考<a href="https://www.dongaigc.com/p/chujiezheng/chat_templates" target="_blank" rel="noopener noreferrer">Chat_templates</a>和<a href="https://mp.weixin.qq.com/s/FYvP8SG4W7OtLmPiR_YbQw" target="_blank" rel="noopener noreferrer">Chat Template</a>。不同的LLM模型的Chat Template格式不一样，常见的如下几种：</p><ol><li>OpenAI ChatML，被 ChatGPT, GPT-4等使用，也是Hugging Face Transformers系列模型默认模板之一，DeepSeek系列和阿里的Qwen系列的Chat Template也采用类似结构：</li></ol><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_start</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">system</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{system_message}</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_end</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_start</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">user</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{user_message_1}</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_end</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_start</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">assistant</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{assistant_message_1}</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_end</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_start</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">user</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{user_message_2}</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_end</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;|</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">im_start</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">assistant  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 模型会从这里开始预测</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>每条消息（包括系统提示、用户输入和助手回复）都以<code>&lt;|im_start|&gt;{role}</code>开头，并以<code>&lt;|im_end|&gt;</code>结尾。</p></li><li><p>模型在预测/生成回复时，会从对话历史中最后一个<code>&lt;|im_start|&gt;assistant</code>标记之后的位置开始输出内容。模型在生成过程中会自动补全其回复内容，并最终输出 <code>&lt;|im_end|&gt;</code>标记来表示回复结束。</p></li></ul><ol start="2"><li>Llama Chat Template：</li></ol><p>Llama Chat Template是Meta的Llama系列模型使用的对话格式，由 <code>&lt;s&gt;[INST]</code> 标记开始，包含系统消息（用 <code>&lt;&lt;SYS&gt;&gt;</code> 和 <code>&lt;&lt;/SYS&gt;&gt;</code> 包裹）和用户消息，以 <code>[/INST]</code> 结束后接助手回复，多轮对话时通过 <code>&lt;/s&gt;&lt;s&gt;[INST]</code> 分隔。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">s</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">INST</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">SYS</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;&gt;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{system_message}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;&lt;/</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">SYS</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;&gt;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{user_message_1} [</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">INST</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] {assistant_message_1} </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;/</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">s</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">s</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">INST</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] {user_message_2} [</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">/</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">INST</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_3-2-2-chat-template实现" tabindex="-1"><a class="header-anchor" href="#_3-2-2-chat-template实现"><span>3.2.2 Chat Template实现</span></a></h4><p>那么在大语言模型中，chat template是如何实现的呢？事实上，大语言模型会提供一个使用Jinja2模板语法定义的字符串，专门用于格式化对话历史生成chat template字符串。Jinja2是一种模板引擎，它提供变量、控制结构（如循环和条件判断）以及过滤器等功能，用于生成动态文本。更多细节可参考<a href="https://huggingface.co/docs/transformers/chat_templating" target="_blank" rel="noopener noreferrer">chat_templating</a>。</p><p>Chat template是Jinja2的具体应用：通过其循环语法遍历消息列表，并按特定格式构建对话历史。常见的chat template字符串通常预置如下Jinja2模板：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>chat_template = &quot;&quot;&quot;</span></span>
<span class="line"><span>{% for message in messages %}</span></span>
<span class="line"><span>{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;\n&#39;}}</span></span>
<span class="line"><span>{% endfor %}</span></span>
<span class="line"><span>&quot;&quot;&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个模板包含两种主要的Jinja2语法元素：</p><ol><li><p>控制结构（由 <code>{% ... %}</code> 包围）：<br> 这是一个<code>for</code>循环，用于遍历 <code>messages</code> 列表中的每个元素。循环内的 <code>message</code> 变量代表当前消息项：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>{% for message in messages %}</span></span>
<span class="line"><span>...</span></span>
<span class="line"><span>{% endfor %}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>表达式（由 <code>{{ ... }}</code> 包围）：<br> 以下表达式生成单个消息的格式化文本。<code>message[&#39;role&#39;]</code> 和 <code>message[&#39;content&#39;]</code> 是动态变量，其值会根据当前消息替换：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;\n&#39;}}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li></ol><p>渲染该模板时，会执行以下步骤：</p><ol><li>解析模板：Jinja2解析器识别控制结构和表达式。</li><li>变量注入：需向模板提供包含 <code>messages</code> 变量的上下文。例如：<div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">context </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;messages&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你好&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;您好！有什么可以帮您？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    ]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li>执行控制结构：Jinja2 遍历 <code>messages</code> 列表中的每个元素。</li><li>计算表达式：对每条消息，生成格式化的文本。</li><li>合并结果：将所有消息文本合并为最终字符串。</li></ol><p>对于上述 <code>context</code>，渲染结果如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&lt;|im_start|&gt;user</span></span>
<span class="line"><span>你好&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;assistant</span></span>
<span class="line"><span>您好！有什么可以帮您？&lt;|im_end|&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>对话模板</strong></p><p>unsloth库提供了便捷的模板管理和应用功能。以下代码展示了如何查看和使用模型自带的ChatML格式对话模板，以及如何通过<code>apply_chat_template</code>方法将对话历史转换为模型可接受的输入格式。unsloth库中该模块的更多介绍见：<a href="https://docs.unsloth.ai/basics/chat-templates" target="_blank" rel="noopener noreferrer">Chat Templates</a>。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># torch2.5版本以下防止unsloth加载出问题</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> not</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> hasattr</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(modeling_utils, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;ALL_PARALLEL_STYLES&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">or</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> is</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;tp&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;colwise&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;rowwise&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 原始模型地址：https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 用modelscope库下载模型到本体，然后通过unsloth加载模型</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载预训练模型 ,利用modelscope库</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 假设这是支持分词的模型名称</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 下载模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从hf中直接调用：model_name = unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 最大序列长度，表示模型在一次前向传递中可以处理的最大令牌数量。</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,           </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 自动检测（BF16或FP16）。BF16范围大，FP16精度高</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    local_files_only</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">   # 只用本地文件</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 注意模板格式是ChatML模板源代码， 为什Jinja2模板格式</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(tokenizer.chat_template)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 显示模型的对话模板（可能很长），便于了解模型期望的输入格式</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用apply_chat_template格式化对话</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># messages是一个用于存储对话历史的列表，其中每个元素代表一条对话消息</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;You are a pirate chatbot who talks in shanties!&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;What&#39;s the best way to hide treasure?&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 应用tokenizer.chat_template模板解析messages数据</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">formatted_input </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    messages,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 设为True则直接返回token IDs</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 在末尾添加提示模型开始生成的标记（如&lt;|im_start|&gt;assistant）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(formatted_input)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&#39;&#39;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&lt;｜begin▁of▁sentence｜&gt;You are a pirate chatbot who talks in shanties!&lt;｜User｜&gt;What&#39;s the best way to hide treasure?&lt;｜Assistant｜&gt;&lt;think&gt;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&#39;&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 如果模型没有默认模板，或者你想覆盖它，可以手动设置</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">chat_template </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;&quot;&quot;{% for message in messages %}{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;\n&#39;}}{% endfor %}&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">tokenizer.chat_template </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> chat_template</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 应用自定义模板解析messages数据</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;You are a pirate chatbot who talks in shanties!&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;What&#39;s the best way to hide treasure?&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">formatted_input </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    messages,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(formatted_input)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&#39;&#39;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&lt;|im_start|&gt;system</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">You are a pirate chatbot who talks in shanties!&lt;|im_end|&gt;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&lt;|im_start|&gt;user</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">What&#39;s the best way to hide treasure?&lt;|im_end|&gt;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&#39;&#39;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>对话历史结构化</strong></p><p>messages是对话历史的结构化表示，它的作用包括:</p><ol><li>符合行业标准：OpenAI的Chat Completions API 和Hugging Face的对话模型都使用这种格式</li><li>清晰区分角色：明确区分系统指令、用户输入和模型回复</li><li>支持多轮对话：可以包含任意长度的对话历史</li></ol><p>为什么需要这种格式？</p><p>上下文维护：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;法国的首都是哪里？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;巴黎&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;它的人口是多少？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># &quot;它&quot;指代巴黎</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>角色设定：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你是一位18世纪的海盗船长&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>多轮对话</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;推荐一部科幻电影&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 用户提问</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;《星际穿越》很不错&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 模型回复</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;为什么推荐这部？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 后续用户提问</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>以下代码展示了如何利用unsloth库基于对话模板生成回答：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># torch2.5版本以下防止unsloth加载出问题</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> not</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> hasattr</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(modeling_utils, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;ALL_PARALLEL_STYLES&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">or</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> is</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;tp&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;colwise&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;rowwise&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 原始模型地址：https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 用modelscope库下载模型到本体，然后通过unsloth加载模型</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载预训练模型 ,利用modelscope库</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 假设这是支持分词的模型名称</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 下载模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir, </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从hf中直接调用：model_name = unsloth/DeepSeek-R1-Distill-Qwen-1.5B</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 最大序列长度，表示模型在一次前向传递中可以处理的最大令牌数量。</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,           </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 自动检测（BF16或FP16）。BF16范围大，FP16精度高</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    local_files_only</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">   # 只用本地文件</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 无模板生成</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> generate_without_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">padding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">truncation</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**inputs, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                             repetition_penalty</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1.1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 减少重复0</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    model_response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(outputs[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_special_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 返回回答，包括输入</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_response</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用模板生成</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">messages</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 将message转换为模型期望的chat template格式</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # add_generation_prompt用于在末尾添加回复引导符（如 &lt;|assistant|&gt;）以指示模型开始生成回复</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 注意对于DeepSeek R1系列模型，add_generation_prompt还会生成&lt;think&gt;符号，以表示进行深度思考</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages, </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                                           tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                                           add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                                           )</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 将message所有内容进行编码</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">padding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">truncation</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**inputs, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">                             repetition_penalty</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1.1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 减少重复</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    model_response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(outputs[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_special_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_response</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 测试用例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">test_prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;你好，请描述一下你最喜欢的季节？&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 无模板生成</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;=== 无message模板的输出 ===&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">raw_output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> generate_without_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(test_prompt)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(raw_output)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 生成风格较为随机</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 有模板生成</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">=== 使用message模板的输出 ===&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你是一个吃货，将从吃货的角度来回答问题&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">},</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: test_prompt}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">templated_output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(templated_output) </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 从美食来回答问题</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>以下代码演示了如何利用unsloth库实现多轮对话功能。核心机制是维护一个持续更新的对话列表（<code>messages</code>），完整记录：</p><ul><li>初始系统设定（如角色定位）</li><li>用户每次输入</li><li>AI每次回复</li></ul><p>当用户提出新问题时，将整个对话历史（含所有上下文）输入模型。模型基于完整上下文生成连贯回复后，将新回复追加到列表中，形成持续增长的记忆链。关键步骤：</p><ol><li>初始化含系统设定的<code>messages</code></li><li>追加用户问题</li><li>输入完整<code>messages</code>调用模型</li><li>追加AI回复到列表</li><li>循环2-4步实现持续对话</li></ol><p>随着轮次增加，<code>messages</code>不断扩展，模型通过完整上下文保持对话一致性。实现代码如下：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> unsloth </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> transformers </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 防止旧版torch兼容性问题</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> not</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> hasattr</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(modeling_utils, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;ALL_PARALLEL_STYLES&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">or</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> is</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    modeling_utils.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">ALL_PARALLEL_STYLES</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;tp&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;colwise&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;rowwise&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> modelscope </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> snapshot_download</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 加载预训练模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_name </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model_dir </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> snapshot_download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model_name)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">model, tokenizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FastLanguageModel.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">from_pretrained</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    model_name</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model_dir,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    max_seq_length</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2048</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">    local_files_only</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">messages</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    使用对话模板生成回复（支持多轮对话）</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    原理：通过维护messages列表保存完整的对话历史</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    每个消息包含角色(role)和内容(content):</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">      - system: 系统设定（仅出现在开头）</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">      - user: 用户输入</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">      - assistant: AI回复</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    每次生成时，模型会根据完整的对话上下文生成连贯的回复</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 应用对话模板，添加生成引导符</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    prompt </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">apply_chat_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        messages, </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        add_generation_prompt</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    )</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    inputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> tokenizer</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(prompt, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">return_tensors</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;pt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">padding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">truncation</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;cuda&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">generate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        **inputs, </span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        max_new_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">        repetition_penalty</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1.1</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    )</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 只提取新生成的回复（去掉输入部分）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    input_length </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> inputs.input_ids.shape[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    generated_ids </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> outputs[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][input_length:]</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(generated_ids, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">skip_special_tokens</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># ===== 多轮对话演示 =====</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 原理说明：通过维护不断增长的messages列表实现上下文记忆</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 初始化对话系统设定</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    {</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;system&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你是一个资深吃货，回答问题时总是从美食角度展开&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 第一轮对话</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">({</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;你好！你最喜欢的季节是什么？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">})</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;用户：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages[</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;content&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;AI：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">({</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: response})  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 关键：将AI回复加入历史</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 第二轮对话（基于之前的所有上下文）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">({</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;为什么喜欢这个季节？有什么特别的美食吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">})</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">用户：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages[</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;content&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;AI：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">({</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;assistant&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: response})  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 再次更新对话历史</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 第三轮对话（继续基于完整上下文）</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">append</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">({</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;role&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;user&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;content&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;能详细说说这道菜怎么做吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">})</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> generate_with_template</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">用户：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">messages[</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;content&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;AI：</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">response</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># ===== 多轮对话原理可视化 =====</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\n\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">=== 多轮对话的上下文结构 ===&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;整个对话过程中维护的messages列表结构:&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> i, msg </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> enumerate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(messages):</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">i</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">] </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">msg[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;role&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">msg[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;content&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">][:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">50</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}{</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;...&#39;</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> if</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(msg[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;content&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">50</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> else</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;&#39;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img12.gif" alt="https://cobusgreyling.substack.com/p/openai-o1-reasoning-models" tabindex="0" loading="lazy"><figcaption>https://cobusgreyling.substack.com/p/openai-o1-reasoning-models</figcaption></figure><h3 id="_3-3-大语言模型训练路径选择" tabindex="-1"><a class="header-anchor" href="#_3-3-大语言模型训练路径选择"><span>3.3 大语言模型训练路径选择</span></a></h3><p>本文梳理了开发自己的大语言模型应用的六大常见典型场景及其训练路径选择指南：</p><ol><li>场景一：算力资源充沛，志在冲击排行榜的全新模型训练</li></ol><ul><li>适用情形：拥有丰富的显卡设备和大量数据，期望训练一个全新模型以实现最优性能表现。</li><li>实施策略：采用标准的预训练流程，配合大规模有监督微调（SFT），再进行对齐训练。该方案资源消耗极大，一般用户很少采用。</li><li>优点： <ul><li>有机会训练出性能卓越的全新模型，在排行榜上取得优异成绩，提升技术影响力。</li><li>完整的训练流程能充分挖掘数据潜力，使模型具备强大的泛化能力。</li></ul></li><li>缺点： <ul><li>资源消耗极大，成本高昂，包括显卡设备购置、电力消耗等。</li><li>训练周期长，需要大量时间进行预训练、微调和对齐训练。</li></ul></li></ul><ol start="2"><li>场景二：手握海量未标注领域数据</li></ol><ul><li>适用情形：积累了大量未标注数据，且其中知识未被预训练模型涵盖，需将其融入模型以契合实际场景。</li><li>实施策略：基于现有预训练模型开展持续预训练。该过程与常规预训练相似，但资源与时间成本较低。</li><li>优点： <ul><li>能有效利用未标注数据，丰富模型的知识储备，使其更好地适应特定领域场景。</li><li>相比常规预训练，资源与时间成本较低，性价比高。</li></ul></li><li>缺点： <ul><li>持续预训练的效果可能不如全新训练，对模型性能提升存在一定局限。</li><li>未标注数据的质量参差不齐，可能影响模型训练效果。</li></ul></li></ul><ol start="3"><li>场景三：凭借标注数据提升特定问答能力</li></ol><ul><li>适用情形：具备标注数据，需让模型掌握特定问答技巧（如依据行业数据提炼大纲）。</li><li>实施策略：对模型进行有监督微调（SFT），需确保标注数据的质量和代表性。</li><li>优点： <ul><li>能针对性地提升模型在特定问答任务上的能力，满足具体业务需求。</li><li>训练过程相对简单，易于实施和调整。</li></ul></li><li>缺点： <ul><li>标注数据的获取成本较高，需要投入大量人力进行标注工作。</li><li>如果标注数据质量不佳或缺乏代表性，可能导致模型过拟合或泛化能力差。</li></ul></li></ul><ol start="4"><li>场景四：回答内容需严格契合既定知识（如金融知识）</li></ol><ul><li>适用情形：要求回答严格依据特定知识源（如法律法规、产品说明书）。</li><li>实施策略： <ul><li>策略A：用自有数据微调模型（提升理解领域知识或利用检索结果的能力），再结合检索增强生成（RAG）进行知识检索。</li><li>策略B：直接利用预训练模型搭配RAG完成检索，效果高度依赖检索系统的准确性和覆盖度。</li></ul></li><li>优点： <ul><li>策略A：通过微调模型能提升其对领域知识的理解和利用能力，结合RAG可确保回答的准确性。</li><li>策略B：实施简单，无需对模型进行复杂训练，能快速利用预训练模型和RAG系统。</li></ul></li><li>缺点： <ul><li>策略A：微调模型需要一定的算力和时间成本，且对自有数据的质量要求较高。</li><li>策略B：效果高度依赖检索系统的准确性和覆盖度，如果检索系统不完善，可能导致回答错误或不完整。</li></ul></li></ul><ol start="5"><li>场景五：定制符合特定规范的领域问答机器人</li></ol><ul><li>适用情形：需开发领域问答机器人，且回答需符合特定格式或风格要求。</li><li>实施策略：先通过SFT学习领域知识，再开展对齐训练规范回答风格，确保输出规范性。</li><li>优点： <ul><li>能开发出符合特定领域需求和规范的问答机器人，提供高质量的服务。</li><li>对齐训练可以有效规范回答风格，提升用户体验。</li></ul></li><li>缺点： <ul><li>训练过程复杂，需要多个阶段的训练和调整，对技术要求较高。</li><li>对齐训练的效果可能受到奖励模型或偏好模型的影响，需要不断优化和改进。</li></ul></li></ul><ol start="6"><li>场景六：资源有限下的高效模型适配</li></ol><ul><li>适用情形：算力紧张（如单卡/少量卡），仍需利用自有数据提升模型在特定任务或领域的表现，追求性价比。</li><li>实施策略： <ul><li>采用参数高效微调（PEFT）技术（如LoRA、Adapter）： <ul><li>有标注数据：直接在目标任务上使用PEFT进行SFT。</li><li>大量未标注数据：结合PEFT进行轻量级持续预训练。</li></ul></li><li>可搭配RAG补充知识，减少模型记忆负担。</li></ul></li><li>优点： <ul><li>在算力有限的情况下，能有效利用自有数据提升模型性能，性价比高。</li><li>PEFT技术减少了需要训练的参数数量，降低了训练难度和成本。</li><li>搭配RAG可以补充知识，进一步提升模型的回答质量。</li></ul></li><li>缺点： <ul><li>PEFT技术的效果可能不如全参数微调，对模型性能提升存在一定限制。</li><li>轻量级持续预训练可能无法充分挖掘未标注数据的潜力，影响模型的知识储备。</li></ul></li></ul><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img13.jpg" alt="https://towardsdatascience.com/6-common-llm-customization-strategies-briefly-explained/" tabindex="0" loading="lazy"><figcaption>https://towardsdatascience.com/6-common-llm-customization-strategies-briefly-explained/</figcaption></figure><h3 id="_3-4-训练方式介绍" tabindex="-1"><a class="header-anchor" href="#_3-4-训练方式介绍"><span>3.4 训练方式介绍</span></a></h3><p>如前所述，大语言模型的训练通常包含三个核心阶段：预训练（构建基础语言能力）、指令微调（赋予任务适配能力）和人类对齐（优化输出的安全性与价值观一致性）。对于资源有限的小型开发者而言，前两个阶段（预训练与指令微调）的技术门槛相对较低，可通过轻量化方法实现高效训练；而人类对齐阶段因涉及复杂的强化学习框架（如 RLHF）与大规模人工标注数据，对算力和工程化能力要求较高，因此小型团队通常难以深度参与。关于构建自己的大语言模型的更多内容，可参考：<a href="https://blog.csdn.net/python1222_/article/details/140790246" target="_blank" rel="noopener noreferrer">大语言模型从零开始训练全面指南</a>。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img14.jpg" alt="https://huyenchip.com/2023/05/02/rlhf.html/" tabindex="0" loading="lazy"><figcaption>https://huyenchip.com/2023/05/02/rlhf.html/</figcaption></figure><p>计划开发自有模型并在预训练阶段继续训练，通常以基础模型（Base Model）为起点，该过程依赖大规模无监督数据。基础模型的进一步预训练有两种主流范式：</p><ol><li>全参数微调（Full Fine-tuning）：更新模型全部参数，计算资源消耗大且需充足无监督数据，适用于算力充沛的机构。</li><li>参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）：冻结基础模型参数，仅训练新增模块（如LoRA、Adapter），显著降低计算和存储成本。</li></ol><p>注：预训练/继续预训练不推荐轻量化PEFT，因为会导致模型泛化能力受限。但在数据有限或资源受限时，其高效性可作为实用方案。</p><p>若需进行指令微调，则需使用标注的指令数据（输入-输出对）。其核心目的是引导模型应用预训练获得的知识与能力，使其能遵循指令并适应多样化任务。指令微调通常无法赋予模型全新知识领域，但对优化其在特定领域的表现往往是必要的，它能将模型内化知识以符合任务要求的形式展现。</p><p>此阶段，基础模型或已具备初步指令能力的对话模型均可作为起点。选择需结合数据规模、任务需求和资源限制：</p><ul><li>数据有限时：优先选择对话模型作为起点更具优势。</li><li>数据充足时：基础模型通常提供更大的定制潜力。</li></ul><p>若资源允许，建议对两种起点均进行实验，依据效果择优。此阶段可灵活选择全量微调或参数高效微调进行训练。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img15.jpg" alt="https://www.tensorops.ai/post/prompt-eng-vs-rag-vs-fine-tuning-what-do-you-need" tabindex="0" loading="lazy"><figcaption>https://www.tensorops.ai/post/prompt-eng-vs-rag-vs-fine-tuning-what-do-you-need</figcaption></figure><h4 id="_3-4-1-预训练训练过程介绍" tabindex="-1"><a class="header-anchor" href="#_3-4-1-预训练训练过程介绍"><span>3.4.1 预训练训练过程介绍</span></a></h4><p>预训练阶段的核心逻辑：</p><ul><li>目标：让模型掌握通用语言能力（语法、事实知识、基础推理），通过海量无标注文本学习语言的统计规律。</li><li>数据：使用纯文本（如网页、书籍、百科），无需人工标注的 input-label 配对。</li><li>训练任务：掩码语言建模（Masked Language Modeling, MLM） 或 自回归语言建模（Autoregressive LM）。 以下以 自回归式预训练（GPT系列采用的方式）为例说明：</li></ul><ol><li><p>原始数据</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>商品签收后7天内，保持完好可无理由退货。请登录您的账户提交退货申请。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li><li><p>分词与Token化</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 分词后的Token序列（示例值）</span></span>
<span class="line"><span>tokens:      [&quot;商品&quot;, &quot;签收&quot;, &quot;后&quot;, &quot;7&quot;, &quot;天内&quot;, &quot;，&quot;, &quot;保持&quot;, &quot;完好&quot;, ...]</span></span>
<span class="line"><span>input_ids:   [1024, 5078, 208, 25, 3819, 6, 3340, 2865, ...]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>标签（labels）的生成</p><ul><li>核心规则：预测下一个Token<br> 预训练的 <code>labels</code>就是输入序列向后平移一位 <ul><li>输入（<code>input_ids</code>）：<code>[1024, 5078, 208, 25, 3819, ...]</code></li><li>标签（<code>labels</code>）： <code>[5078, 208, 25, 3819, ..., -100]</code></li><li>最后一个Token无下一个Token，用 <code>-100</code> 忽略**</li></ul></li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 自回归预训练的输入与标签关系</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">input_ids: [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1024</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5078</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">208</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">25</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3819</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 输入序列</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">labels:    [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5078</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">208</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">25</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3819</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 目标：预测下一个Token</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>训练过程（自回归预测）</p><ul><li>模型根据前k个Token，预测第k+1个Token（即 <code>labels</code> 中的第 <code>k</code> 个值）：<div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>输入: [1024]          → 预测: 5078 (标签中的第1个值)</span></span>
<span class="line"><span>输入: [1024, 5078]   → 预测: 208  (标签中的第2个值)</span></span>
<span class="line"><span>输入: [1024,5078,208]→ 预测: 25   (标签中的第3个值)</span></span>
<span class="line"><span>...</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li>损失计算：对所有非 <code>-100</code> 位置的预测计算交叉熵损失（即整个序列除了最后一个位置）。</li></ul></li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img15.gif" alt="https://masteringllm.medium.com/3-interview-questions-on-large-language-models-llms-e5fee3916f46" tabindex="0" loading="lazy"><figcaption>https://masteringllm.medium.com/3-interview-questions-on-large-language-models-llms-e5fee3916f46</figcaption></figure><p>如果想在预训练模型的基础上进行增量预训练，可参考这篇关于增量预训练的文章：<a href="https://zhuanlan.zhihu.com/p/636334904" target="_blank" rel="noopener noreferrer">增量预训练</a>。</p><h4 id="_3-4-2-指令微调训练过程介绍" tabindex="-1"><a class="header-anchor" href="#_3-4-2-指令微调训练过程介绍"><span>3.4.2 指令微调训练过程介绍</span></a></h4><p>在大语言模型（LLM）的指令微调任务中，我们会使用标注好的数据集。这些数据不仅包含用户的输入（问题），还包含我们期望模型给出的标准答案（labels）。为了训练模型，我们需要将用户的输入和期望的答案按照一个特定的模板（template）组合成一个完整的文本序列。这个序列随后会被转换成模型能够处理的数字形式，也就是TokenID序列。关于模型微调的更多内容，可参考：<a href="https://blog.csdn.net/leiwuhen92/article/details/148758135" target="_blank" rel="noopener noreferrer">大模型微调Fine-tuning</a>。</p><p>假设我们正在微调一个客服机器人模型。一条标注数据可能是：</p><ul><li>用户输入（Input）：&quot;如何办理退货？&quot;</li><li>期望答案（Label）：&quot;您好，商品签收后7天内，保持完好可无理由退货。请登录您的账户提交退货申请。&quot;</li></ul><p>根据设定的模板（如 &quot;用户问：<code>{input}\n</code>客服答：<code>{label}</code>&quot;），组合后的完整序列是：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>用户问：如何办理退货？\n客服答：您好，商品签收后7天内，保持完好可无理由退货。请登录您的账户提交退货申请。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这个文本序列会被分词器（Tokenizer）转换成对应的Token ID序列（以下是省略版）：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 组合序列的Token ID表示 (示例值)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">input_ids: [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">18</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">42</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">77</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">93</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">25</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,     </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">88</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">52</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">39</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">71</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">           |</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">用户问题部分</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-|</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> |</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">期望答案部分</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>同时，我们会准备一个对应的 <code>labels</code> 序列，用于指导模型的学习目标：</p><ul><li>用户问题部分对应的Token会被替换成一个特殊的忽略标记 <code>-100</code>。</li><li>期望答案部分对应的Token则保持不变。</li></ul><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 对应的 labels 序列</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">labels:    [</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">100</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">64</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">88</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">52</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">39</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">71</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">           |</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">--------</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">被忽略部分</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">--------</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-|</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> |</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">学习目标部分</span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">----</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">|</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>那么模型是如何学习的？</p><p>模型的核心训练任务是自回归预测（Autoregressive Prediction）：它根据当前看到的所有前面的Token，预测序列中下一个最可能出现的Token是什么。这个过程就像一步步填空：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>已知Token序列开头        -&gt; 模型预测的下一个Token (真实的下一个Token)</span></span>
<span class="line"><span>[18]                   -&gt; 预测：50   (错误，真实是：42)</span></span>
<span class="line"><span>[18, 42]               -&gt; 预测：29   (错误，真实是：77)</span></span>
<span class="line"><span>[18, 42, 77]           -&gt; 预测：93   (正确！)</span></span>
<span class="line"><span>[18, 42, 77, 93]       -&gt; 预测：10   (错误，真实是：25)</span></span>
<span class="line"><span>[18, 42, 77, 93, 25]   -&gt; 预测：70   (错误，真实是：64)  # 从这里开始预测答案部分</span></span>
<span class="line"><span>... (后续预测以此类推)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img16.jpg" alt="https://www.jerrylsu.net/articles/Self-Instruct.html" tabindex="0" loading="lazy"><figcaption>https://www.jerrylsu.net/articles/Self-Instruct.html</figcaption></figure><p>训练过程中的关键点：</p><ol><li>屏蔽输入，聚焦答案： 模型在预测用户问题部分（对应labels中<code>-100</code>的位置）时产生的任何错误预测，都会被<code>-100</code>标记屏蔽掉。这些错误不会被计入模型需要改进的部分，因为模型的任务不是复述问题。</li><li>关注答案生成： 只有当模型尝试生成答案部分（即对应labels中非<code>-100</code>的位置）时，它的预测才会被评估。</li><li>计算损失（Loss）： 系统会比较模型在答案部分实际预测出的整个Token序列（例如 <code>[70, 81, 47, 36, 65]</code>）与真实的期望答案序列（<code>[64, 88, 52, 39, 71]</code>），通过交叉熵计算两者之间的差异。这个差异量化的指标称为损失值（Loss）。损失值越大，意味着预测与期望答案偏差越大。</li><li>反向传播与优化： 基于计算出的Loss，训练算法（通常是反向传播和梯度下降）会调整模型内部的参数（如神经网络权重）。调整的目标是让模型下次在相同或类似上下文下，预测出的答案部分Token序列尽可能接近真实序列，从而降低Loss。</li></ol><p>回到客服机器人例子。训练时：</p><ul><li>模型看到<code>用户问：如何办理退货？\n客服答：</code>对应的Token (<code>[18, 42, 77, 93, 25]</code>)。</li><li>当模型开始预测客服回答的第一个Token（即期望答案 &quot;您好&quot; 对应的Token <code>64</code>）时，它的预测（比如错误地预测了<code>70</code>）会被计算到Loss中。</li><li>接着，模型看到 &quot;用户问...客服答：您好&quot;，预测下一个Token（期望是 &quot;商品&quot; 对应的Token <code>88</code>），如果预测错误（如<code>81</code>），这个错误也会计入Loss。</li><li>如此继续，直到预测完整个期望答案序列。只有模型在生成<code>您好，商品签收后7天内...</code> 这部分答案时犯的错误才会影响Loss的计算和模型参数的更新。模型在问题部分<code>用户问：如何办理退货？\n</code>的任何预测错误都会被忽略（<code>-100</code>的作用）。</li><li>通过大量这样的样本训练，模型逐渐学会在看到 &quot;如何办理退货？&quot; 这类问题时，生成 &quot;您好，商品签收后7天内...&quot; 这样的标准回答。</li></ul><h4 id="_3-4-3-lora介绍" tabindex="-1"><a class="header-anchor" href="#_3-4-3-lora介绍"><span>3.4.3 LoRA介绍</span></a></h4><p>LoRA（Low-Rank Adaptation，低秩适应）是大语言模型训练中常见的方式。关于LoRA的变体介绍见：<a href="https://zhuanlan.zhihu.com/p/687168177" target="_blank" rel="noopener noreferrer">LoRA及其变体概述</a>。</p><p>直接修改庞大语言模型（如GPT、Stable Diffusion）的核心参数非常困难。LoRA提供了一种巧妙方法：不直接改动原模型，而是添加一个微小的“技能升级包”。</p><p>LoRA工作原理：</p><ol><li>添加小模块： 针对模型中一个关键的大型参数矩阵（M x N），LoRA 添加两个极小的辅助矩阵（M x d 和 d x N，其中 <code>d</code> 远小于M或N）。</li><li>冻结原参数： 训练新任务时，原始大型矩阵的参数完全锁定（冻结），保持不变。</li><li>只训练小模块： 仅训练新添加的两个小矩阵。</li><li>组合与叠加： 训练完成后，将两个小矩阵相乘（得到 M x N 矩阵），再将这个结果叠加到原冻结的大矩阵上。这相当于应用了一个精密的“微调补丁”。</li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/Python-Study-Notes/大模型/大模型学习3-模型训练与微调/img/img17.jpg" alt="https://huggingface.co/blog/mlabonne/sft-llama3" tabindex="0" loading="lazy"><figcaption>https://huggingface.co/blog/mlabonne/sft-llama3</figcaption></figure><p>那么为什么要使用LoRA：</p><ul><li>信息压缩的： 研究发现，大语言模型适应新任务所需的“关键调整”远少于其总参数量（矩阵的“有效秩”很低）。LoRA 通过设定一个小的 <code>d</code>（秩），统一捕捉这些最关键的方向。虽然略有简化（可能损失微量精度），但换来了巨大的效率提升，对于微调通常是极佳权衡。</li><li>效果好的原因：大语言模型“学有余力” - 预训练好的大语言模型已具备海量通用知识。微调新任务（如特定写作风格）通常只需小幅调整其行为方向。 <ul><li>LoRA 的两个小矩阵就像一张精准的“提示卡”，引导模型侧重或微调其已有知识。模型本身足够强大，配合这个小指引就能高效完成新任务。</li><li>“低秩”设定 (<code>d</code>) 恰好匹配了模型微调所需的关键调整方向数量，实现了资源消耗与任务效果的高效平衡。</li></ul></li></ul><p>LoRA 的优势与应用：</p><ul><li>省资源：只需训练两个微小的矩阵（<code>d</code> 很小），所需的计算力（算力）和显存大幅降低。这使得在普通硬件（如消费级显卡）上微调超大模型成为可能。</li><li>轻量便捷： 生成的“升级包”（两个小矩阵）文件极小（几MB到几十MB），易于分享、加载和使用。</li><li>灵活部署： <ul><li>可动态加载：将“升级包”与原模型配合使用（稍有额外计算）。</li><li>或完全融合：将“升级包”永久合并到原模型中，使用效率等同原模型。</li></ul></li><li>持续进化： 基于LoRA，出现了更智能的变体（如AdaLoRA, SoRA），可自动调整不同矩阵的 <code>d</code> 值，寻求更好的效率精度平衡。</li></ul><h2 id="_4-参考" tabindex="-1"><a class="header-anchor" href="#_4-参考"><span>4 参考</span></a></h2><ul><li><a href="https://blog.csdn.net/LuohenYJ/article/details/144858528" target="_blank" rel="noopener noreferrer">大语言模型基础知识</a></li><li><a href="https://www.cnblogs.com/luohenyueji/p/18996523" target="_blank" rel="noopener noreferrer">大模型学习3上-模型训练与微调</a></li><li><a href="https://blog.csdn.net/Tang_is_learning/article/details/146303837" target="_blank" rel="noopener noreferrer">一张图彻底拆解DeepSeek V3和R1双模型</a></li><li><a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer">unsloth</a></li><li><a href="https://huggingface.co/docs/transformers/chat_templating" target="_blank" rel="noopener noreferrer">chat_templating</a></li><li><a href="https://mp.weixin.qq.com/s/FYvP8SG4W7OtLmPiR_YbQw" target="_blank" rel="noopener noreferrer">Chat Template</a></li><li><a href="https://www.dongaigc.com/p/chujiezheng/chat_templates" target="_blank" rel="noopener noreferrer">Chat_templates</a></li><li><a href="https://docs.unsloth.ai/basics/chat-templates" target="_blank" rel="noopener noreferrer">Chat Templates</a></li><li><a href="https://blog.csdn.net/python1222_/article/details/140790246" target="_blank" rel="noopener noreferrer">大语言模型从零开始训练全面指南</a></li><li><a href="https://zhuanlan.zhihu.com/p/636334904" target="_blank" rel="noopener noreferrer">增量预训练</a></li><li><a href="https://blog.csdn.net/leiwuhen92/article/details/148758135" target="_blank" rel="noopener noreferrer">大模型微调Fine-tuning</a></li><li><a href="https://zhuanlan.zhihu.com/p/687168177" target="_blank" rel="noopener noreferrer">LoRA及其变体概述</a></li></ul></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-07-21-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A03%E4%B8%8A-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83.html" aria-label="[深度学习] 大模型学习3上-模型训练与微调" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->[深度学习] 大模型学习3上-模型训练与微调</div></a><a class="route-link auto-link next" href="/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/2025-08-08-_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A04-RAG%E6%8A%80%E6%9C%AF%E5%85%A8%E6%99%AF%E8%A7%A3%E6%9E%90.html" aria-label="[深度学习] 大模型学习4-RAG技术全景解析" iconsizing="both"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">[深度学习] 大模型学习4-RAG技术全景解析<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><!----><div class="vp-copyright">Copyright © 2025 落痕月极 </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-HB0Nuzez.js" defer></script>
  </body>
</html>
