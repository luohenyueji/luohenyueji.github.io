import{_ as l,c as i,a,d as e,o as s}from"./app-BNuIUq7T.js";const t={},p={class:"MathJax",jax:"SVG",display:"true",style:{position:"relative"}},r={style:{"vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"17.005ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 7516 1000","aria-hidden":"true"};function o(c,n){return s(),i("div",null,[n[2]||(n[2]=a('<h1 id="opencv实战-49-对极几何与立体视觉初探" tabindex="-1"><a class="header-anchor" href="#opencv实战-49-对极几何与立体视觉初探"><span>[OpenCV实战]49 对极几何与立体视觉初探</span></a></h1><p>本文主要介绍对极几何(Epipolar Geometry)与立体视觉(Stereo Vision)的相关知识。对极几何简单点来说，其目的就是描述是两幅视图之间的内部对应关系，用来对立体视觉进行建模，实际上就是一种约束条件，这样可以确定立体匹配时的最优解。对极几何是计算机视觉领域中一个基础概念，具体可以学习文章-<a href="https://blog.csdn.net/xjtuse123/article/details/90311846" target="_blank" rel="noopener noreferrer">对极几何(Epipolar)</a>。对极几何/极几何在各个坐标系（世界坐标系，观察坐标系，像素坐标系）相互转换中是十分重要的一个概念。立体视觉是一种很常用的计算机视觉技术，其目的是从两幅或两幅以上的图像中推理出图像中每个像素点的深度信息,这样可以重构出物体的三维信息，应用价值极高。具体介绍可以见<a href="https://blog.csdn.net/sanwandoujiang/article/details/40951783" target="_blank" rel="noopener noreferrer">立体视觉小结(重点基于双目视觉)</a>和<a href="https://blog.csdn.net/u014055323/article/details/82285525" target="_blank" rel="noopener noreferrer">立体视觉（一 概述）</a>。</p><p>本文主要参考<a href="https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/" target="_blank" rel="noopener noreferrer">Introduction to Epipolar Geometry and Stereo Vision</a>。 本文所有代码见：</p><ul><li>github: <a href="https://github.com/luohenyueji/OpenCV-Practical-Exercise" target="_blank" rel="noopener noreferrer">OpenCV-Practical-Exercise</a></li><li>gitee(备份，主要是下载速度快): <a href="https://gitee.com/luohenyueji/OpenCV-Practical-Exercise-Gitee" target="_blank" rel="noopener noreferrer">OpenCV-Practical-Exercise-gitee</a></li></ul><p><strong>值得注意的是，本文只是基础理论介绍，可以不看，实际这套理论工程并不多,代码也是半成品，了解下就行了。</strong></p><h2 id="_1-基本理论" tabindex="-1"><a class="header-anchor" href="#_1-基本理论"><span>1 基本理论</span></a></h2><h3 id="_1-1-背景介绍" tabindex="-1"><a class="header-anchor" href="#_1-1-背景介绍"><span>1.1 背景介绍</span></a></h3><p>您是否曾经想过为什么戴着3D眼镜观看电影时能体验到如此奇妙的3D效果？或为什么一只眼睛闭上很难接乒乓球？所有这些都与立体视觉有关，立体视觉是我们用两只眼睛感知深度的能力。如下图所示，你可以看到人和椅子的相对移动距离。这篇文章就是使用OpenCV和立体视觉为计算机提供了这种感知深度的能力。该代码在Python和C++中提供。</p><figure><img src="https://img-blog.csdnimg.cn/20210212162614649.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>当然要注意的是本文介绍的相关知识都是最基础的，实际上真实应用都是用深度学习来做以上工作，如下图我们常见的自动驾驶就搭载了基于深度学习的深度预测算法，图(b)中越黑的区域代表距离越远。具体可以见<a href="https://cv.gluon.ai/build/examples_depth/index.html" target="_blank" rel="noopener noreferrer">gluoncv-Depth Prediction</a>。但是了解基础的立体视觉知识还是很有用的。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/2.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>回到第一张所示人体和椅子的图，这张图除了能够检测出不同的对象外，计算机还能够分辨它们的距离。这意味着它可以感知深度！对于这张图，使用了<a href="https://www.kickstarter.com/projects/opencv/opencv-ai-kit" target="_blank" rel="noopener noreferrer">OAK-D(OpenCV AI Kit-Depth)</a>的立体摄像机来帮助计算机感知深度。那么什么是立体摄像机，我们如何使用它为计算机提供深度感？它与立体视觉有关吗？通过理解过与对极几何和立体视觉有关的基本概念可以回答这些问题。 想要学习立体视觉的基础知识，可以参考<a href="https://book.douban.com/subject/1841346/" target="_blank" rel="noopener noreferrer">Multiple View Geometry in Computer Vision</a>这本书。书出版时间挺久了，但是想要深入从事立体视觉相关的工作，这本书是必读的，里面有很多系统性的数学推导和理论解释，基本概念都有涉及。但是如果真的看不懂，用深度学习做做相关应用也是可以的。</p><h3 id="_1-2-为什么需要一张以上的图像来计算深度" tabindex="-1"><a class="header-anchor" href="#_1-2-为什么需要一张以上的图像来计算深度"><span>1.2 为什么需要一张以上的图像来计算深度？</span></a></h3><p>当我们捕获图像中的3D对象时，一般来说我们会将其从三维立体空间投影到二维平面投影空间。也就是我们所说的平面投影。问题是由于这种平面投影，我们会丢失物体深度信息。所以我们需要从二维平面图像恢复物体的深度三维信息。那么我们可以只从一张二维平面图像完成这项工作吗，具体看下面解答。</p><p>在下图中，C1和X是三维空间中的点，单位向量L1给出了光线从C1到X的方向。现在，如果我们知道点C1和方向向量L1的值，我们能找到X吗？</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/3.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>从几何数学上讲，它只意味着求解方程中的X:</p>',17)),e("mjx-container",p,[(s(),i("svg",r,n[0]||(n[0]=[a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1129.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2185.6,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(2945.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3667.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4668,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(5557,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5946,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(6627,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(7127,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g>',1)]))),n[1]||(n[1]=e("mjx-assistive-mml",{unselectable:"on",display:"block"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[e("mi",null,"X"),e("mo",null,"="),e("mi",null,"C"),e("mn",null,"1"),e("mo",null,"+"),e("mi",null,"K"),e("mo",{stretchy:"false"},"("),e("mi",null,"L"),e("mn",null,"1"),e("mo",{stretchy:"false"},")")])],-1))]),n[3]||(n[3]=a(`<p>但是现在，由于我们无法确定K的值，所以我们找不到X的唯一值。</p><p>那么在下图中，我们添加一个附加点C2和一个从C2到X的方向向量L2。如果我们也知道C2和L2，那么我们可以找到X的唯一值吗？答案是肯定的，因为从C1和C2发出的光线在一个唯一的点，即X点本身，清楚地相交。这种方法叫做三角测量/三角测量（Triangulation）。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/4.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>总之，如下图5所示，当在两个不同的视图（图像）中捕获（投影）时，如何使用三角测量来计算点（X）的深度。在该图中，C1和C2分别是左照相机和右照相机的已知3D位置。我们还需要知道L1和L2的相关系，因此通过照相机分别捕获X点的信息，得到图像x1和x2。x1是由左照相机捕获的3D点X的图像，x2是由右照相机捕获的X的图像。x1和x2被称为对应点，因为它们是同一个3D点的投影。我们用x1和C1来寻找L1，用x2和C2来寻找L2。这样我们获得求出X点的信息。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/5.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>从上面的示例中，我们了解到要使用从不同视角捕获的两个图像对3D点进行三角测量，关键要求是：</p><ul><li>摄像头的位置–C1和C2。</li><li>对应点的位置–x1和x2。</li></ul><p>但这只是我们试图计算的一个点，根据不共线的三点可以确定一个平面。我们对两个视图中捕获的所有点重复上述过程，就可以捕捉真实世界场景来计算其中物体的三维结构。</p><h3 id="_1-3-对二视图几何的实践与理论理解" tabindex="-1"><a class="header-anchor" href="#_1-3-对二视图几何的实践与理论理解"><span>1.3 对二视图几何的实践与理论理解</span></a></h3><p>下图显示了两个从不同视角捕获现实场景的图像。为了计算3D结构，我们尝试找到前面提到的两个关键要求：摄像机在现实世界坐标系（C1和C2）中的位置。我们通过假设其中一个摄像机位置（C1或C2）作为原点来计算3D点来简化这个问题。我们通过使用已知的校准模式校准双视图系统来找到它。这个过程称为立体校准/立体定标。具体可以参考<a href="https://blog.csdn.net/u013626386/article/details/51939952" target="_blank" rel="noopener noreferrer">基于OpenCV的立体相机标定StereoCalibration与目标三维坐标定位</a>。其次要计算的场景中每个3D点（X）的点对应关系（x1和x2）。我们将讨论计算点对应的各种改进，并最终了解对极几何如何帮助我们简化问题。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/6.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>请注意，立体相机校准仅当图像由一对相互固定的相机捕获时才有用。如果一台相机从两个不同的角度拍摄图像，那么我们只能找到一个比例的深度。绝对深度是未知的，除非我们有一些关于捕获场景的特殊几何信息，可以用来找到实际的比例。</p><p>下图显示了手动标记的这些对应点。对我们来说，识别相应的点很容易，但是如何使计算机做到这一点呢。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/7.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>人们在计算机视觉社区中经常使用的一种方法称为特征匹配。下图显示了使用ORB特征描述符在左右图像之间匹配的特征。这是一种找到点对应关系（匹配）的方法。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/8.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>特征匹配应用很广泛，具体应用可以见<a href="https://blog.csdn.net/LuohenYJ/article/details/88355444" target="_blank" rel="noopener noreferrer">[OpenCV实战]6 基于特征点匹配的视频稳像</a>和<a href="https://blog.csdn.net/LuohenYJ/article/details/88603274" target="_blank" rel="noopener noreferrer">[OpenCV实战]10 使用Hu矩进行形状匹配</a>。</p><p>但是，我们观察到具有已知点对应关系的像素数与像素总数的比率是最小的。这意味着我们将拥有一个非常稀疏的3D场景。对于更密集的重建，我们需要获得尽可能多的像素点对应。找到点对应的一种简化方法是找到具有相似相邻像素信息的像素。在下图中，我们观察到使用这种匹配具有相似相邻信息的像素的方法会导致来自一个图像的单个像素在另一个图像中具有多个匹配。我们发现编写一个算法来确定真正的匹配很有挑战性，这就是为什么传统算法做立体视觉效果一般的一个主要原因。</p><p>有没有办法减少我们的搜索空间？我们可以使用一些定理来消除所有导致不正确对应的额外假匹配吗？我们在这里利用对极几何，下面我们将了解对极几何在减少点对应关系的搜索空间中的重要性。</p><h3 id="_1-4-对极几何及其在点对应中的应用" tabindex="-1"><a class="header-anchor" href="#_1-4-对极几何及其在点对应中的应用"><span>1.4 对极几何及其在点对应中的应用</span></a></h3><p>在下图中，分别通过C1和C2处的相机在x1和x2处捕获3D点X。因为x1是X的投影，所以如果我们尝试从C1穿过x1的光线R1延伸，它也应该穿过X。在图像i2中，此光线R1被捕获为线L2&#39;，而X被捕获为x2。 当X位于R1上时，x2应该位于L2&#39;上。 这样，x2的可能位置被限制在一条直线L22上。我们可以使用对极几何找到L2。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/9.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>上图中e2是相机成像中心C1在图像i2中的投影，而e1是相机成像中心C2在图像i1中的投影。e1和e2 的术语是极点（epipole）。因此，在双视图几何设置中，极点是一个视图在另一个视图中的相机中心的图像。连接两个相机中心的线称为基线。因此，极点也可以定义为基线与图像平面的交点。上图中使用R1和基线，我们可以定义一个平面P。这个平面还包含X、C1、x1、x2和C2。我们称这个平面为极平面(Epipolar Plane)。此外，从极平面和图像平面的交点获得的线被称为极线（Epipolar geometry）。因此在我们的例子中，L2是一条极线（极平面P和图像i2的交点）。对于不同的X值，我们将有不同的极平面，因此不同的极线。然而，所有的极平面相交于基线，所有的极线相交于极点。所有这些一起形成了对极几何。</p><p>上面已经囊括了我们到目前为止所学到的所有技术术语。我们使用基线B和射线R1创建了极平面P。e1和e2是极点，L2是对极线。基于给定图的对极几何，将图像i2中与像素x1对应的像素的搜索空间限制为一条2D线，即极线l2。这称为极线约束。有没有办法用单个矩阵表示整个对极几何？此外，我们可以仅使用两个捕获的图像来计算该矩阵吗？好消息是有这样一个矩阵，它被称为基本矩阵（the Fundamental matrix。 关于以上更详细的理论解释和公式推导见<a href="https://blog.csdn.net/u011306452/article/details/53924796" target="_blank" rel="noopener noreferrer">对极几何与基本矩阵</a>和<a href="https://blog.csdn.net/qq_41784565/article/details/105671165" target="_blank" rel="noopener noreferrer">计算机视觉--对极几何与基础矩阵</a>。这些东西不是几句话能讲清楚，如果专门研究这些东西，还是看<a href="https://book.douban.com/subject/1841346/" target="_blank" rel="noopener noreferrer">Multiple View Geometry in Computer Vision</a>这本书。</p><p>如果工程应用只要知道我们能够使用基本矩阵找到极线，通过OpenCV的findFundamentalMat方法提供了各种算法（例如7点算法，8点算法，RANSAC算法和L​​MedS算法）的实现，以使用匹配的特征点计算基本矩阵。</p><h2 id="_2-特例" tabindex="-1"><a class="header-anchor" href="#_2-特例"><span>2 特例</span></a></h2><p>现在我们来说说两视图视觉的特例-平行成像平面(parallel imaging planes)。我们一直在努力解决通信问题。我们开始使用特征匹配，但我们观察到它会导致一个稀疏的三维结构。我们学习了如何使用极线几何将点对应的搜索空间缩小到一条线——极线。那么我们可以进一步简化寻找密集点对应关系的过程吗？ 下图展示了显示特征匹配结果的图像和极线上的对应点结果的图像。不管是图(a)和图(b)显示了两对不同图像的特征匹配结果和极线约束。在特征匹配和极线方面，两个图形之间最显着的区别是什么？对于每张图的下图。我们会发现所有的极线平行，右图与左图中的各个点具有相同的纵坐标。我们在左图像的每个像素中搜索右图像的同一行中的相应像素。这是成像平面平行的两视图几何的特殊情况。这大大简化了密集点对应的问题。它帮助我们应用立体视差(stereo disparity)。它类似于立体视觉，帮助人类感知深度的方法。让我们详细了解一下。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/10.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>下图是使用Middlebury Stereo Datasets 2005中的图像生成的。它演示了相机的纯平移运动，使成像平面平行。我们可以清楚地说，底部的玩具牛比最上面一排的玩具更接近镜头。我们是怎么做到的？我们基本上可以在两张图片中看到物体的移动。物体移动越少，物体离我们越近。这种转变就是我们所说的视差。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/11.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>我们为每个像素计算视差（两个图像中像素的偏移），并应用比例映射来查找给定视差值的深度。<a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_depthmap/py_depthmap.html" target="_blank" rel="noopener noreferrer">Depth Map from Stereo Images</a>中的图进一步证明了这一点。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/12.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>上图解释了视差（x–x&#39;）与深度Z之间的关系。视差= x – x&#39;= Bf / Z。其中B是基线（相机之间的距离），f是焦距。上面公式具体推导见<a href="https://blog.csdn.net/weixin_29543211/article/details/112512339" target="_blank" rel="noopener noreferrer">双目视觉的基础知识</a>。</p><p>接下来使用OpenCV的StereoSGBM方法编写代码，以计算给定图像对的视差图。函数参数很多，具体介绍见<a href="https://docs.opencv.org/master/d2/d85/classcv_1_1StereoSGBM.html" target="_blank" rel="noopener noreferrer">StereoSGBM接口介绍</a>，尽可能使用默认参数，如果自行设定参数注意查看详细函数说明，避免出错。</p><p>代码如下：</p><p><strong>C++</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>#include &lt;opencv2/opencv.hpp&gt;</span></span>
<span class="line"><span>#include &lt;stdio.h&gt;</span></span>
<span class="line"><span>#include &lt;string.h&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>int main()</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>	cv::Mat imgL, imgR;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	// 读图</span></span>
<span class="line"><span>	imgL = cv::imread(&quot;images/im0.png&quot;, 0);</span></span>
<span class="line"><span>	cv::resize(imgL, imgL, cv::Size(600, 600));</span></span>
<span class="line"><span>	imgR = cv::imread(&quot;images/im1.png&quot;, 0);</span></span>
<span class="line"><span>	cv::resize(imgR, imgR, cv::Size(600, 600));</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	int minDisparity = 0;</span></span>
<span class="line"><span>	int numDisparities = 64;</span></span>
<span class="line"><span>	int blockSize = 8;</span></span>
<span class="line"><span>	int disp12MaxDiff = 1;</span></span>
<span class="line"><span>	int uniquenessRatio = 10;</span></span>
<span class="line"><span>	int speckleWindowSize = 10;</span></span>
<span class="line"><span>	int speckleRange = 8;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	// 创建StereoSGBM对象</span></span>
<span class="line"><span>	cv::Ptr&lt;cv::StereoSGBM&gt; stereo = cv::StereoSGBM::create(minDisparity, numDisparities, blockSize, disp12MaxDiff, uniquenessRatio, speckleWindowSize, speckleRange);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	cv::Mat disp;</span></span>
<span class="line"><span>	// 计算视差</span></span>
<span class="line"><span>	stereo-&gt;compute(imgL, imgR, disp);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	// 结果归一化</span></span>
<span class="line"><span>	cv::normalize(disp, disp, 0, 255, cv::NORM_MINMAX, CV_8UC1);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	cv::imshow(&quot;Left image&quot;, imgL);</span></span>
<span class="line"><span>	cv::imshow(&quot;Right image&quot;, imgR);</span></span>
<span class="line"><span>	cv::imshow(&quot;disparity&quot;, disp);</span></span>
<span class="line"><span>	cv::waitKey(0);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>	return 0;</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Python</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>from __future__ import print_function</span></span>
<span class="line"><span></span></span>
<span class="line"><span>import numpy as np</span></span>
<span class="line"><span>import cv2</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span># ----- 读图</span></span>
<span class="line"><span>imgL = cv2.imread(&quot;images/im0.png&quot;,1)</span></span>
<span class="line"><span>imgL = cv2.resize(imgL,(600,600))</span></span>
<span class="line"><span></span></span>
<span class="line"><span>imgR = cv2.imread(&quot;images/im1.png&quot;,1)</span></span>
<span class="line"><span>imgR = cv2.resize(imgR,(600,600))</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Setting parameters for StereoSGBM algorithm</span></span>
<span class="line"><span># 设置 StereoSGBM相关参数</span></span>
<span class="line"><span>minDisparity = 0</span></span>
<span class="line"><span>numDisparities = 64</span></span>
<span class="line"><span>blockSize = 8</span></span>
<span class="line"><span>disp12MaxDiff = 1</span></span>
<span class="line"><span>uniquenessRatio = 10</span></span>
<span class="line"><span>speckleWindowSize = 10</span></span>
<span class="line"><span>speckleRange = 8</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Creating an object of StereoSGBM algorithm</span></span>
<span class="line"><span># 创建StereoSGBM对象</span></span>
<span class="line"><span>stereo = cv2.StereoSGBM_create(minDisparity = minDisparity,</span></span>
<span class="line"><span>    numDisparities = numDisparities,</span></span>
<span class="line"><span>    blockSize = blockSize,</span></span>
<span class="line"><span>    disp12MaxDiff = disp12MaxDiff,</span></span>
<span class="line"><span>    uniquenessRatio = uniquenessRatio,</span></span>
<span class="line"><span>    speckleWindowSize = speckleWindowSize,</span></span>
<span class="line"><span>    speckleRange = speckleRange</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Calculating disparith using the StereoSGBM algorithm</span></span>
<span class="line"><span># 计算视差</span></span>
<span class="line"><span>disp = stereo.compute(imgL, imgR).astype(np.float32)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 结果归一化</span></span>
<span class="line"><span>disp = cv2.normalize(disp,0,255,cv2.NORM_MINMAX)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Displaying the disparity map</span></span>
<span class="line"><span># 显示结果</span></span>
<span class="line"><span>cv2.imshow(&quot;disparity&quot;,disp)</span></span>
<span class="line"><span>cv2.imshow(&quot;left image&quot;,imgL)</span></span>
<span class="line"><span>cv2.imshow(&quot;right image&quot;,imgR)</span></span>
<span class="line"><span>cv2.waitKey(0)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>cv2.destroyAllWindows()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面程序结果如下图所示。</p><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]49 对极几何与立体视觉初探/image/13.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-参考" tabindex="-1"><a class="header-anchor" href="#_3-参考"><span>3 参考</span></a></h2><h3 id="_3-1-理论" tabindex="-1"><a class="header-anchor" href="#_3-1-理论"><span>3.1 理论</span></a></h3><ul><li><a href="https://blog.csdn.net/xjtuse123/article/details/90311846" target="_blank" rel="noopener noreferrer">对极几何(Epipolar)</a></li><li><a href="https://blog.csdn.net/sanwandoujiang/article/details/40951783" target="_blank" rel="noopener noreferrer">立体视觉小结(重点基于双目视觉)</a></li><li><a href="https://blog.csdn.net/u014055323/article/details/82285525" target="_blank" rel="noopener noreferrer">立体视觉（一 概述）</a></li><li><a href="https://cv.gluon.ai/build/examples_depth/index.html" target="_blank" rel="noopener noreferrer">gluoncv-Depth Prediction¶</a></li><li><a href="https://www.kickstarter.com/projects/opencv/opencv-ai-kit" target="_blank" rel="noopener noreferrer">OAK-D(OpenCV AI Kit-Depth)</a></li><li><a href="https://book.douban.com/subject/1841346/" target="_blank" rel="noopener noreferrer">Multiple View Geometry in Computer Vision</a></li><li><a href="https://blog.csdn.net/u013626386/article/details/51939952" target="_blank" rel="noopener noreferrer">基于OpenCV的立体相机标定StereoCalibration与目标三维坐标定位</a></li><li><a href="https://blog.csdn.net/u011306452/article/details/53924796" target="_blank" rel="noopener noreferrer">对极几何与基本矩阵</a></li><li><a href="https://blog.csdn.net/qq_41784565/article/details/105671165" target="_blank" rel="noopener noreferrer">计算机视觉--对极几何与基础矩阵</a></li><li><a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_depthmap/py_depthmap.html" target="_blank" rel="noopener noreferrer">Depth Map from Stereo Images</a></li><li><a href="https://blog.csdn.net/weixin_29543211/article/details/112512339" target="_blank" rel="noopener noreferrer">双目视觉的基础知识</a></li></ul><h3 id="_3-2-工程" tabindex="-1"><a class="header-anchor" href="#_3-2-工程"><span>3.2 工程</span></a></h3><ul><li><a href="https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/" target="_blank" rel="noopener noreferrer">Introduction to Epipolar Geometry and Stereo Vision</a></li><li><a href="https://blog.csdn.net/LuohenYJ/article/details/88355444" target="_blank" rel="noopener noreferrer">[OpenCV实战]6 基于特征点匹配的视频稳像</a></li><li><a href="https://blog.csdn.net/LuohenYJ/article/details/88603274" target="_blank" rel="noopener noreferrer">[OpenCV实战]10 使用Hu矩进行形状匹配</a></li><li><a href="https://docs.opencv.org/master/d2/d85/classcv_1_1StereoSGBM.html" target="_blank" rel="noopener noreferrer">StereoSGBM接口介绍</a></li></ul>`,46))])}const g=l(t,[["render",o],["__file","2021-02-12-_OpenCV实战_49 对极几何与立体视觉初探.html.vue"]]),m=JSON.parse('{"path":"/blog/opencv/opencv%E5%AE%9E%E6%88%98/2021-02-12-_OpenCV%E5%AE%9E%E6%88%98_49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2.html","title":"[OpenCV实战]49 对极几何与立体视觉初探","lang":"zh-CN","frontmatter":{"category":["OpenCV"],"date":"2021-02-12T16:27:12.000Z","tag":["OpenCV实战","OpenCV","图像处理"],"description":"[OpenCV实战]49 对极几何与立体视觉初探 本文主要介绍对极几何(Epipolar Geometry)与立体视觉(Stereo Vision)的相关知识。对极几何简单点来说，其目的就是描述是两幅视图之间的内部对应关系，用来对立体视觉进行建模，实际上就是一种约束条件，这样可以确定立体匹配时的最优解。对极几何是计算机视觉领域中一个基础概念，具体可以学...","head":[["meta",{"property":"og:url","content":"https://luohenyueji.github.io/blog/opencv/opencv%E5%AE%9E%E6%88%98/2021-02-12-_OpenCV%E5%AE%9E%E6%88%98_49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2.html"}],["meta",{"property":"og:site_name","content":"落痕月极的博客"}],["meta",{"property":"og:title","content":"[OpenCV实战]49 对极几何与立体视觉初探"}],["meta",{"property":"og:description","content":"[OpenCV实战]49 对极几何与立体视觉初探 本文主要介绍对极几何(Epipolar Geometry)与立体视觉(Stereo Vision)的相关知识。对极几何简单点来说，其目的就是描述是两幅视图之间的内部对应关系，用来对立体视觉进行建模，实际上就是一种约束条件，这样可以确定立体匹配时的最优解。对极几何是计算机视觉领域中一个基础概念，具体可以学..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://img-blog.csdnimg.cn/20210212162614649.gif"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:tag","content":"OpenCV实战"}],["meta",{"property":"article:tag","content":"OpenCV"}],["meta",{"property":"article:tag","content":"图像处理"}],["meta",{"property":"article:published_time","content":"2021-02-12T16:27:12.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"[OpenCV实战]49 对极几何与立体视觉初探\\",\\"image\\":[\\"https://img-blog.csdnimg.cn/20210212162614649.gif\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/2.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/3.png\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/4.png\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/5.png\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/6.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/7.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/8.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/9.png\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/10.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/11.gif\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/12.jpg\\",\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D49%20%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95%E4%B8%8E%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89%E5%88%9D%E6%8E%A2/image/13.jpg\\"],\\"datePublished\\":\\"2021-02-12T16:27:12.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"落痕月极\\",\\"url\\":\\"/\\"}]}"]]},"headers":[{"level":2,"title":"1 基本理论","slug":"_1-基本理论","link":"#_1-基本理论","children":[{"level":3,"title":"1.1 背景介绍","slug":"_1-1-背景介绍","link":"#_1-1-背景介绍","children":[]},{"level":3,"title":"1.2 为什么需要一张以上的图像来计算深度？","slug":"_1-2-为什么需要一张以上的图像来计算深度","link":"#_1-2-为什么需要一张以上的图像来计算深度","children":[]},{"level":3,"title":"1.3 对二视图几何的实践与理论理解","slug":"_1-3-对二视图几何的实践与理论理解","link":"#_1-3-对二视图几何的实践与理论理解","children":[]},{"level":3,"title":"1.4 对极几何及其在点对应中的应用","slug":"_1-4-对极几何及其在点对应中的应用","link":"#_1-4-对极几何及其在点对应中的应用","children":[]}]},{"level":2,"title":"2 特例","slug":"_2-特例","link":"#_2-特例","children":[]},{"level":2,"title":"3 参考","slug":"_3-参考","link":"#_3-参考","children":[{"level":3,"title":"3.1 理论","slug":"_3-1-理论","link":"#_3-1-理论","children":[]},{"level":3,"title":"3.2 工程","slug":"_3-2-工程","link":"#_3-2-工程","children":[]}]}],"git":{},"readingTime":{"minutes":16,"words":4799},"filePathRelative":"blog/opencv/opencv实战/2021-02-12-[OpenCV实战]49 对极几何与立体视觉初探.md","localizedDate":"2021年2月13日","excerpt":"\\n<p>本文主要介绍对极几何(Epipolar Geometry)与立体视觉(Stereo Vision)的相关知识。对极几何简单点来说，其目的就是描述是两幅视图之间的内部对应关系，用来对立体视觉进行建模，实际上就是一种约束条件，这样可以确定立体匹配时的最优解。对极几何是计算机视觉领域中一个基础概念，具体可以学习文章-<a href=\\"https://blog.csdn.net/xjtuse123/article/details/90311846\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">对极几何(Epipolar)</a>。对极几何/极几何在各个坐标系（世界坐标系，观察坐标系，像素坐标系）相互转换中是十分重要的一个概念。立体视觉是一种很常用的计算机视觉技术，其目的是从两幅或两幅以上的图像中推理出图像中每个像素点的深度信息,这样可以重构出物体的三维信息，应用价值极高。具体介绍可以见<a href=\\"https://blog.csdn.net/sanwandoujiang/article/details/40951783\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">立体视觉小结(重点基于双目视觉)</a>和<a href=\\"https://blog.csdn.net/u014055323/article/details/82285525\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">立体视觉（一 概述）</a>。</p>","autoDesc":true}');export{g as comp,m as data};
