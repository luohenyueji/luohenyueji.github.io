import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as o,o as l,c as s,a as t,b as e,d as r,e as n}from"./app-MsA2k2kn.js";const d={},h=t("h1",{id:"深度学习-深度学习优化器选择学习笔记",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#深度学习-深度学习优化器选择学习笔记","aria-hidden":"true"},"#"),e(" [深度学习] 深度学习优化器选择学习笔记")],-1),c={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},g=t("strong",null,"pytorch-optimizer",-1),p={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},_=t("strong",null,"pytorch-optimizer",-1),b=t("h2",{id:"_1-简介",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_1-简介","aria-hidden":"true"},"#"),e(" 1 简介")],-1),u={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},y=t("strong",null,"pytorch-optimizer",-1),m=t("thead",null,[t("tr",null,[t("th",{style:{"text-align":"center"}},"optimizer"),t("th",{style:{"text-align":"left"}},"paper")])],-1),x=t("td",{style:{"text-align":"center"}},"A2GradExp",-1),f={style:{"text-align":"left"}},w={href:"https://arxiv.org/abs/1810.00553",target:"_blank",rel:"noopener noreferrer"},k=t("td",{style:{"text-align":"center"}},"A2GradInc",-1),A={style:{"text-align":"left"}},S={href:"https://arxiv.org/abs/1810.00553",target:"_blank",rel:"noopener noreferrer"},D=t("td",{style:{"text-align":"center"}},"A2GradUni",-1),v={style:{"text-align":"left"}},z={href:"https://arxiv.org/abs/1810.00553",target:"_blank",rel:"noopener noreferrer"},N=t("td",{style:{"text-align":"center"}},"AccSGD",-1),C={style:{"text-align":"left"}},j={href:"https://arxiv.org/abs/1803.05591",target:"_blank",rel:"noopener noreferrer"},G=t("td",{style:{"text-align":"center"}},"AdaBelief",-1),R={style:{"text-align":"left"}},P={href:"https://arxiv.org/abs/2010.07468",target:"_blank",rel:"noopener noreferrer"},L=t("td",{style:{"text-align":"center"}},"AdaBound",-1),M={style:{"text-align":"left"}},I={href:"https://arxiv.org/abs/1902.09843",target:"_blank",rel:"noopener noreferrer"},O=t("td",{style:{"text-align":"center"}},"AdaMod",-1),B={style:{"text-align":"left"}},Q={href:"https://arxiv.org/abs/1910.12249",target:"_blank",rel:"noopener noreferrer"},H=t("td",{style:{"text-align":"center"}},"Adafactor",-1),Y={style:{"text-align":"left"}},V={href:"https://arxiv.org/abs/1804.04235",target:"_blank",rel:"noopener noreferrer"},W=t("td",{style:{"text-align":"center"}},"AdamP",-1),T={style:{"text-align":"left"}},q={href:"https://arxiv.org/abs/2006.08217",target:"_blank",rel:"noopener noreferrer"},E=t("td",{style:{"text-align":"center"}},"AggMo",-1),U={style:{"text-align":"left"}},K={href:"https://arxiv.org/abs/1804.00325",target:"_blank",rel:"noopener noreferrer"},F=t("td",{style:{"text-align":"center"}},"Apollo",-1),J={style:{"text-align":"left"}},X={href:"https://arxiv.org/abs/2009.13586",target:"_blank",rel:"noopener noreferrer"},Z=t("td",{style:{"text-align":"center"}},"DiffGrad",-1),$={style:{"text-align":"left"}},tt={href:"https://arxiv.org/abs/1909.11015",target:"_blank",rel:"noopener noreferrer"},et=t("td",{style:{"text-align":"center"}},"Lamb",-1),at={style:{"text-align":"left"}},rt={href:"https://arxiv.org/abs/1904.00962",target:"_blank",rel:"noopener noreferrer"},nt=t("td",{style:{"text-align":"center"}},"Lookahead",-1),it={style:{"text-align":"left"}},ot={href:"https://arxiv.org/abs/1907.08610",target:"_blank",rel:"noopener noreferrer"},lt=t("td",{style:{"text-align":"center"}},"NovoGrad",-1),st={style:{"text-align":"left"}},dt={href:"https://arxiv.org/abs/1905.11286",target:"_blank",rel:"noopener noreferrer"},ht=t("td",{style:{"text-align":"center"}},"PID",-1),ct={style:{"text-align":"left"}},gt={href:"https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf",target:"_blank",rel:"noopener noreferrer"},pt=t("td",{style:{"text-align":"center"}},"QHAdam",-1),_t={style:{"text-align":"left"}},bt={href:"https://arxiv.org/abs/1810.06801",target:"_blank",rel:"noopener noreferrer"},ut=t("td",{style:{"text-align":"center"}},"QHM",-1),yt={style:{"text-align":"left"}},mt={href:"https://arxiv.org/abs/1810.06801",target:"_blank",rel:"noopener noreferrer"},xt=t("td",{style:{"text-align":"center"}},"RAdam",-1),ft={style:{"text-align":"left"}},wt={href:"https://arxiv.org/abs/1908.03265",target:"_blank",rel:"noopener noreferrer"},kt=t("td",{style:{"text-align":"center"}},"Ranger",-1),At={style:{"text-align":"left"}},St={href:"https://arxiv.org/abs/1908.00700v2",target:"_blank",rel:"noopener noreferrer"},Dt=t("td",{style:{"text-align":"center"}},"RangerQH",-1),vt={style:{"text-align":"left"}},zt={href:"https://arxiv.org/abs/1908.00700v2",target:"_blank",rel:"noopener noreferrer"},Nt=t("td",{style:{"text-align":"center"}},"RangerVA",-1),Ct={style:{"text-align":"left"}},jt={href:"https://arxiv.org/abs/1908.00700v2",target:"_blank",rel:"noopener noreferrer"},Gt=t("td",{style:{"text-align":"center"}},"SGDP",-1),Rt={style:{"text-align":"left"}},Pt={href:"https://arxiv.org/abs/2006.08217",target:"_blank",rel:"noopener noreferrer"},Lt=t("td",{style:{"text-align":"center"}},"SGDW",-1),Mt={style:{"text-align":"left"}},It={href:"https://arxiv.org/abs/1608.03983",target:"_blank",rel:"noopener noreferrer"},Ot=t("td",{style:{"text-align":"center"}},"SWATS",-1),Bt={style:{"text-align":"left"}},Qt={href:"https://arxiv.org/abs/1712.07628",target:"_blank",rel:"noopener noreferrer"},Ht=t("td",{style:{"text-align":"center"}},"Shampoo",-1),Yt={style:{"text-align":"left"}},Vt={href:"https://arxiv.org/abs/1802.09568",target:"_blank",rel:"noopener noreferrer"},Wt=t("td",{style:{"text-align":"center"}},"Yogi",-1),Tt={style:{"text-align":"left"}},qt={href:"https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization",target:"_blank",rel:"noopener noreferrer"},Et={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},Ut=t("strong",null,"pytorch-optimizer",-1),Kt={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},Ft=t("strong",null,"pytorch-optimizer",-1),Jt=n('<ol><li>Rosenbrock（也称为香蕉函数）是具有一个全局最小值(1.0,1.0)的非凸函数。整体最小值位于一个细长的，抛物线形的平坦山谷内。寻找山谷是微不足道的。但是，要收敛到全局最小值(1.0，1.0)是很困难的。优化算法可能会陷入局部最小值。</li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/demo/1.jpg" alt="Rosenbrock" tabindex="0" loading="lazy"><figcaption>Rosenbrock</figcaption></figure><ol start="2"><li>Rastrigin函数是非凸函数，并且在(0.0，0.0)中具有一个全局最小值。由于此函数的搜索空间很大且局部最小值很大，因此找到该函数的最小值是一个相当困难的工作。</li></ol><figure><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/demo/2.jpg" alt="Rastrigin" tabindex="0" loading="lazy"><figcaption>Rastrigin</figcaption></figure><h2 id="_2-结果" tabindex="-1"><a class="header-anchor" href="#_2-结果" aria-hidden="true">#</a> 2 结果</h2><p>下面分别显示不同年份算法在Rastrigin和Rosenbrock函数下的结果，结果显示为Rastrigin和Rosenbroc从上往下的投影图，其中绿色点表示最优点，结果坐标越接近绿色点表示optimizer效果越好。个人觉得效果较好的方法会在方法标题后加*。</p><h3 id="a2gradexp-2018" tabindex="-1"><a class="header-anchor" href="#a2gradexp-2018" aria-hidden="true">#</a> A2GradExp(2018)</h3><p>Paper: Optimal Adaptive and Accelerated Stochastic Gradient Descent (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_A2GradExp.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_A2GradExp.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="a2gradinc-2018" tabindex="-1"><a class="header-anchor" href="#a2gradinc-2018" aria-hidden="true">#</a> A2GradInc(2018)</h3><p>Paper: Optimal Adaptive and Accelerated Stochastic Gradient Descent (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_A2GradInc.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_A2GradInc.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="a2graduni-2018" tabindex="-1"><a class="header-anchor" href="#a2graduni-2018" aria-hidden="true">#</a> A2GradUni(2018)</h3><p>Paper: Optimal Adaptive and Accelerated Stochastic Gradient Descent (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_A2GradUni.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_A2GradUni.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="accsgd-2019" tabindex="-1"><a class="header-anchor" href="#accsgd-2019" aria-hidden="true">#</a> AccSGD(2019)</h3><p>Paper: On the insufficiency of existing momentum schemes for Stochastic Optimization (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AccSGD.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AccSGD.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adabelief-2020" tabindex="-1"><a class="header-anchor" href="#adabelief-2020" aria-hidden="true">#</a> AdaBelief(2020)</h3><p>Paper: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients (2020)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AdaBelief.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AdaBelief.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adabound-2019" tabindex="-1"><a class="header-anchor" href="#adabound-2019" aria-hidden="true">#</a> AdaBound(2019)</h3><p>Paper: An Adaptive and Momental Bound Method for Stochastic Learning. (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AdaBound.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AdaBound.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adamod-2019" tabindex="-1"><a class="header-anchor" href="#adamod-2019" aria-hidden="true">#</a> AdaMod(2019)</h3><p>Paper: An Adaptive and Momental Bound Method for Stochastic Learning. (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AdaMod.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AdaMod.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adafactor-2018" tabindex="-1"><a class="header-anchor" href="#adafactor-2018" aria-hidden="true">#</a> Adafactor(2018)</h3><p>Paper: Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Adafactor.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Adafactor.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adamp-2020" tabindex="-1"><a class="header-anchor" href="#adamp-2020" aria-hidden="true">#</a> AdamP(2020)</h3><p>Paper: Slowing Down the Weight Norm Increase in Momentum-based Optimizers. (2020)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AdamP.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AdamP.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="aggmo-2019" tabindex="-1"><a class="header-anchor" href="#aggmo-2019" aria-hidden="true">#</a> AggMo(2019)</h3><p>Paper: Aggregated Momentum: Stability Through Passive Damping. (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_AggMo.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_AggMo.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="apollo-2020" tabindex="-1"><a class="header-anchor" href="#apollo-2020" aria-hidden="true">#</a> Apollo(2020)</h3><p>Paper: Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization. (2020)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Apollo.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Apollo.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="diffgrad-2019" tabindex="-1"><a class="header-anchor" href="#diffgrad-2019" aria-hidden="true">#</a> DiffGrad*(2019)</h3>',40),Xt={href:"https://arxiv.org/abs/1909.11015",target:"_blank",rel:"noopener noreferrer"},Zt={href:"https://github.com/shivram1987/diffGrad",target:"_blank",rel:"noopener noreferrer"},$t=n('<table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_DiffGrad.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_DiffGrad.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="lamb-2019" tabindex="-1"><a class="header-anchor" href="#lamb-2019" aria-hidden="true">#</a> Lamb(2019)</h3><p>Paper: Large Batch Optimization for Deep Learning: Training BERT in 76 minutes (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Lamb.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Lamb.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="lookahead-2019" tabindex="-1"><a class="header-anchor" href="#lookahead-2019" aria-hidden="true">#</a> Lookahead*(2019)</h3>',5),te={href:"https://arxiv.org/abs/1907.08610",target:"_blank",rel:"noopener noreferrer"},ee={href:"https://github.com/alphadl/lookahead.pytorch",target:"_blank",rel:"noopener noreferrer"},ae=n('<p><strong>非常需要注意的是Lookahead严格来说不算一种优化器，Lookahead需要一种其他优化器搭配工作，这里Lookahead搭配Yogi进行优化</strong></p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_LookaheadYogi.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_LookaheadYogi.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="novograd-2019" tabindex="-1"><a class="header-anchor" href="#novograd-2019" aria-hidden="true">#</a> NovoGrad(2019)</h3><p>Paper: Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_NovoGrad.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_NovoGrad.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="pid-2018" tabindex="-1"><a class="header-anchor" href="#pid-2018" aria-hidden="true">#</a> PID(2018)</h3><p>Paper: A PID Controller Approach for Stochastic Optimization of Deep Networks (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_PID.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_PID.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="qhadam-2019" tabindex="-1"><a class="header-anchor" href="#qhadam-2019" aria-hidden="true">#</a> QHAdam(2019)</h3><p>Paper: Quasi-hyperbolic momentum and Adam for deep learning (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_QHAdam.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_QHAdam.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="qhm-2019" tabindex="-1"><a class="header-anchor" href="#qhm-2019" aria-hidden="true">#</a> QHM(2019)</h3><p>Paper: Quasi-hyperbolic momentum and Adam for deep learning (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_QHM.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_QHM.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="radam-2019" tabindex="-1"><a class="header-anchor" href="#radam-2019" aria-hidden="true">#</a> RAdam*(2019)</h3>',15),re={href:"https://arxiv.org/abs/1908.03265",target:"_blank",rel:"noopener noreferrer"},ne={href:"https://github.com/LiyuanLucasLiu/RAdam",target:"_blank",rel:"noopener noreferrer"},ie=n('<table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_RAdam.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_RAdam.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="ranger-2019" tabindex="-1"><a class="header-anchor" href="#ranger-2019" aria-hidden="true">#</a> Ranger(2019)</h3><p>Paper: Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Ranger.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Ranger.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="rangerqh-2019" tabindex="-1"><a class="header-anchor" href="#rangerqh-2019" aria-hidden="true">#</a> RangerQH(2019)</h3><p>Paper: Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_RangerQH.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_RangerQH.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="rangerva-2019" tabindex="-1"><a class="header-anchor" href="#rangerva-2019" aria-hidden="true">#</a> RangerVA(2019)</h3><p>Paper: Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM (2019)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_RangerVA.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_RangerVA.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="sgdp-2020" tabindex="-1"><a class="header-anchor" href="#sgdp-2020" aria-hidden="true">#</a> SGDP(2020)</h3><p>Paper: Slowing Down the Weight Norm Increase in Momentum-based Optimizers. (2020)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_SGDP.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_SGDP.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="sgdw-2017" tabindex="-1"><a class="header-anchor" href="#sgdw-2017" aria-hidden="true">#</a> SGDW(2017)</h3><p>Paper: SGDR: Stochastic Gradient Descent with Warm Restarts (2017)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_SGDW.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_SGDW.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="swats-2017" tabindex="-1"><a class="header-anchor" href="#swats-2017" aria-hidden="true">#</a> SWATS(2017)</h3><p>Paper: Improving Generalization Performance by Switching from Adam to SGD (2017)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_SWATS.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_SWATS.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="shampoo-2018" tabindex="-1"><a class="header-anchor" href="#shampoo-2018" aria-hidden="true">#</a> Shampoo(2018)</h3><p>Paper: Shampoo: Preconditioned Stochastic Tensor Optimization (2018)</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Shampoo.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Shampoo.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="yogi-2018" tabindex="-1"><a class="header-anchor" href="#yogi-2018" aria-hidden="true">#</a> Yogi*(2018)</h3>',23),oe={href:"https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization",target:"_blank",rel:"noopener noreferrer"},le={href:"https://github.com/4rtemi5/Yogi-Optimizer_Keras",target:"_blank",rel:"noopener noreferrer"},se=n('<table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Yogi.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Yogi.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="adam" tabindex="-1"><a class="header-anchor" href="#adam" aria-hidden="true">#</a> Adam</h3><p>pytorch自带</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_Adam.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_Adam.png" alt="" loading="lazy"></td></tr></tbody></table><h3 id="sgd" tabindex="-1"><a class="header-anchor" href="#sgd" aria-hidden="true">#</a> SGD</h3><p>pytorch自带</p><table><thead><tr><th style="text-align:center;">rastrigin</th><th style="text-align:center;">rosenbrock</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rastrigin_SGD.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/CSDN/[深度学习] 深度学习优化器选择学习笔记/docs/rosenbrock_SGD.png" alt="" loading="lazy"></td></tr></tbody></table><h2 id="_3-评价" tabindex="-1"><a class="header-anchor" href="#_3-评价" aria-hidden="true">#</a> 3 评价</h2>',8),de={href:"https://www.zhihu.com/question/340834465",target:"_blank",rel:"noopener noreferrer"},he=t("p",null,"结合可视化结果，实际下调参，先试试不同的学习率，然后再选择不同的优化器，如果不会调参，优化器个人推荐选择顺序如下：",-1),ce=t("ol",null,[t("li",null,"Adam"),t("li",null,"Lookahead + (Adam or Yogi or RAdam)"),t("li",null,"带有动量的SGD"),t("li",null,"RAdam,Yogi,DiffGrad")],-1),ge=t("h2",{id:"_4-参考",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#_4-参考","aria-hidden":"true"},"#"),e(" 4 参考")],-1),pe={href:"https://github.com/jettify/pytorch-optimizer",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://github.com/pytorch/pytorch",target:"_blank",rel:"noopener noreferrer"},be={href:"https://www.zhihu.com/question/340834465",target:"_blank",rel:"noopener noreferrer"};function ue(ye,me){const a=o("ExternalLinkIcon");return l(),s("div",null,[h,t("p",null,[e("本文主要展示各类深度学习优化器Optimizer的效果。所有结果基于pytorch实现，参考github项目"),t("a",c,[g,e("(仓库地址)"),r(a)]),e("的结果。"),t("a",p,[_,r(a)]),e("基于pytorch实现了常用的optimizer，非常推荐使用并加星该仓库。")]),b,t("p",null,[t("a",u,[y,r(a)]),e("中所实现的optimizer及其文章主要如下所示。关于optimizer的优化研究非常多，但是不同任务，不同数据集所使用的optimizer效果都不一样，看看研究结果就行了。")]),t("table",null,[m,t("tbody",null,[t("tr",null,[x,t("td",f,[t("a",w,[e("https://arxiv.org/abs/1810.00553"),r(a)])])]),t("tr",null,[k,t("td",A,[t("a",S,[e("https://arxiv.org/abs/1810.00553"),r(a)])])]),t("tr",null,[D,t("td",v,[t("a",z,[e("https://arxiv.org/abs/1810.00553"),r(a)])])]),t("tr",null,[N,t("td",C,[t("a",j,[e("https://arxiv.org/abs/1803.05591"),r(a)])])]),t("tr",null,[G,t("td",R,[t("a",P,[e("https://arxiv.org/abs/2010.07468"),r(a)])])]),t("tr",null,[L,t("td",M,[t("a",I,[e("https://arxiv.org/abs/1902.09843"),r(a)])])]),t("tr",null,[O,t("td",B,[t("a",Q,[e("https://arxiv.org/abs/1910.12249"),r(a)])])]),t("tr",null,[H,t("td",Y,[t("a",V,[e("https://arxiv.org/abs/1804.04235"),r(a)])])]),t("tr",null,[W,t("td",T,[t("a",q,[e("https://arxiv.org/abs/2006.08217"),r(a)])])]),t("tr",null,[E,t("td",U,[t("a",K,[e("https://arxiv.org/abs/1804.00325"),r(a)])])]),t("tr",null,[F,t("td",J,[t("a",X,[e("https://arxiv.org/abs/2009.13586"),r(a)])])]),t("tr",null,[Z,t("td",$,[t("a",tt,[e("https://arxiv.org/abs/1909.11015"),r(a)])])]),t("tr",null,[et,t("td",at,[t("a",rt,[e("https://arxiv.org/abs/1904.00962"),r(a)])])]),t("tr",null,[nt,t("td",it,[t("a",ot,[e("https://arxiv.org/abs/1907.08610"),r(a)])])]),t("tr",null,[lt,t("td",st,[t("a",dt,[e("https://arxiv.org/abs/1905.11286"),r(a)])])]),t("tr",null,[ht,t("td",ct,[t("a",gt,[e("https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf"),r(a)])])]),t("tr",null,[pt,t("td",_t,[t("a",bt,[e("https://arxiv.org/abs/1810.06801"),r(a)])])]),t("tr",null,[ut,t("td",yt,[t("a",mt,[e("https://arxiv.org/abs/1810.06801"),r(a)])])]),t("tr",null,[xt,t("td",ft,[t("a",wt,[e("https://arxiv.org/abs/1908.03265"),r(a)])])]),t("tr",null,[kt,t("td",At,[t("a",St,[e("https://arxiv.org/abs/1908.00700v2"),r(a)])])]),t("tr",null,[Dt,t("td",vt,[t("a",zt,[e("https://arxiv.org/abs/1908.00700v2"),r(a)])])]),t("tr",null,[Nt,t("td",Ct,[t("a",jt,[e("https://arxiv.org/abs/1908.00700v2"),r(a)])])]),t("tr",null,[Gt,t("td",Rt,[t("a",Pt,[e("https://arxiv.org/abs/2006.08217"),r(a)])])]),t("tr",null,[Lt,t("td",Mt,[t("a",It,[e("https://arxiv.org/abs/1608.03983"),r(a)])])]),t("tr",null,[Ot,t("td",Bt,[t("a",Qt,[e("https://arxiv.org/abs/1712.07628"),r(a)])])]),t("tr",null,[Ht,t("td",Yt,[t("a",Vt,[e("https://arxiv.org/abs/1802.09568"),r(a)])])]),t("tr",null,[Wt,t("td",Tt,[t("a",qt,[e("https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization"),r(a)])])])])]),t("p",null,[e("为了评估不同optimizer的效果，"),t("a",Et,[Ut,r(a)]),e("使用可视化方法来评估optimizer。可视化帮助我们了解不同的算法如何处理简单的情况，例如：鞍点，局部极小值，最低值等，并可能为算法的内部工作提供有趣的见解。"),t("a",Kt,[Ft,r(a)]),e("选择了Rosenbrock和Rastrigin 函数来进行可视化。具体如下：")]),Jt,t("p",null,[e("Paper: "),t("a",Xt,[e("diffGrad: An Optimization Method for Convolutional Neural Networks. (2019)"),r(a)])]),t("p",null,[e("Reference Code: "),t("a",Zt,[e("https://github.com/shivram1987/diffGrad"),r(a)])]),$t,t("p",null,[e("Paper: "),t("a",te,[e("Lookahead Optimizer: k steps forward, 1 step back (2019)"),r(a)])]),t("p",null,[e("Reference Code: "),t("a",ee,[e("https://github.com/alphadl/lookahead.pytorch"),r(a)])]),ae,t("p",null,[e("Paper: "),t("a",re,[e("On the Variance of the Adaptive Learning Rate and Beyond (2019)"),r(a)])]),t("p",null,[e("Reference Code: "),t("a",ne,[e("https://github.com/LiyuanLucasLiu/RAdam"),r(a)])]),ie,t("p",null,[e("Paper: "),t("a",oe,[e("Adaptive Methods for Nonconvex Optimization (2018)"),r(a)])]),t("p",null,[e("Reference Code: "),t("a",le,[e("https://github.com/4rtemi5/Yogi-Optimizer_Keras"),r(a)])]),se,t("p",null,[e("看了第2节的结果，DiffGrad，Lookahead,RAdam,Yogi的结果应该还算不错。但是这种可视化结果并不完全正确，一方面训练的epoch太少，另外一方面数据不同以及学习率不同，结果也会大大不同。所以选择合适的优化器在实际调参中还是要具体应用。比如在这个可视化结果中，SGD和Adam效果一般，但是实际上SGD和Adam是广泛验证的优化器，各个任务都能获得不错的结果。SGD是著名的大后期选手，Adam无脑调参最优算法。RAdam很不错，但是并没有那么强，具体RAdam的评价见"),t("a",de,[e("如何看待最新提出的Rectified Adam (RAdam)？"),r(a)]),e("。DiffGrad和Yogi某些任务不错，在某些任务可能效果更差，实际选择需要多次评估。Lookahead是Adam作者和Hinton联合推出的训练优化器，Lookahead可以配合多种优化器，好用是好用，可能没效果，但是一般都会有点提升，实际用Lookahead还是挺不错的。")]),he,ce,ge,t("ul",null,[t("li",null,[t("a",pe,[e("pytorch-optimizer"),r(a)])]),t("li",null,[t("a",_e,[e("pytorch"),r(a)])]),t("li",null,[t("a",be,[e("如何看待最新提出的Rectified Adam (RAdam)？"),r(a)])])])])}const we=i(d,[["render",ue],["__file","2020-11-19-_深度学习_ 深度学习优化器选择学习笔记.html.vue"]]);export{we as default};
