import{_ as a,c as n,a as i,o as e}from"./app-BNuIUq7T.js";const l={};function t(U,s){return e(),n("div",null,s[0]||(s[0]=[i(`<h1 id="opencv实战-37-图像质量评价brisque" tabindex="-1"><a class="header-anchor" href="#opencv实战-37-图像质量评价brisque"><span>[OpenCV实战]37 图像质量评价BRISQUE</span></a></h1><p>摄影是全世界数百万人最喜爱的爱好。毕竟，这有多难啊！用美国著名摄影师阿巴斯•黛安娜的话来说:</p><blockquote><p>拍照就像深夜踮着脚尖走进厨房，偷奥利奥饼干。</p></blockquote><p>拍照很容易，但是拍一张高质量的照片却很难。它需要良好的组成和照明。正确的镜头和优越的设备可以带来很大的不同。但最重要的是，一张高质量的照片需要良好的品味和判断力。但是，是否有一种数学质量度量来捕捉人类的判断呢？答案只能为是或者否。<br> 对于算法来说，有一些质量度量是很容易捕捉到的。例如，我们可以查看像素捕获的信息，并将图像标记为噪声或模糊。但是另一方面，一些质量度量几乎不可能被算法捕捉到。例如，算法很难评估需要有文化背景的图片质量。在这篇文章中，我们将学习一种预测图像质量分数的算法。</p><h2 id="_1-介绍" tabindex="-1"><a class="header-anchor" href="#_1-介绍"><span>1 介绍</span></a></h2><h3 id="_1-1-什么是图像质量评估image-quality-assessment-iqa" tabindex="-1"><a class="header-anchor" href="#_1-1-什么是图像质量评估image-quality-assessment-iqa"><span>1.1 什么是图像质量评估Image Quality Assessment (IQA)？</span></a></h3><p>图像质量评价(IQA)算法以任意图像作为输入，输出质量分数作为输出。有三种类型的IQA：</p><ol><li>全参考图像质量评价 适用情形：您有一个“干净”参考(非扭曲)图像，以衡量您的扭曲图像的质量。此度量可用于评估图像压缩算法的质量，在该算法中，我们可以访问原始图像及其压缩版本。</li><li>半参考图像质量评价 适用情形：如果没有参考图像，而是具有一些选择性信息的图像(例如，水印图像)来比较和测量失真图像的质量。</li><li>无参考图像质量评价 适用情形：算法得到的唯一输入是要测量其质量的图像</li></ol><h3 id="_1-2-无参考图像质量评价" tabindex="-1"><a class="header-anchor" href="#_1-2-无参考图像质量评价"><span>1.2 无参考图像质量评价</span></a></h3><p>在这篇文章中，我们将讨论一个无参考图像质量评价的IQA度量算法，称为盲/无参考图像空间质量评估器Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)。在深入研究这个理论之前，让我们先了解两个基本术语：</p><ol><li><p>失真图像(扭曲图像) 顾名思义，失真图像是指被模糊、噪声、水印、颜色变换、几何变换等因素扭曲的原始图像。下图是TID 2008数据库中使用的图像失真情况。<br><img src="https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/[OpenCV实战]37 图像质量评价BRISQUE/1.jpg?ref_type=heads" alt="在这里插入图片描述" loading="lazy"></p></li><li><p>自然图像 自然图像是指由相机直接拍摄没有后期处理的图像。以下自然图像和失真图像的示例。左图是自然图像，右图是失真图像。 <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8yLmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p></li></ol><p>正如你所能想象的，图像清晰度与图像是失真的还是自然的没有关系。例如，当视频被巧妙地用运动模糊渲染时，由于故意模糊，算法可能会对其质量产生混淆。因此，我们必须在正确的背景下使用这种正确的图像质量评价方法来度量图像。</p><h3 id="_1-3-图像质量评估-iqa-数据集" tabindex="-1"><a class="header-anchor" href="#_1-3-图像质量评估-iqa-数据集"><span>1.3 图像质量评估(IQA)数据集</span></a></h3><p>图像质量好于坏是一个主观问题。为了获得一个优秀的图像质量评估算法，我们需要给出许多图像的算法示例和它们的质量分数。谁为这些训练图像给定质量得分？当然是人类。但我们不能仅仅依靠一个人的意见。因此，我们需要综合评估个人的意见，并为图像分配0（最佳）和100（最差）之间的平均分数。该分数在学术文献中称为平均质量分数。幸运的是我们不需要自己收集这些数据，有一个数据集称为TID 2008已经被用于研究目的。有关TID2008的信息见： <a href="https://pdfs.semanticscholar.org/6dd3/7b57f3391b438fa588f98a1c2067365ae5ca.pdf" target="_blank" rel="noopener noreferrer">https://pdfs.semanticscholar.org/6dd3/7b57f3391b438fa588f98a1c2067365ae5ca.pdf</a></p><p>如下图所示，TID2008图像质量得分范围为0到100，得分越小，主观质量越好。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8zLmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p><h2 id="_2-盲-无参考图像空间质量评估器-brisque" tabindex="-1"><a class="header-anchor" href="#_2-盲-无参考图像空间质量评估器-brisque"><span>2 盲/无参考图像空间质量评估器（BRISQUE）</span></a></h2><p>下图给出了计算BRISQUE所涉及的步骤 <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS80LmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>主要为三步：</p><ol><li>提取自然场景统计（NSS）</li><li>计算特征向量</li><li>预测图像质量得分</li></ol><h3 id="_2-1-提取自然场景统计-nss" tabindex="-1"><a class="header-anchor" href="#_2-1-提取自然场景统计-nss"><span>2.1 提取自然场景统计（NSS）</span></a></h3><p>自然图像的像素强度的分布不同于失真图像的像素强度的分布。当归一化像素强度并计算这些归一化强度上的分布时，这种分布差异更加明显。特别是经过归一化后的自然图像像素强度服从高斯分布(Bell曲线)，而非自然图像或畸变图像的像素强度不服从高斯分布。因此，通过像素高斯分布曲线是衡量图像失真的一种方法。我们已在下图中说明了这一点。 <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS81LmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"> 上图左图显示了一幅有添加人工效应的自然图像，符合高斯分布。右边为一个人造图像，但不适合同样的分布。</p><h4 id="_2-1-1-mscn-mean-subtracted-contrast-normalized-系数" tabindex="-1"><a class="header-anchor" href="#_2-1-1-mscn-mean-subtracted-contrast-normalized-系数"><span>2.1.1 MSCN（Mean Subtracted Contrast Normalized）系数</span></a></h4><p>有几种不同的方法来规范化图像。一种这样的归一化称为MSCN。下图显示了如何计算MSCN系数。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS82LmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>上图MSCN计算过程能够可视化为：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS83LmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>下面要讲述详细的数学推导，但不要让下面的数学吓倒你。数学之后的代码要简单得多，容易理解！</p><figure><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS84LmpwZw?x-oss-process=image/format,png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>有关高斯模糊相关概念解释见：</p><blockquote><p><a href="https://www.jianshu.com/p/302a895c12dd" target="_blank" rel="noopener noreferrer">https://www.jianshu.com/p/302a895c12dd</a></p></blockquote><p>我们在C++和Python中使用GaussianBlur函数计算MSCN系数，如下所示：</p><p><strong>C++</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Mat im = imread(&quot;image_scenery.jpg&quot;); // read image</span></span>
<span class="line"><span>cvtColor(im, im, COLOR_BGR2GRAY); // convert to grayscale</span></span>
<span class="line"><span></span></span>
<span class="line"><span>im.convertTo(im, 1.0/255); // normalize and copy the read image to orig_bw</span></span>
<span class="line"><span>Mat mu(im.size(), CV_64FC1, 1);</span></span>
<span class="line"><span>GaussianBlur(im, mu, Size(7, 7), 1.166); // apply gaussian blur</span></span>
<span class="line"><span>Mat mu_sq = mu.mul(mu); </span></span>
<span class="line"><span></span></span>
<span class="line"><span>// compute sigma</span></span>
<span class="line"><span>Mat sigma = im.size(), CV_64FC1, 1);</span></span>
<span class="line"><span>sigma = im.mul(im); </span></span>
<span class="line"><span>GaussianBlur(sigma, sigma, Size(7, 7), 1.166); // apply gaussian blur</span></span>
<span class="line"><span>subtract(sigma, mu_sq, sigma); // sigma = sigma - mu_sq</span></span>
<span class="line"><span>cv::pow(sigma, double(0.5), sigma); // sigma = sqrt(sigma)</span></span>
<span class="line"><span>add(sigma, Scalar(1.0/255), sigma); // to avoid DivideByZero Exception</span></span>
<span class="line"><span>Mat structdis(im.size(), CV_64FC1, 1);</span></span>
<span class="line"><span>subtract(im, mu, structdis); // structdis = im - mu</span></span>
<span class="line"><span>divide(structdis, sigma, structdis);</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Python</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>im = cv2.imread(&quot;image_scenery.jpg&quot;, 0) # read as gray scale</span></span>
<span class="line"><span>blurred = cv2.GaussianBlur(im, (7, 7), 1.166) # apply gaussian blur to the image</span></span>
<span class="line"><span>blurred_sq = blurred * blurred </span></span>
<span class="line"><span>sigma = cv2.GaussianBlur(im * im, (7, 7), 1.166) </span></span>
<span class="line"><span>sigma = (sigma - blurred_sq) ** 0.5</span></span>
<span class="line"><span>sigma = sigma + 1.0/255 # to make sure the denominator doesn&#39;t give DivideByZero Exception</span></span>
<span class="line"><span>structdis = (im - blurred)/sigma # final MSCN(i, j) image</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-1-2-相邻像素间的乘积关系" tabindex="-1"><a class="header-anchor" href="#_2-1-2-相邻像素间的乘积关系"><span>2.1.2 相邻像素间的乘积关系</span></a></h4><p>MSCN为像素强度提供了不错的归一化。然而自然退休与失真图像的差异不仅限于像素强度分布，还包括相邻像素之间的关系。为了捕获相邻像素的关系，在四个方向来求出相邻元素的两两乘积，即：水平（H），垂直（V），左对角线（D1），右对角线（D2）。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS85LmpwZw?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>可以用Python和C++计算两两乘积，如下所示：</p><p><strong>C++</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>// declare shifting indices array</span></span>
<span class="line"><span>int shifts[4][2] = { {0, 1}, {1, 0}, {1, 1}, {-1, 1} };</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// calculate pair-wise products for every combination of shifting indices</span></span>
<span class="line"><span>for(int itr_shift = 1; itr_shift &lt;= 4; itr_shift++) </span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>    int* reqshift = shifts[itr_shift - 1]; // the required shift index</span></span>
<span class="line"><span>    // declare shifted image</span></span>
<span class="line"><span>    Mat shifted_structdis(imdist_scaled.size(), CV_64F, 1);</span></span>
<span class="line"><span>    // BwImage is a helper class to create a copy of the image and create helper functions to access it&#39;s pixel values</span></span>
<span class="line"><span>    BwImage OrigArr(structdis);</span></span>
<span class="line"><span>    BwImage ShiftArr(shifted_structdis); </span></span>
<span class="line"><span>    // calculate pair-wise component for the given orientation</span></span>
<span class="line"><span>    for(int i = 0; i &lt; structdis.rows; i++)</span></span>
<span class="line"><span>    {</span></span>
<span class="line"><span>        for(int j = 0; j &lt; structdis.cols; j++) { if(i + reqshift[0] &gt;= 0 &amp;&amp; i + reqshift[0] &lt; structdis.rows &amp;&amp; j + reqshift[1] &gt;= 0 &amp;&amp; j + reqshift[1] &lt; structdis.cols)</span></span>
<span class="line"><span>            {</span></span>
<span class="line"><span>                ShiftArr[i][j] = OrigArr[i + reqshift[0]][j + reqshift[1]];</span></span>
<span class="line"><span>            }f</span></span>
<span class="line"><span>            else</span></span>
<span class="line"><span>            {</span></span>
<span class="line"><span>                ShiftArr[i][j] = 0;</span></span>
<span class="line"><span>            }</span></span>
<span class="line"><span>        }   </span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>    Mat shifted_new_structdis;</span></span>
<span class="line"><span>    shifted_new_structdis = ShiftArr.equate(shifted_new_structdis);</span></span>
<span class="line"><span>    // find the pairwise product</span></span>
<span class="line"><span>    multiply(structdis, shifted_new_structdis, shifted_new_structdis);</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Python</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># indices to calculate pair-wise products (H, V, D1, D2)</span></span>
<span class="line"><span>shifts = [[0,1], [1,0], [1,1], [-1,1]]</span></span>
<span class="line"><span># calculate pairwise components in each orientation</span></span>
<span class="line"><span>for itr_shift in range(1, len(shifts) + 1):</span></span>
<span class="line"><span>    OrigArr = structdis</span></span>
<span class="line"><span>    reqshift = shifts[itr_shift-1] # shifting index</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    for i in range(structdis.shape[0]):</span></span>
<span class="line"><span>        for j in range(structdis.shape[1]):</span></span>
<span class="line"><span>            if(i + reqshift[0] &gt;= 0 and i + reqshift[0] &lt; structdis.shape[0] \\ and j + reqshift[1] &gt;= 0 and j  + reqshift[1] &lt; structdis.shape[1]):</span></span>
<span class="line"><span>               ShiftArr[i, j] = OrigArr[i + reqshift[0], j + reqshift[1]]</span></span>
<span class="line"><span>            else:</span></span>
<span class="line"><span>               ShiftArr[i, j] = 0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以使用cv2.warpAffine方法将两个for循环简化为几行代码。这将大大加快计算速度。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># create affine matrix (to shift the image)</span></span>
<span class="line"><span>M = np.float32([[1, 0, reqshift[1]], [0, 1, reqshift[0]]])</span></span>
<span class="line"><span>ShiftArr = cv2.warpAffine(OrigArr, M, (structdis.shape[1], structdis.shape[0])</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-2-计算特征向量" tabindex="-1"><a class="header-anchor" href="#_2-2-计算特征向量"><span>2.2 计算特征向量</span></a></h3><p>到目前为止，我们已经从原始图像派生了5个图像参数，即1个MSCN图像和4个两两乘积图像，以捕获相邻关系(水平、垂直、左对角、右对角)。接下来，我们将使用以上5个参数来计算尺寸为36×1的特征向量。请注意，原始输入图像可以是任何尺寸（宽度/高度），但特征向量的大小始终为36×1。<br> 通过将MSCN图像拟合到广义高斯分布(GGD)来计算36×1特征向量的前两个元素。GGD有两个参数-一个用于形状，另一个用于方差。<br> 接下来，用非对称广义高斯分布（AGGD）拟合相邻元素乘积参数中的任何一个。AGGD是广义高斯拟合（GGD）的不对称形式。它有四个参数，即形状，均值，左方差和右方差。由于有4个两两相乘图像参数，我们最终得到16个值。 因此，我们最终得到了特征向量的18个元素。图像缩小到原来大小的一半，并重复相同的过程以获得18个新的数字，使总数达到36个。下表概述了这一点。</p><table><thead><tr><th style="text-align:center;">特征范围</th><th style="text-align:center;">功能描述</th><th style="text-align:center;">程序</th></tr></thead><tbody><tr><td style="text-align:center;">1-2</td><td style="text-align:center;">形状和方差</td><td style="text-align:center;">GGD拟合MSCN参数</td></tr><tr><td style="text-align:center;">3-6</td><td style="text-align:center;">形状，均值，左方差，右方差</td><td style="text-align:center;">AGGD拟合水平参数</td></tr><tr><td style="text-align:center;">7-10</td><td style="text-align:center;">形状，均值，左方差，右方差</td><td style="text-align:center;">AGGD拟合垂直参数</td></tr><tr><td style="text-align:center;">11-14</td><td style="text-align:center;">形状，均值，左方差，右方差</td><td style="text-align:center;">AGGD拟合对角线（左）参数</td></tr><tr><td style="text-align:center;">15-18</td><td style="text-align:center;">形状，均值，左方差，右方差</td><td style="text-align:center;">AGGD拟合对角线（右）参数</td></tr></tbody></table><h3 id="_2-3-图像质量评分的预测" tabindex="-1"><a class="header-anchor" href="#_2-3-图像质量评分的预测"><span>2.3 图像质量评分的预测</span></a></h3><p>在典型的机器学习应用程序中，首先将图像转换为特征向量。然后将训练数据集中所有图像的特征向量和输出(在这种情况下是质量分数)输入到支持向量机(SVM)等学习算法中。可以下载tid2008数据并且训练一个支持向量机来解决这个问题，但是在这篇文章中，我们将简单地使用作者提供的经过训练的模型。</p><p>下载地址：</p><blockquote><p><a href="http://www.ponomarenko.info/tid2008.htm" target="_blank" rel="noopener noreferrer">http://www.ponomarenko.info/tid2008.htm</a></p></blockquote><p>首先加载训练后的模型，然后利用模型产生的支持向量预测概率，利用LIBSVM预测最终的质量分数。需要注意的是，特征向量首先被缩放为-1到1，然后用于预测。我们共享Python和C++实现这一点的方法：</p><h2 id="_3-结果和代码" tabindex="-1"><a class="header-anchor" href="#_3-结果和代码"><span>3 结果和代码</span></a></h2><h3 id="_3-1-结果" tabindex="-1"><a class="header-anchor" href="#_3-1-结果"><span>3.1 结果</span></a></h3><p>我们对四种类型的失真进行了度量。以下是理论上每个失真的最终质量分数。但是这个仅仅是理论上的，实际上效果在不同电脑和不同图像分辨率上并不是这样的。对于图像压缩结果可能大相径庭。不过总的来说BRISQUE效果不错，但是精度不够，主要是svm还需要自己训练。</p><table><thead><tr><th>原始图像</th><th>JPEG2K压缩</th><th>重压缩</th><th>高斯噪声</th><th>中位数模糊</th></tr></thead><tbody><tr><td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMC5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></td><td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMS5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></td><td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMi5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></td><td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMy5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></td><td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNC5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></td></tr><tr><td>26.8286</td><td>30.7417</td><td>33.0692</td><td>79.8751</td><td>72.7044</td></tr></tbody></table><h3 id="_3-2-代码" tabindex="-1"><a class="header-anchor" href="#_3-2-代码"><span>3.2 代码</span></a></h3><p>OpenCV4.1版本的扩展模块已经有BRISQUE的实现代码，具体可以见官方文档。对于自己编写代码，需要引入libsvm，但是实现起来也不难。具体应用如下：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNS5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>C++将libsvm与代码文件放到同一目录下，并导入libsvm中的svm.h文件和svm.cpp文件 其中computescore用于计算brisque得分，brisque用于提取特征。</p><p>此外在文件目录下还有allmodel文件和allrange文件，allmodel文件为svm模型文件，allrange为训练数据的图像特征相关信息文件。 <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNi5qcGc?x-oss-process=image/format,png" alt="" loading="lazy"></p><p>对于python，运行brisquequality.py文件就行了，但是要注意libsvm需要和brisquequality.py在同一目录下。</p><p>全部代码就不在这里展示，所有代码见：</p><blockquote><p><a href="https://github.com/luohenyueji/OpenCV-Practical-Exercise" target="_blank" rel="noopener noreferrer">https://github.com/luohenyueji/OpenCV-Practical-Exercise</a></p></blockquote><h2 id="_4-参考" tabindex="-1"><a class="header-anchor" href="#_4-参考"><span>4 参考</span></a></h2><blockquote><p><a href="https://www.learnopencv.com/image-quality-assessment-brisque/" target="_blank" rel="noopener noreferrer">https://www.learnopencv.com/image-quality-assessment-brisque/</a><a href="https://blog.csdn.net/weixin_34265814/article/details/94455579" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/weixin_34265814/article/details/94455570</a></p></blockquote>`,63)]))}const J=a(l,[["render",t],["__file","2020-02-29-_OpenCV实战_37 图像质量评价BRISQUE.html.vue"]]),p=JSON.parse('{"path":"/blog/opencv/opencv%E5%AE%9E%E6%88%98/2020-02-29-_OpenCV%E5%AE%9E%E6%88%98_37%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7BRISQUE.html","title":"[OpenCV实战]37 图像质量评价BRISQUE","lang":"zh-CN","frontmatter":{"category":["OpenCV"],"date":"2020-02-29T21:41:08.000Z","tag":["OpenCV实战","OpenCV"],"description":"[OpenCV实战]37 图像质量评价BRISQUE 摄影是全世界数百万人最喜爱的爱好。毕竟，这有多难啊！用美国著名摄影师阿巴斯•黛安娜的话来说: 拍照就像深夜踮着脚尖走进厨房，偷奥利奥饼干。 拍照很容易，但是拍一张高质量的照片却很难。它需要良好的组成和照明。正确的镜头和优越的设备可以带来很大的不同。但最重要的是，一张高质量的照片需要良好的品味和判断力...","head":[["meta",{"property":"og:url","content":"https://luohenyueji.github.io/blog/opencv/opencv%E5%AE%9E%E6%88%98/2020-02-29-_OpenCV%E5%AE%9E%E6%88%98_37%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7BRISQUE.html"}],["meta",{"property":"og:site_name","content":"落痕月极的博客"}],["meta",{"property":"og:title","content":"[OpenCV实战]37 图像质量评价BRISQUE"}],["meta",{"property":"og:description","content":"[OpenCV实战]37 图像质量评价BRISQUE 摄影是全世界数百万人最喜爱的爱好。毕竟，这有多难啊！用美国著名摄影师阿巴斯•黛安娜的话来说: 拍照就像深夜踮着脚尖走进厨房，偷奥利奥饼干。 拍照很容易，但是拍一张高质量的照片却很难。它需要良好的组成和照明。正确的镜头和优越的设备可以带来很大的不同。但最重要的是，一张高质量的照片需要良好的品味和判断力..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D37%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7BRISQUE/1.jpg?ref_type=heads"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:tag","content":"OpenCV实战"}],["meta",{"property":"article:tag","content":"OpenCV"}],["meta",{"property":"article:published_time","content":"2020-02-29T21:41:08.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"[OpenCV实战]37 图像质量评价BRISQUE\\",\\"image\\":[\\"https://gitlab.com/luohenyueji/article_picture_warehouse/-/raw/main/blog/%5BOpenCV%E5%AE%9E%E6%88%98%5D37%20%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7BRISQUE/1.jpg?ref_type=heads\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8yLmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8zLmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS80LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS81LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS82LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS83LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS84LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS85LmpwZw?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMC5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMS5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMi5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xMy5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNC5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNS5qcGc?x-oss-process=image/format,png\\",\\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9DU0ROLyU1Qk9wZW5DViVFNSVBRSU5RSVFNiU4OCU5OCU1RDM3JTIwJUU1JTlCJUJFJUU1JTgzJThGJUU4JUI0JUE4JUU5JTg3JThGJUU4JUFGJTg0JUU0JUJCJUI3QlJJU1FVRS8xNi5qcGc?x-oss-process=image/format,png\\"],\\"datePublished\\":\\"2020-02-29T21:41:08.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"落痕月极\\",\\"url\\":\\"/\\"}]}"]]},"headers":[{"level":2,"title":"1 介绍","slug":"_1-介绍","link":"#_1-介绍","children":[{"level":3,"title":"1.1 什么是图像质量评估Image Quality Assessment (IQA)？","slug":"_1-1-什么是图像质量评估image-quality-assessment-iqa","link":"#_1-1-什么是图像质量评估image-quality-assessment-iqa","children":[]},{"level":3,"title":"1.2 无参考图像质量评价","slug":"_1-2-无参考图像质量评价","link":"#_1-2-无参考图像质量评价","children":[]},{"level":3,"title":"1.3 图像质量评估(IQA)数据集","slug":"_1-3-图像质量评估-iqa-数据集","link":"#_1-3-图像质量评估-iqa-数据集","children":[]}]},{"level":2,"title":"2 盲/无参考图像空间质量评估器（BRISQUE）","slug":"_2-盲-无参考图像空间质量评估器-brisque","link":"#_2-盲-无参考图像空间质量评估器-brisque","children":[{"level":3,"title":"2.1 提取自然场景统计（NSS）","slug":"_2-1-提取自然场景统计-nss","link":"#_2-1-提取自然场景统计-nss","children":[]},{"level":3,"title":"2.2 计算特征向量","slug":"_2-2-计算特征向量","link":"#_2-2-计算特征向量","children":[]},{"level":3,"title":"2.3 图像质量评分的预测","slug":"_2-3-图像质量评分的预测","link":"#_2-3-图像质量评分的预测","children":[]}]},{"level":2,"title":"3 结果和代码","slug":"_3-结果和代码","link":"#_3-结果和代码","children":[{"level":3,"title":"3.1 结果","slug":"_3-1-结果","link":"#_3-1-结果","children":[]},{"level":3,"title":"3.2 代码","slug":"_3-2-代码","link":"#_3-2-代码","children":[]}]},{"level":2,"title":"4 参考","slug":"_4-参考","link":"#_4-参考","children":[]}],"git":{},"readingTime":{"minutes":10.62,"words":3186},"filePathRelative":"blog/opencv/opencv实战/2020-02-29-[OpenCV实战]37 图像质量评价BRISQUE.md","localizedDate":"2020年3月1日","excerpt":"\\n<p>摄影是全世界数百万人最喜爱的爱好。毕竟，这有多难啊！用美国著名摄影师阿巴斯•黛安娜的话来说:</p>\\n<blockquote>\\n<p>拍照就像深夜踮着脚尖走进厨房，偷奥利奥饼干。</p>\\n</blockquote>\\n<p>拍照很容易，但是拍一张高质量的照片却很难。它需要良好的组成和照明。正确的镜头和优越的设备可以带来很大的不同。但最重要的是，一张高质量的照片需要良好的品味和判断力。但是，是否有一种数学质量度量来捕捉人类的判断呢？答案只能为是或者否。<br>\\n对于算法来说，有一些质量度量是很容易捕捉到的。例如，我们可以查看像素捕获的信息，并将图像标记为噪声或模糊。但是另一方面，一些质量度量几乎不可能被算法捕捉到。例如，算法很难评估需要有文化背景的图片质量。在这篇文章中，我们将学习一种预测图像质量分数的算法。</p>","autoDesc":true}');export{J as comp,p as data};
